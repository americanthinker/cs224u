{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc88e2aa-db48-4dae-8a7e-8e56cfa0b6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n",
    "from torch_tree_nn import TorchTreeNN\n",
    "import sst\n",
    "import utils\n",
    "import spacy \n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from typing import List, Union\n",
    "from faker import Faker\n",
    "import multiprocessing as mp\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeca33a5-feb1-42eb-bad7-ff9e55739eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SST_HOME = os.path.join('data', 'sentiment')\n",
    "data_path = '/home/americanthinker/notebooks/pytorch/cs224u/data/sentiment/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cefacc52-3ad2-40c8-bd70-a25d54ac0466",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_train = sst.train_reader(SST_HOME, include_subtrees=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72628ea8-469f-457e-91ab-da8c7d8e2ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    3610\n",
       "negative    3310\n",
       "neutral     1624\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst_train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3ab2cf1-e7cc-4dd0-a369-8ad62c95270f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8544 entries, 0 to 318573\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   example_id  8544 non-null   object\n",
      " 1   sentence    8544 non-null   object\n",
      " 2   label       8544 non-null   object\n",
      " 3   is_subtree  8544 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 333.8+ KB\n"
     ]
    }
   ],
   "source": [
    "sst_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be1eb51-46fb-4004-9587-490223cec71e",
   "metadata": {},
   "source": [
    "### 1.Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40548907-84f1-4134-a6b7-248ecb51aff4",
   "metadata": {},
   "source": [
    "#### Part A: Replacing Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c4143200-f44a-4cae-85a6-a87cf5f8c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sst_train.sentence.values.tolist()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "345b2b0d-0e82-4b4f-8a6e-be67faf8c01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\",\n",
       " \"The gorgeously elaborate continuation of `` The Lord of the Rings '' trilogy is so huge that a column of words can not adequately describe co-writer\\\\/director Peter Jackson 's expanded vision of J.R.R. Tolkien 's Middle-earth .\",\n",
       " 'Singer\\\\/composer Bryan Adams contributes a slew of songs -- a few potential hits , a few more simply intrusive to the story -- but the whole package certainly captures the intended , er , spirit of the piece .',\n",
       " \"You 'd think by now America would have had enough of plucky British eccentrics with hearts of gold .\",\n",
       " 'Yet the act is still charming here .']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d33bf0f4-2948-4265-8701-93749b19bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4c7f64c4-d612-446c-be63-889a55185982",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [nlp(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "904f3e9e-34b7-418c-85bb-622bf6cf4221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PERSON', 'DATE', 'WORK_OF_ART', 'PERSON', 'PERSON', 'PERSON']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found\n"
     ]
    }
   ],
   "source": [
    "test = nlp(sentences[0])\n",
    "test_labels = [ent.label_ for ent in test.ents]\n",
    "test_labels\n",
    "if \"PERSON\" in test_labels:\n",
    "    print(\"found\")\n",
    "else: print(\"Does not exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "302db9e2-8fc6-4750-a343-eda348939d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_sentence_generator(df: pd.DataFrame, n_augments: int = None, labels: Union[str, List[str]] = 'neutral') -> pd.DataFrame:\n",
    "    augmented_df = df.copy()\n",
    "    faker = Faker()\n",
    "    frames = []\n",
    "    \n",
    "    if isinstance(labels, str):\n",
    "        labels = [labels]\n",
    "    \n",
    "    #start augmentation process for each label provided\n",
    "    for label in labels:\n",
    "        new_sentences = []\n",
    "        \n",
    "        #coerce to a list of sentences\n",
    "        if n_augments:\n",
    "            sentences = augmented_df[augmented_df['label'] == label].sentence.values.tolist()[:n_augments]\n",
    "        else:\n",
    "            sentences = augmented_df[augmented_df['label'] == label].sentence.values.tolist()\n",
    "            \n",
    "        #coerce to a list of spacy docs\n",
    "        docs = [nlp(s) for s in sentences]\n",
    "        \n",
    "        #grab tokens and match with original sentences by index\n",
    "        for index, doc in enumerate(docs):\n",
    "            sentence = sentences[index]\n",
    "            \n",
    "            #check to see if there is a PERSON entity in the sentence\n",
    "            entities = [ent.label_ for ent in doc.ents]\n",
    "            if \"PERSON\" not in entities:\n",
    "                continue\n",
    "            else:\n",
    "                for token in doc:\n",
    "                    if token.ent_type_ == 'PERSON':\n",
    "                        sentence = sentence.replace(str(token), faker.name().split()[0])\n",
    "                new_sentences.append(sentence)\n",
    "        \n",
    "        #coerce sentences into dict to make new df\n",
    "        row_dicts = [{'example_id':'augmented', 'sentence':sentence, 'label':label, 'is_subtree':0} for sentence in new_sentences]\n",
    "        aug_df = pd.DataFrame(row_dicts)\n",
    "        frames.append(aug_df)\n",
    "    \n",
    "    #concat all frames first if multilabel\n",
    "    if len(frames) > 1:\n",
    "        augmented_frame = pd.concat(frames, ignore_index=True)\n",
    "        new_df = pd.concat([augmented_df, augmented_frame])\n",
    "        return new_df   \n",
    "    \n",
    "    elif len(frames) == 1:\n",
    "        new_df = pd.concat([augmented_df, frames[0]], ignore_index=True)\n",
    "        return new_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8030d4bf-5813-4e6a-bfb0-b4b0a6fc7654",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multiprocess_ner_sentence_generator(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    augmented_df = df.copy()\n",
    "    faker = Faker()\n",
    "    #n_augments = 1000\n",
    "\n",
    "    #start augmentation process for each label provided\n",
    "    \n",
    "    new_sentences = []\n",
    "    \n",
    "    #coerce to a list of sentences\n",
    "    sentences = augmented_df[augmented_df['label'] == 'neutral'].sentence.values.tolist()\n",
    "        \n",
    "    #coerce to a list of spacy docs\n",
    "    docs = [nlp(s) for s in sentences]\n",
    "    \n",
    "    #grab tokens and match with original sentences by index\n",
    "    for index, doc in enumerate(docs):\n",
    "        sentence = sentences[index]\n",
    "        \n",
    "        #check to see if there is a PERSON entity in the sentence\n",
    "        entities = [ent.label_ for ent in doc.ents]\n",
    "        if \"PERSON\" not in entities:\n",
    "            continue\n",
    "        else:\n",
    "            for token in doc:\n",
    "                if token.ent_type_ == 'PERSON':\n",
    "                    sentence = sentence.replace(str(token), faker.name().split()[0])\n",
    "            new_sentences.append(sentence)\n",
    "    \n",
    "    #coerce sentences into dict to make new df\n",
    "    row_dicts = [{'example_id':'augmented', 'sentence':sentence, 'label':'neutral', 'is_subtree':0} for sentence in new_sentences]\n",
    "    aug_df = pd.DataFrame(row_dicts)\n",
    "    \n",
    "    return aug_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b746c71f-04f2-4754-8e96-b9a481794545",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df, func, n_cores=mp.cpu_count()):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = mp.Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a161f2c4-e53e-455f-8fcc-34c86a2f6709",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a618d67-b768-46aa-865f-cbe00fe09fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jeanette gives a good performance in a film that does not merit it .'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug[aug['example_id'] == 'augmented'].loc[8820].values[1].replace(\"n't\", \" not\").replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92b893-be75-466c-988a-e8627ae968d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
