{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework and bake-off: Word relatedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2021\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [Set-up](#Set-up)\n",
    "1. [Development dataset](#Development-dataset)\n",
    "  1. [Vocabulary](#Vocabulary)\n",
    "  1. [Score distribution](#Score-distribution)\n",
    "  1. [Repeated pairs](#Repeated-pairs)\n",
    "1. [Evaluation](#Evaluation)\n",
    "1. [Error analysis](#Error-analysis)\n",
    "1. [Homework questions](#Homework-questions)\n",
    "  1. [PPMI as a baseline [0.5 points]](#PPMI-as-a-baseline-[0.5-points])\n",
    "  1. [Gigaword with LSA at different dimensions [0.5 points]](#Gigaword-with-LSA-at-different-dimensions-[0.5-points])\n",
    "  1. [t-test reweighting [2 points]](#t-test-reweighting-[2-points])\n",
    "  1. [Pooled BERT representations [1 point]](#Pooled-BERT-representations-[1-point])\n",
    "  1. [Learned distance functions [2 points]](#Learned-distance-functions-[2-points])\n",
    "  1. [Your original system [3 points]](#Your-original-system-[3-points])\n",
    "1. [Bake-off [1 point]](#Bake-off-[1-point])\n",
    "1. [Submission Instruction](#Submission-Instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Word similarity and relatedness datasets have long been used to evaluate distributed representations. This notebook provides code for conducting such analyses with a new word relatedness datasets. It consists of word pairs, each with an associated human-annotated relatedness score. \n",
    "\n",
    "The evaluation metric for each dataset is the [Spearman correlation coefficient $\\rho$](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) between the annotated scores and your distances, as is standard in the literature.\n",
    "\n",
    "This homework ([questions at the bottom of this notebook](#Homework-questions)) asks you to write code that uses the count matrices in `data/vsmdata` to create and evaluate some baseline models. The final question asks you to create your own original system for this task, using any data you wish. This accounts for 9 of the 10 points for this assignment.\n",
    "\n",
    "For the associated bake-off, we will distribute a new dataset, and you will evaluate your original system (no additional training or tuning allowed!) on that datasets and submit your predictions. Systems that enter will receive the additional homework point, and systems that achieve the top score will receive an additional 0.5 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "\n",
    "import vsm\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VSM_HOME = os.path.join('data', 'vsmdata')\n",
    "\n",
    "DATA_HOME = os.path.join('data', 'wordrelatedness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use development dataset freely, since our bake-off evalutions involve a new test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv(\n",
    "    os.path.join(DATA_HOME, \"cs224u-wordrelatedness-dev.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4756 entries, 0 to 4755\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   word1   4756 non-null   object \n",
      " 1   word2   4756 non-null   object \n",
      " 2   score   4756 non-null   float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 111.6+ KB\n"
     ]
    }
   ],
   "source": [
    "dev_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of word pairs with scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>accentuate</td>\n",
       "      <td>highlight</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>automobile</td>\n",
       "      <td>car</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>balanced</td>\n",
       "      <td>symmetrical</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>boast</td>\n",
       "      <td>brag</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>build</td>\n",
       "      <td>construct</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>compare</td>\n",
       "      <td>comparing</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>crab</td>\n",
       "      <td>crabs</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119</th>\n",
       "      <td>divide</td>\n",
       "      <td>split</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2324</th>\n",
       "      <td>end</td>\n",
       "      <td>terminate</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533</th>\n",
       "      <td>female</td>\n",
       "      <td>woman</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3485</th>\n",
       "      <td>lengthy</td>\n",
       "      <td>long</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3544</th>\n",
       "      <td>lingerie</td>\n",
       "      <td>underwear</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word1        word2  score\n",
       "18    accentuate    highlight    1.0\n",
       "386   automobile          car    1.0\n",
       "450     balanced  symmetrical    1.0\n",
       "782        boast         brag    1.0\n",
       "991        build    construct    1.0\n",
       "1562     compare    comparing    1.0\n",
       "1712        crab        crabs    1.0\n",
       "2119      divide        split    1.0\n",
       "2324         end    terminate    1.0\n",
       "2533      female        woman    1.0\n",
       "3485     lengthy         long    1.0\n",
       "3544    lingerie    underwear    1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df[(dev_df['word1'] != dev_df['word2'])&(dev_df['score']==1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives the number of word pairs in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4756, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set will contain 1500 word pairs with scores of the same type. No word pair in the development set appears in the test set, but some of the individual words are repeated in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full vocabulary in the dataframe can be extracted as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_vocab = set(dev_df.word1.values) | set(dev_df.word2.values)\n",
    "len(dev_vocab)\n",
    "sorted_vocab = sorted(dev_vocab)\n",
    "from string import ascii_lowercase\n",
    "\n",
    "word_dist = {k:0 for k in ascii_lowercase}\n",
    "\n",
    "for word in sorted_vocab:\n",
    "    if word[0] in word_dist:\n",
    "        word_dist[word[0]] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 363),\n",
       " ('c', 300),\n",
       " ('p', 243),\n",
       " ('b', 184),\n",
       " ('a', 179),\n",
       " ('d', 174),\n",
       " ('r', 166),\n",
       " ('m', 143),\n",
       " ('f', 133),\n",
       " ('t', 124),\n",
       " ('l', 110),\n",
       " ('e', 103),\n",
       " ('i', 96),\n",
       " ('h', 93),\n",
       " ('g', 86),\n",
       " ('w', 75),\n",
       " ('o', 49),\n",
       " ('n', 41),\n",
       " ('v', 40),\n",
       " ('u', 37),\n",
       " ('j', 23),\n",
       " ('k', 20),\n",
       " ('q', 11),\n",
       " ('y', 11),\n",
       " ('z', 5),\n",
       " ('x', 0)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_dist.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#[word for word in sorted_vocab if word.startswith('c')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>776.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.895955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.072187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.838210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            score\n",
       "count  776.000000\n",
       "mean     0.895955\n",
       "std      0.072187\n",
       "min      0.800000\n",
       "25%      0.838210\n",
       "50%      0.880000\n",
       "75%      0.980000\n",
       "max      1.000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_relates = dev_df[dev_df['score'] >= 0.8]\n",
    "high_relates.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary for the bake-off test is different â€“ it is partly overlapping with the above. If you want to be sure ahead of time that your system has a representation for every word in the dev and test sets, then you can check against the vocabularies of any of the VSMs in `data/vsmdata` (which all have the same vocabulary). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_index = pd.read_csv(\n",
    "    os.path.join(VSM_HOME, 'yelp_window5-scaled.csv.gz'),\n",
    "    usecols=[0], index_col=0)\n",
    "\n",
    "full_task_vocab = list(task_index.index)\n",
    "len(full_task_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('s', 713),\n",
       " ('c', 596),\n",
       " ('p', 472),\n",
       " ('a', 393),\n",
       " ('d', 348),\n",
       " ('b', 346),\n",
       " ('r', 344),\n",
       " ('t', 327),\n",
       " ('m', 290),\n",
       " ('f', 273),\n",
       " ('e', 256),\n",
       " ('l', 232),\n",
       " ('h', 216),\n",
       " ('i', 206),\n",
       " ('w', 205),\n",
       " ('g', 175),\n",
       " ('o', 144),\n",
       " ('n', 111),\n",
       " ('v', 83),\n",
       " ('u', 77),\n",
       " ('j', 72),\n",
       " ('k', 49),\n",
       " ('y', 31),\n",
       " ('q', 24),\n",
       " ('z', 6),\n",
       " ('x', 0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_word_dist = {k:0 for k in ascii_lowercase}\n",
    "\n",
    "for word in full_task_vocab:\n",
    "    if word[0] in full_word_dist:\n",
    "        full_word_dist[word[0]] += 1\n",
    "        \n",
    "sorted(full_word_dist.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can process every one of those words, then you are all set. Alternatively, you can wait to see the test set and make system adjustments to ensure that you can process all those words. This is fine as long as you are not tuning your predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the scores fall in $[0, 1]$, and the dataset skews towards words with low scores, meaning low relatedness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('ggplot')\n",
    "ax = dev_df.plot.hist(color='lightblue', edgecolor='black').set_xlabel(\"Relatedness score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The development data has some word pairs with multiple distinct scores in it. Here we create a `pd.Series` that contains these word pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word1        word2     \n",
       "buck         dollar        0.378592\n",
       "furnace      stove         0.274653\n",
       "cash         money         0.247104\n",
       "boxing       round         0.189342\n",
       "money        possession    0.187652\n",
       "                             ...   \n",
       "dollar       money         0.000069\n",
       "combination  direction     0.000034\n",
       "form         type          0.000014\n",
       "delay        news          0.000014\n",
       "noon         string        0.000001\n",
       "Name: score variance, Length: 279, dtype: float64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeats = dev_df.groupby(['word1', 'word2']).apply(lambda x: x.score.var())\n",
    "\n",
    "repeats = repeats[repeats > 0].sort_values(ascending=False)\n",
    "\n",
    "repeats.name = 'score variance'\n",
    "\n",
    "repeats[repeats > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat_list = repeats.index.tolist()\n",
    "len(repeat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_words(first_word, second_word):\n",
    "    return dev_df[(dev_df['word1']==first_word)&(dev_df['word2']==second_word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_remover(df, index):\n",
    "    val_df = df.drop(index=index, axis=0)\n",
    "    return val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For repeats in the development dataset, I am going to (initially) try three approaches:\n",
    "- Lowest score: remove higher score of the two\n",
    "- Highest score: remove the lower score of the two\n",
    "- Average score: take the mean of the two scores\n",
    "- Nothing: leave scores as-is\n",
    "\n",
    "#### As an initial hypothesis, I think the highest score will perform best, given that after my cursory review of these pairs, I, as a human annotator would have labeld these pairs with similiar high scores.  I think the mean or doing nothing will perform the worst. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>buck</td>\n",
       "      <td>dollar</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>buck</td>\n",
       "      <td>dollar</td>\n",
       "      <td>0.920164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word1   word2     score\n",
       "972  buck  dollar  0.050000\n",
       "973  buck  dollar  0.920164"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = repeat_words('buck', 'dollar')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48508188331627444"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms = test.score.mean()\n",
    "ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[973]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = test.index[0].tolist()\n",
    "drops = test.index[1:].tolist()\n",
    "drops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/haystack/lib/python3.8/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "source": [
    "test.loc[973, 'score'] = ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word1        buck\n",
       "word2      dollar\n",
       "score    0.485082\n",
       "Name: 972, dtype: object"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.loc[index,:].replace(test.loc[index,:].score, ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>buck</td>\n",
       "      <td>dollar</td>\n",
       "      <td>0.485082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>buck</td>\n",
       "      <td>dollar</td>\n",
       "      <td>0.485082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word1   word2     score\n",
       "972  buck  dollar  0.485082\n",
       "973  buck  dollar  0.485082"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/haystack/lib/python3.8/site-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>buck</td>\n",
       "      <td>dollar</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word1   word2  score\n",
       "972  buck  dollar   0.05"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.drop(index=drops, axis=0, inplace=True)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devdf_generator(df, scoring=str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Removes repeated pairs with distinct scores from dev_df based on scoring removal string.\n",
    "    '''\n",
    "\n",
    "    temp_df = df.copy()\n",
    "    for pair in repeat_list:\n",
    "        repeat = repeat_words(pair[0], pair[1])\n",
    "        if scoring == 'highest':\n",
    "            while len(repeat.index) > 1:\n",
    "                min_score = repeat.score.min()\n",
    "                index = repeat[repeat.score == min_score].index.values[0]\n",
    "                temp_df.drop(index=index, axis=0, inplace=True)\n",
    "                repeat.drop(index=index, axis=0, inplace=True)\n",
    "        elif scoring == 'lowest':\n",
    "            while len(repeat.index) > 1:\n",
    "                max_score = repeat.score.max()\n",
    "                index = repeat[repeat.score == max_score].index.values[0]\n",
    "                temp_df.drop(index=index, axis=0, inplace=True)\n",
    "                repeat.drop(index=index, axis=0, inplace=True)\n",
    "        elif scoring == 'mean':\n",
    "                mean_score = repeat.score.mean()\n",
    "                index = repeat.index[0]\n",
    "                drops = repeat.index[1:]\n",
    "                temp_df.loc[index, 'score'] = mean_score\n",
    "                temp_df.drop(index=drops, axis=0, inplace=True)\n",
    "                \n",
    "    #check to see that variant pairs are dropped\n",
    "    repeats = temp_df.groupby(['word1', 'word2']).apply(lambda x: x.score.var())\n",
    "    repeats = repeats[repeats > 0.06].sort_values(ascending=False)\n",
    "    repeats.name = 'score variance'\n",
    "    answer = repeats[repeats > 0].sort_values(ascending=False)\n",
    "    print(answer)\n",
    "    \n",
    "    return temp_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/haystack/lib/python3.8/site-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: score variance, dtype: float64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>bank</td>\n",
       "      <td>money</td>\n",
       "      <td>0.846469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word1  word2     score\n",
       "479  bank  money  0.846469"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest = devdf_generator(dev_df, scoring='highest')\n",
    "highest[(highest['word1'] == 'bank')&(highest['word2'] == 'money')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/haystack/lib/python3.8/site-packages/pandas/core/frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: score variance, dtype: float64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>bank</td>\n",
       "      <td>money</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word1  word2  score\n",
       "476  bank  money   0.25"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowest = devdf_generator(dev_df, scoring='lowest')\n",
    "lowest[(lowest['word1'] == 'bank')&(lowest['word2'] == 'money')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: score variance, dtype: float64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>bank</td>\n",
       "      <td>money</td>\n",
       "      <td>0.581011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word1  word2     score\n",
       "476  bank  money  0.581011"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average = devdf_generator(dev_df, scoring='mean')\n",
    "average[(average['word1'] == 'bank')&(average['word2'] == 'money')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pd.Series` is sorted with the highest variance items at the top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>accepted</td>\n",
       "      <td>accepted</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>accommodate</td>\n",
       "      <td>accommodate</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>activity</td>\n",
       "      <td>activity</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>affect</td>\n",
       "      <td>affect</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>aged</td>\n",
       "      <td>aged</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4740</th>\n",
       "      <td>walking</td>\n",
       "      <td>walking</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748</th>\n",
       "      <td>white</td>\n",
       "      <td>white</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>withdraw</td>\n",
       "      <td>withdraw</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4753</th>\n",
       "      <td>workings</td>\n",
       "      <td>workings</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4755</th>\n",
       "      <td>yielding</td>\n",
       "      <td>yielding</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word1        word2  score\n",
       "24       accepted     accepted    1.0\n",
       "32    accommodate  accommodate    1.0\n",
       "64       activity     activity    1.0\n",
       "99         affect       affect    1.0\n",
       "110          aged         aged    1.0\n",
       "...           ...          ...    ...\n",
       "4740      walking      walking    1.0\n",
       "4748        white        white    1.0\n",
       "4752     withdraw     withdraw    1.0\n",
       "4753     workings     workings    1.0\n",
       "4755     yielding     yielding    1.0\n",
       "\n",
       "[177 rows x 3 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_words = dev_df[dev_df['word1'] == dev_df['word2']]\n",
    "same_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is development data, it is up to you how you want to handle these repeats. The test set has no repeated pairs in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation function is `vsm.word_relatedness_evaluation`. Its arguments:\n",
    "    \n",
    "1. A relatedness dataset `pd.DataFrame` â€“ e.g., `dev_df` as given above.\n",
    "1. A VSM `pd.DataFrame` â€“ e.g., `giga5` or some transformation thereof, or a GloVe embedding space, or something you have created on your own. The function checks that you can supply a representation for every word in `dev_df` and raises an exception if you can't.\n",
    "1. Optionally a `distfunc` argument, which defaults to `vsm.cosine`.\n",
    "\n",
    "The function returns a tuple:\n",
    "\n",
    "1. A copy of `dev_df` with a new column giving your predictions.\n",
    "1. The Spearman $\\rho$ value (our primary score).\n",
    "\n",
    "Important note: Internally, `vsm.word_relatedness_evaluation` uses `-distfunc(x1, x2)` as its score, where `x1` and `x2` are vector representations of words. This is because the scores in our data are _positive_ relatedness scores, whereas we are assuming that `distfunc` is a _distance_ function.\n",
    "\n",
    "Here's a simple illustration using one of our count matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = pd.read_csv(\n",
    "    os.path.join(VSM_HOME, \"giga_window5-scaled.csv.gz\"), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_pred_df, count_rho = vsm.word_relatedness_evaluation(dev_df, count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2776320615138188"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandon</td>\n",
       "      <td>button</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.336291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandon</td>\n",
       "      <td>consigning</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-0.085422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandon</td>\n",
       "      <td>crane</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.307229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandon</td>\n",
       "      <td>ditch</td>\n",
       "      <td>0.63</td>\n",
       "      <td>-0.211550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abandon</td>\n",
       "      <td>left</td>\n",
       "      <td>0.57</td>\n",
       "      <td>-0.337866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word1       word2  score  prediction\n",
       "0  abandon      button   0.18   -0.336291\n",
       "1  abandon  consigning   0.40   -0.085422\n",
       "2  abandon       crane   0.16   -0.307229\n",
       "3  abandon       ditch   0.63   -0.211550\n",
       "4  abandon        left   0.57   -0.337866"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's instructive to compare this against a truly random system, which we can create by simply having a custom distance function that returns a random number in [0, 1] for each example, making no use of the VSM itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_scorer(x1, x2):\n",
    "    \"\"\"`x1` and `x2` are vectors, to conform to the requirements\n",
    "    of `vsm.word_relatedness_evaluation`, but this function just\n",
    "    returns a random number in [0, 1].\"\"\"\n",
    "    return random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00059854523902375"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_pred_df, random_rho = vsm.word_relatedness_evaluation(\n",
    "    dev_df, count_df, distfunc=random_scorer)\n",
    "\n",
    "random_rho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a truly baseline system!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis\n",
    "\n",
    "For error analysis, we can look at the words with the largest delta between the gold score and the distance value in our VSM. We do these comparisons based on ranks, just as with our primary metric (Spearman $\\rho$), and we normalize both rankings so that they have a comparable number of levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(pred_df):\n",
    "    pred_df = pred_df.copy()\n",
    "    pred_df['score_rank'] = _normalized_ranking(pred_df.score)\n",
    "    pred_df['relatedness_rank'] = _normalized_ranking(pred_df.prediction)\n",
    "    pred_df['error'] =  abs(pred_df['relatedness_rank'] - pred_df['score_rank'])\n",
    "    return pred_df.sort_values('error')\n",
    "\n",
    "\n",
    "def _normalized_ranking(series):\n",
    "    ranks = series.rank(method='dense')\n",
    "    return ranks / ranks.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4443</th>\n",
       "      <td>se</td>\n",
       "      <td>sunflower</td>\n",
       "      <td>0.78000</td>\n",
       "      <td>-0.836894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4442</th>\n",
       "      <td>se</td>\n",
       "      <td>stair</td>\n",
       "      <td>0.18000</td>\n",
       "      <td>-0.796295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3904</th>\n",
       "      <td>noon</td>\n",
       "      <td>string</td>\n",
       "      <td>0.03000</td>\n",
       "      <td>-0.753739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3905</th>\n",
       "      <td>noon</td>\n",
       "      <td>string</td>\n",
       "      <td>0.03173</td>\n",
       "      <td>-0.753739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338</th>\n",
       "      <td>repeating</td>\n",
       "      <td>replicate</td>\n",
       "      <td>0.92500</td>\n",
       "      <td>-0.728052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word1      word2    score  prediction\n",
       "4443         se  sunflower  0.78000   -0.836894\n",
       "4442         se      stair  0.18000   -0.796295\n",
       "3904       noon     string  0.03000   -0.753739\n",
       "3905       noon     string  0.03173   -0.753739\n",
       "4338  repeating  replicate  0.92500   -0.728052"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inds = count_pred_df.prediction.rank(method='dense').sort_values().head().index.tolist()\n",
    "count_pred_df.loc[inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "      <th>prediction</th>\n",
       "      <th>score_rank</th>\n",
       "      <th>relatedness_rank</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3121</th>\n",
       "      <td>health</td>\n",
       "      <td>psychology</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>-0.238147</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>3.284913e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2984</th>\n",
       "      <td>grey</td>\n",
       "      <td>purple</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>-0.125642</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>3.895284e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>eye</td>\n",
       "      <td>organ</td>\n",
       "      <td>0.789141</td>\n",
       "      <td>-0.073605</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>6.168281e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3591</th>\n",
       "      <td>losses</td>\n",
       "      <td>play</td>\n",
       "      <td>0.543103</td>\n",
       "      <td>-0.163590</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>1.214181e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>appearance</td>\n",
       "      <td>image</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>-0.166412</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>1.518298e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word1       word2     score  prediction  score_rank  \\\n",
       "3121      health  psychology  0.325000   -0.238147    0.000127   \n",
       "2984        grey      purple  0.660000   -0.125642    0.000283   \n",
       "2439         eye       organ  0.789141   -0.073605    0.000361   \n",
       "3591      losses        play  0.543103   -0.163590    0.000222   \n",
       "239   appearance       image  0.535000   -0.166412    0.000218   \n",
       "\n",
       "      relatedness_rank         error  \n",
       "3121          0.000127  3.284913e-08  \n",
       "2984          0.000283  3.895284e-08  \n",
       "2439          0.000361  6.168281e-08  \n",
       "3591          0.000223  1.214181e-07  \n",
       "239           0.000218  1.518298e-07  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_analysis(count_pred_df).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worst predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "      <th>prediction</th>\n",
       "      <th>score_rank</th>\n",
       "      <th>relatedness_rank</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4643</th>\n",
       "      <td>submit</td>\n",
       "      <td>yield</td>\n",
       "      <td>0.916750</td>\n",
       "      <td>-0.473501</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>1.440789e-05</td>\n",
       "      <td>0.000408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>bulletin</td>\n",
       "      <td>news</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>-0.503304</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>1.082946e-05</td>\n",
       "      <td>0.000414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4091</th>\n",
       "      <td>photo</td>\n",
       "      <td>picture</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>-0.632888</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>2.165892e-06</td>\n",
       "      <td>0.000421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3001</th>\n",
       "      <td>grow</td>\n",
       "      <td>sprouting</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>-0.518391</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>9.134414e-06</td>\n",
       "      <td>0.000421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338</th>\n",
       "      <td>repeating</td>\n",
       "      <td>replicate</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>-0.728052</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>3.766768e-07</td>\n",
       "      <td>0.000424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word1      word2     score  prediction  score_rank  \\\n",
       "4643     submit      yield  0.916750   -0.473501    0.000423   \n",
       "1010   bulletin       news  0.925926   -0.503304    0.000425   \n",
       "4091      photo    picture  0.920000   -0.632888    0.000423   \n",
       "3001       grow  sprouting  0.950000   -0.518391    0.000430   \n",
       "4338  repeating  replicate  0.925000   -0.728052    0.000424   \n",
       "\n",
       "      relatedness_rank     error  \n",
       "4643      1.440789e-05  0.000408  \n",
       "1010      1.082946e-05  0.000414  \n",
       "4091      2.165892e-06  0.000421  \n",
       "3001      9.134414e-06  0.000421  \n",
       "4338      3.766768e-07  0.000424  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_analysis(count_pred_df).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework questions\n",
    "\n",
    "Please embed your homework responses in this notebook, and do not delete any cells from the notebook. (You are free to add as many cells as you like as part of your responses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPMI as a baseline [0.5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The insight behind PPMI is a recurring theme in word representation learning, so it is a natural baseline for our task. This question asks you to write code for conducting such experiments.\n",
    "\n",
    "Your task: write a function called `run_giga_ppmi_baseline` that does the following:\n",
    "\n",
    "1. Reads the Gigaword count matrix with a window of 20 and a flat scaling function into a `pd.DataFrame`, as is done in the VSM notebooks. The file is `data/vsmdata/giga_window20-flat.csv.gz`, and the VSM notebooks provide examples of the needed code.\n",
    "1. Reweights this count matrix with PPMI.\n",
    "1. Evaluates this reweighted matrix using `vsm.word_relatedness_evaluation` on `dev_df` as defined above, with `distfunc` set to the default of `vsm.cosine`.\n",
    "1. Returns the return value of this call to `vsm.word_relatedness_evaluation`.\n",
    "\n",
    "The goal of this question is to help you get more familiar with the code in `vsm` and the function `vsm.word_relatedness_evaluation`.\n",
    "\n",
    "The function `test_run_giga_ppmi_baseline` can be used to test that you've implemented this specification correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vsm import observed_over_expected, pmi\n",
    "\n",
    "def run_giga_ppmi_baseline():\n",
    "    '''\n",
    "    Hardcoded implementation of reweighting giga_window20 dataset.  \n",
    "    Returns copy of dataset with new \"Predicted\" col and Spearmann Rho compared against dev_df\n",
    "    '''\n",
    "    giga_df = pd.read_csv(os.path.join(VSM_HOME, \"giga_window20-flat.csv.gz\"), index_col=0)\n",
    "    ppmi_df = pmi(giga_df)\n",
    "    giga_pred, giga_rho =  vsm.word_relatedness_evaluation(dev_df, ppmi_df)\n",
    "    \n",
    "    return giga_pred, giga_rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run_giga_ppmi_baseline(func):\n",
    "    \"\"\"`func` should be `run_giga_ppmi_baseline\"\"\"\n",
    "    pred_df, rho = func()\n",
    "    rho = round(rho, 3)\n",
    "    expected = 0.586\n",
    "    assert rho == expected, \\\n",
    "        \"Expected rho of {}; got {}\".format(expected, rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_run_giga_ppmi_baseline(run_giga_ppmi_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, giga_rho = run_giga_ppmi_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5861912198807531"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "giga_rho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gigaword with LSA at different dimensions [0.5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might expect PPMI and LSA to form a solid pipeline that combines the strengths of PPMI with those of dimensionality reduction. However, LSA has a hyper-parameter $k$ â€“ the dimensionality of the final representations â€“ that will impact performance. This problem asks you to create code that will help you explore this approach.\n",
    "\n",
    "Your task: write a wrapper function `run_ppmi_lsa_pipeline` that does the following:\n",
    "\n",
    "1. Takes as input a count `pd.DataFrame` and an LSA parameter `k`.\n",
    "1. Reweights the count matrix with PPMI.\n",
    "1. Applies LSA with dimensionality `k`.\n",
    "1. Evaluates this reweighted matrix using `vsm.word_relatedness_evaluation` with `dev_df` as defined above. The return value of `run_ppmi_lsa_pipeline` should be the return value of this call to `vsm.word_relatedness_evaluation`.\n",
    "\n",
    "The goal of this question is to help you get a feel for how LSA can contribute to this problem. \n",
    "\n",
    "The  function `test_run_ppmi_lsa_pipeline` will test your function on the count matrix in `data/vsmdata/giga_window20-flat.csv.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ppmi_lsa_pipeline(count_df, k, rel_df=dev_df):\n",
    "    #reweights count matrix with PPMI\n",
    "    ppmi_df = pmi(count_df)\n",
    "    \n",
    "    #reduce dimensions to k\n",
    "    lsa_df = vsm.lsa(ppmi_df, k=k)\n",
    "    \n",
    "    #evaluate matrices and return rho value\n",
    "    return vsm.word_relatedness_evaluation(rel_df, lsa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run_ppmi_lsa_pipeline(func):\n",
    "    \"\"\"`func` should be `run_ppmi_lsa_pipeline`\"\"\"\n",
    "    giga20 = pd.read_csv(\n",
    "        os.path.join(VSM_HOME, \"giga_window20-flat.csv.gz\"), index_col=0)\n",
    "    pred_df, rho = func(giga20, k=10)\n",
    "    rho = round(rho, 3)\n",
    "    expected = 0.545\n",
    "    assert rho == expected,\\\n",
    "        \"Expected rho of {}; got {}\".format(expected, rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_run_ppmi_lsa_pipeline(run_ppmi_lsa_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "giga20 = pd.read_csv(os.path.join(VSM_HOME, \"giga_window20-flat.csv.gz\"), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [07:06<00:00, 53.25s/it]\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "\n",
    "for k in tqdm(range(180,220,5)):\n",
    "    df, lsa_rho = run_ppmi_lsa_pipeline(giga20,k,rel_df=highest)\n",
    "    scores[k] = np.round(lsa_rho, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: 0.5447,\n",
       " 20: 0.5735,\n",
       " 30: 0.5864,\n",
       " 40: 0.593,\n",
       " 50: 0.5969,\n",
       " 60: 0.5995,\n",
       " 70: 0.602,\n",
       " 80: 0.6037,\n",
       " 90: 0.605,\n",
       " 100: 0.6068}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{120: 0.609, 140: 0.6103, 160: 0.6115, 180: 0.6123, 200: 0.6124, 220: 0.612}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{200: 0.6124, 220: 0.612, 240: 0.6115, 260: 0.6111, 280: 0.6108, 300: 0.6099}"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{180: 0.6242,\n",
       " 185: 0.6242,\n",
       " 190: 0.6241,\n",
       " 195: 0.6242,\n",
       " 200: 0.6244,\n",
       " 205: 0.6245,\n",
       " 210: 0.6244,\n",
       " 215: 0.6241}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-test reweighting [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-test statistic can be thought of as a reweighting scheme. For a count matrix $X$, row index $i$, and column index $j$:\n",
    "\n",
    "$$\\textbf{ttest}(X, i, j) = \n",
    "\\frac{\n",
    "    P(X, i, j) - \\big(P(X, i, *)P(X, *, j)\\big)\n",
    "}{\n",
    "\\sqrt{(P(X, i, *)P(X, *, j))}\n",
    "}$$\n",
    "\n",
    "where $P(X, i, j)$ is $X_{ij}$ divided by the total values in $X$, $P(X, i, *)$ is the sum of the values in row $i$ of $X$ divided by the total values in $X$, and $P(X, *, j)$ is the sum of the values in column $j$ of $X$ divided by the total values in $X$.\n",
    "\n",
    "Your task: implement this reweighting scheme. You can use `test_ttest_implementation` below to check that your implementation is correct.  You do not need to use this for any evaluations, though we hope you will be curious enough to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest(df):\n",
    "    \n",
    "    #compute intermediate steps i.e. row, col, and matrix totals\n",
    "    col_totals = df.sum(axis=0)\n",
    "    row_totals = df.sum(axis=1)\n",
    "    total = col_totals.sum()\n",
    "    assert total == df.sum().sum(), 'Total methods do not match'\n",
    "    assert np.isscalar(total), 'Not a scalar value'\n",
    "    \n",
    "    #compute overall matrix probability\n",
    "    Prob_Xij = df/total    \n",
    "    \n",
    "    #compute individual row and col probabilites and calculate their product\n",
    "    Prob_Xi, Prob_Xj = row_totals/total, col_totals/total\n",
    "    product = np.outer(Prob_Xi, Prob_Xj)\n",
    "    assert product.shape == df.shape, 'final product shape is not the same as original matrix'\n",
    "    \n",
    "    #define numerator and denominator values\n",
    "    num = Prob_Xij - product\n",
    "    denom = np.sqrt(product)\n",
    "    return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ttest_implementation(func):\n",
    "    \"\"\"`func` should be `ttest`\"\"\"\n",
    "    X = pd.DataFrame([\n",
    "        [1.,  4.,  3.,  0.],\n",
    "        [2., 43.,  7., 12.],\n",
    "        [5.,  6., 19.,  0.],\n",
    "        [1., 11.,  1.,  4.]])\n",
    "    actual = np.array([\n",
    "        [ 0.04655, -0.01337,  0.06346, -0.09507],\n",
    "        [-0.11835,  0.13406, -0.20846,  0.10609],\n",
    "        [ 0.16621, -0.23129,  0.38123, -0.18411],\n",
    "        [-0.0231 ,  0.0563 , -0.14549,  0.10394]])\n",
    "    predicted = func(X)\n",
    "    assert np.array_equal(predicted.round(5), actual), \\\n",
    "        \"Your ttest result is\\n{}\".format(predicted.round(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_ttest_implementation(ttest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooled BERT representations [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook [vsm_04_contextualreps.ipynb](vsm_04_contextualreps.ipynb) explores methods for deriving static vector representations of words from the contextual representations given by models like BERT and RoBERTa. The methods are due to [Bommasani et al. 2020](https://www.aclweb.org/anthology/2020.acl-main.431). The simplest of these methods involves processing the words as independent texts and pooling the sub-word representations that result, using a function like mean or max.\n",
    "\n",
    "Your task: write a function `evaluate_pooled_bert` that will enable exploration of this approach. The function should do the following:\n",
    "\n",
    "1. Take as its arguments (a) a word relatedness `pd.DataFrame` `rel_df` (e.g., `dev_df`), (b) a `layer` index (see below), and (c) a `pool_func` value (see below).\n",
    "1. Set up a BERT tokenizer and BERT model based on `'bert-base-uncased'`.\n",
    "1. Use `vsm.create_subword_pooling_vsm` to create a VSM (a `pd.DataFrame`) with the user's values for `layer` and `pool_func`.\n",
    "1. Return the return value of `vsm.word_relatedness_evaluation` using this new VSM, evaluated on `rel_df` with `distfunc` set to its default value.\n",
    "\n",
    "The function `vsm.create_subword_pooling_vsm` does the heavy-lifting. Your task is really just to put these pieces together. The result will be the start of a flexible framework for seeing how these methods do on our task. \n",
    "\n",
    "The function `test_evaluate_pooled_bert` can help you obtain the design we are seeking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "def evaluate_pooled_bert(rel_df, layer, pool_func):\n",
    "    bert_weights_name = 'bert-base-uncased'\n",
    "\n",
    "    # Initialize a BERT tokenizer and BERT model based on\n",
    "    # `bert_weights_name`:\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_weights_name)\n",
    "    model = BertModel.from_pretrained(bert_weights_name)\n",
    "\n",
    "    # Get the vocabulary from `rel_df`:\n",
    "    ##### YOUR CODE HERE\n",
    "    vocab = set(rel_df.word1.values) | set(rel_df.word2.values)\n",
    "    \n",
    "    # Use `vsm.create_subword_pooling_vsm` with the user's arguments:\n",
    "    #### YOUR CODE HERE\n",
    "    pooled_df = vsm.create_subword_pooling_vsm(vocab, tokenizer, model, layer=layer, pool_func=pool_func)\n",
    "    \n",
    "    # Return the results of the relatedness evalution:\n",
    "    ##### YOUR CODE HERE\n",
    "    return vsm.word_relatedness_evaluation(rel_df, pooled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluate_pooled_bert(func):\n",
    "    import torch\n",
    "    rel_df = pd.DataFrame([\n",
    "        {'word1': 'porcupine', 'word2': 'capybara', 'score': 0.6},\n",
    "        {'word1': 'antelope', 'word2': 'springbok', 'score': 0.5},\n",
    "        {'word1': 'llama', 'word2': 'camel', 'score': 0.4},\n",
    "        {'word1': 'movie', 'word2': 'play', 'score': 0.3}])\n",
    "    layer = 2\n",
    "    pool_func = vsm.max_pooling\n",
    "    pred_df, rho = evaluate_pooled_bert(rel_df, layer, pool_func)\n",
    "    rho = round(rho, 2)\n",
    "    expected_rho = 0.40\n",
    "    assert rho == expected_rho, \\\n",
    "        \"Expected rho={}; got rho={}\".format(expected_rho, rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_evaluate_pooled_bert(evaluate_pooled_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learned distance functions [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presentation thus far leads one to assume that the `distfunc` argument used in the experiments will be a standard vector distance function like `vsm.cosine` or `vsm.euclidean`. However, the framework itself simply requires that this function map two fixed-dimensional vectors to a real number. This opens up a world of possibilities. This question asks you to dip a toe in these waters.\n",
    "\n",
    "Your task: write a function `run_knn_score_model` for models in this class. The function should:\n",
    "\n",
    "1. Take as its arguments (a) a VSM dataframe `vsm_df`, (b) a relatedness dataset (e.g., `dev_df`), and (c) a `test_size` value between 0.0 and 1.0 that can be passed directly to `train_test_split` (see below).\n",
    "1. Create a feature matrix `X`: each word pair in `dev_df` should be represented by the concatenation of the vectors for word1 and word2 from `vsm_df`.\n",
    "1. Create a score vector `y`, which is just the `score` column in `dev_df`.\n",
    "1. Split the dataset `(X, y)` into train and test portions using [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "1. Train an [sklearn.neighbors.KNeighborsRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor) model on the train split from step 4, with default hyperparameters.\n",
    "1. Return the value of the `score` method of the trained `KNeighborsRegressor` model on the test split from step 4.\n",
    "\n",
    "The functions `test_knn_feature_matrix` and `knn_represent` will help you test the crucial representational aspects of this.\n",
    "\n",
    "Note: if you decide to apply this approach to our task as part of an original system, recall that `vsm.create_subword_pooling_vsm` returns `-d` where `d` is the value computed by `distfunc`, since it assumes that `distfunc` is a distance value of some kind rather than a relatedness/similarity value. Since most regression models will return positive scores for positive associations, you will probably want to undo this by having your `distfunc` return the negative of its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "def run_knn_score_model(vsm_df, dev_df, test_size=0.20):\n",
    "    \n",
    "    # Complete `knn_feature_matrix` for this step.\n",
    "    X = knn_feature_matrix(vsm_df, dev_df)\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Get the values of the 'score' column in `dev_df`\n",
    "    # and store them in a list or array `y`.\n",
    "    y = dev_df.score.values\n",
    "\n",
    "    # Use `train_test_split` to split (X, y) into train and\n",
    "    # test protions, with `test_size` as the test size.\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X,y,test_size=test_size)\n",
    "\n",
    "    # Instantiate a `KNeighborsRegressor` with default arguments:\n",
    "    KNN = KNeighborsRegressor()\n",
    "    # Fit the model on the training data:\n",
    "    KNN.fit(xtrain, ytrain)\n",
    "\n",
    "    # Return the value of `score` for your model on the test split\n",
    "    # you created above:\n",
    "    score = KNN.predict(xtest, ytest)\n",
    "    return score\n",
    "\n",
    "\n",
    "def knn_feature_matrix(vsm_df, rel_df):\n",
    "    # Complete `knn_represent` and use it to create a feature\n",
    "    # matrix `np.array`:\n",
    "    matrix = rel_df.apply(lambda x: knn_represent(x.word1, x.word2, vsm_df), axis=1)\n",
    "    return matrix.tolist()\n",
    "    \n",
    "def knn_represent(word1, word2, vsm_df):\n",
    "    # Use `vsm_df` to get vectors for `word1` and `word2`\n",
    "    # and concatenate them into a single vector:\n",
    "    w1vec = vsm_df.loc[word1]\n",
    "    w2vec = vsm_df.loc[word2]\n",
    "    return np.hstack((w1vec, w2vec))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_knn_feature_matrix(func):\n",
    "    rel_df = pd.DataFrame([\n",
    "        {'word1': 'w1', 'word2': 'w2', 'score': 0.1},\n",
    "        {'word1': 'w1', 'word2': 'w3', 'score': 0.2}])\n",
    "    vsm_df = pd.DataFrame([\n",
    "        [1, 2, 3.],\n",
    "        [4, 5, 6.],\n",
    "        [7, 8, 9.]], index=['w1', 'w2', 'w3'])\n",
    "    expected = np.array([\n",
    "        [1, 2, 3, 4, 5, 6.],\n",
    "        [1, 2, 3, 7, 8, 9.]])\n",
    "    result = func(vsm_df, rel_df)\n",
    "    assert np.array_equal(result, expected), \\\n",
    "        \"Your `knn_feature_matrix` returns: {}\\nWe expect: {}\".format(\n",
    "        result, expected)\n",
    "\n",
    "def test_knn_represent(func):\n",
    "    vsm_df = pd.DataFrame([\n",
    "        [1, 2, 3.],\n",
    "        [4, 5, 6.],\n",
    "        [7, 8, 9.]], index=['w1', 'w2', 'w3'])\n",
    "    result = func('w1', 'w3', vsm_df)\n",
    "    expected = np.array([1, 2, 3, 7, 8, 9.])\n",
    "    assert np.array_equal(result, expected), \\\n",
    "        \"Your `knn_represent` returns: {}\\nWe expect: {}\".format(\n",
    "        result, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_knn_represent(knn_represent)\n",
    "    test_knn_feature_matrix(knn_feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your original system [3 points]\n",
    "\n",
    "This question asks you to design your own model. You can of course include steps made above (ideally, the above questions informed your system design!), but your model should not be literally identical to any of the above models. Other ideas: retrofitting, autoencoders, GloVe, subword modeling, ... \n",
    "\n",
    "Requirements:\n",
    "\n",
    "1. Your system must work with `vsm.word_relatedness_evaluation`. You are free to specify the VSM and the value of `distfunc`.\n",
    "\n",
    "1. Your code must be self-contained, so that we can work with your model directly in your homework submission notebook. If your model depends on external data or other resources, please submit a ZIP archive containing these resources along with your submission.\n",
    "\n",
    "In the cell below, please provide a brief technical description of your original system, so that the teaching team can gain an understanding of what it does. This will help us to understand your code and analyze all the submissions to identify patterns and strategies. We also ask that you report the best score your system got during development, just to help us understand how systems performed overall.\n",
    "\n",
    "<font color='red'>Please review the descriptions in the following comment and follow the instructions.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE MAKE SURE TO INCLUDE THE FOLLOWING BETWEEN THE START AND STOP COMMENTS:\n",
    "#   1) Textual description of your system.\n",
    "#   2) The code for your original system.\n",
    "#   3) The score achieved by your system in place of MY_NUMBER.\n",
    "#        With no other changes to that line.\n",
    "#        You should report your score as a decimal value <=1.0\n",
    "# PLEASE MAKE SURE NOT TO DELETE OR EDIT THE START AND STOP COMMENTS\n",
    "\n",
    "# NOTE: MODULES, CODE AND DATASETS REQUIRED FOR YOUR ORIGINAL SYSTEM\n",
    "# SHOULD BE ADDED BELOW THE 'IS_GRADESCOPE_ENV' CHECK CONDITION. DOING\n",
    "# SO ABOVE THE CHECK MAY CAUSE THE AUTOGRADER TO FAIL.\n",
    "\n",
    "# START COMMENT: Enter your system description in this cell.\n",
    "# My peak score was: MY_NUMBER\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    pass\n",
    "\n",
    "# STOP COMMENT: Please do not remove this comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bake-off [1 point]\n",
    "\n",
    "For the bake-off, you simply need to evaluate your original system on the file \n",
    "\n",
    "`wordrelatedness/cs224u-wordrelatedness-test-unlabeled.csv`\n",
    "\n",
    "This contains only word pairs (no scores), so `vsm.word_relatedness_evaluation` will simply make predictions without doing any scoring. Use that function to make predictions with your original system, store the resulting `pred_df` to a file, and then upload the file as your bake-off submission.\n",
    "\n",
    "The following function should be used to conduct this evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bakeoff_submission(\n",
    "        vsm_df,\n",
    "        distfunc,\n",
    "        output_filename=\"cs224u-wordrelatedness-bakeoff-entry.csv\"):\n",
    "\n",
    "    test_df = pd.read_csv(\n",
    "        os.path.join(DATA_HOME, \"cs224u-wordrelatedness-test-unlabeled.csv\"))\n",
    "\n",
    "    pred_df, _ = vsm.word_relatedness_evaluation(test_df, vsm_df, distfunc=distfunc)\n",
    "\n",
    "    pred_df.to_csv(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if `count_df` were the VSM for my system, and I wanted my distance function to be `vsm.euclidean`, I would do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This check ensure that the following code only runs on the local environment.\n",
    "# The following call will not be run on the autograder environment.\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    pass\n",
    "    create_bakeoff_submission(count_df, vsm.euclidean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a file `cs224u-wordrelatedness-bakeoff-entry.csv` in the current directory. That file should be uploaded as-is. Please do not change its name.\n",
    "\n",
    "Only one upload per team is permitted, and you should do no tuning of your system based on what you see in `pred_df` â€“ you should not study that file in anyway, beyond perhaps checking that it contains what you expected it to contain. The upload function will do some additional checking to ensure that your file is well-formed.\n",
    "\n",
    "People who enter will receive the additional homework point, and people whose systems achieve the top score will receive an additional 0.5 points. We will test the top-performing systems ourselves, and only systems for which we can reproduce the reported results will win the extra 0.5 points.\n",
    "\n",
    "Late entries will be accepted, but they cannot earn the extra 0.5 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instruction\n",
    "\n",
    "Review and follow the [Homework and bake-off code: Formatting guide](hw_formatting_guide.ipynb).\n",
    "Please do not change the file name as described below.\n",
    "\n",
    "Submit the following files to Gradescope:\n",
    "\n",
    "- `hw_wordrelatedness.ipynb` (this notebook)\n",
    "- `cs224u-wordrelatedness-bakeoff-entry.csv` (bake-off output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
