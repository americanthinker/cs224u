{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3579940b-7a20-472d-bd89-8d3f7474f733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#HuggingFace imports\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers.file_utils import PaddingStrategy\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "#data science imports\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "#cs224u imports\n",
    "import sst, vsm, utils\n",
    "\n",
    "#python standard libraries\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = \"cpu\" if not torch.cuda.is_available() else 'cuda:0'\n",
    "#device = \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d59882d-2825-4b53-961f-43028ce959ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 11 18:06:24 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   29C    P0    33W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093b985a-3d90-4fff-8be4-4d3523dda831",
   "metadata": {},
   "source": [
    "## Assemble Data(sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ef5deb-023d-4d31-881e-7aa805d6b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_path = '/home/americanthinker/notebooks/pytorch/cs224u/data/sentiment/sst3-train.csv'\n",
    "dev_df_path = '/home/americanthinker/notebooks/pytorch/cs224u/data/sentiment/sst3-dev.csv'\n",
    "SST_HOME = os.path.join('data', 'sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70eb67cc-02af-41ed-a944-ebb2ea2d31e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8534 entries, 0 to 8533\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   example_id  8534 non-null   object\n",
      " 1   sentence    8534 non-null   object\n",
      " 2   label       8534 non-null   object\n",
      " 3   is_subtree  8534 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 266.8+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1100 entries, 0 to 1099\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   example_id  1100 non-null   object\n",
      " 1   sentence    1100 non-null   object\n",
      " 2   label       1100 non-null   object\n",
      " 3   is_subtree  1100 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 34.5+ KB\n"
     ]
    }
   ],
   "source": [
    "sst_train = sst.train_reader(SST_HOME, dedup=True)\n",
    "sst_train.info()\n",
    "sst_dev = sst.dev_reader(SST_HOME, dedup=True)\n",
    "sst_dev.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0765bb3-e66e-4c3a-b6fd-f2d47f026a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8544 entries, 0 to 318573\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  8544 non-null   object\n",
      " 1   label     8544 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 200.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#import non-subtree version of training data\n",
    "train_df = pd.read_csv(train_df_path, usecols=['sentence', 'label', 'is_subtree'])\n",
    "train_df = train_df[train_df['is_subtree'] == 0]\n",
    "train_df.drop('is_subtree', axis=1, inplace=True)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87c4ae61-7203-483b-a0e3-4c52399a2121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1101 entries, 0 to 1100\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  1101 non-null   object\n",
      " 1   label     1101 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 25.8+ KB\n"
     ]
    }
   ],
   "source": [
    "dev_df = pd.read_csv(dev_df_path, usecols=['sentence', 'label', 'is_subtree'])\n",
    "dev_df = dev_df[dev_df['is_subtree'] == 0]\n",
    "dev_df.drop('is_subtree', axis=1, inplace=True)\n",
    "dev_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6cc6a78-e89f-4587-bdce-80c8060a171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert text/labels to model consumable input\n",
    "def create_dataset(df: pd.DataFrame) -> np.array:\n",
    "    label_types = sorted(df.label.unique().tolist())\n",
    "    assert ['negative', 'neutral', 'positive'] == label_types\n",
    "    label_map = {label:index for index, label in enumerate(label_types)}\n",
    "    \n",
    "    text = df.sentence.values.tolist()\n",
    "    labels = df.label.apply(lambda x: label_map[x]).values.tolist()\n",
    "    assert len(text) == len(labels)\n",
    "    \n",
    "    return text, labels\n",
    "\n",
    "train_X, train_y = create_dataset(train_df)\n",
    "dev_X, dev_y = create_dataset(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99b9e771-589b-417d-a19e-7d26c97c9664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate bert tokenizer\n",
    "weights_name = 'bert-base-cased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a76979-360e-4228-82ef-2b92dfa99b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#design dataset class for use with DataLoader\n",
    "\n",
    "class getSentences(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_len):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Sentences: {len(self.sentences)}     Labels: {len(self.labels)}'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.sentences))\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        sentence = self.sentences[index]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "                          sentence,\n",
    "                          add_special_tokens=True,\n",
    "                          max_length=self.max_len,\n",
    "                          truncation=True,\n",
    "                          return_token_type_ids=False,\n",
    "                          padding=PaddingStrategy.MAX_LENGTH,\n",
    "                          return_attention_mask=True,\n",
    "                          return_tensors='pt')\n",
    "        \n",
    "        return {'text' : sentence,\n",
    "                'input_id': encoding['input_ids'].flatten(),\n",
    "                'attention_mask':encoding['attention_mask'].flatten(),\n",
    "                'labels': torch.tensor(self.labels[index], dtype = torch.long)\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e52463e-4f3c-4486-b0d1-76b3474ec67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate Dataset and build DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 300\n",
    "NUM_TRAIN_SAMPLES = len(train_X)\n",
    "NUM_VAL_SAMPLES = len(dev_X)\n",
    "\n",
    "training_data = getSentences(\n",
    "                    sentences = train_X,\n",
    "                    labels = train_y,\n",
    "                    tokenizer = bert_tokenizer,\n",
    "                    max_len = MAX_LEN)\n",
    "\n",
    "val_data = getSentences(\n",
    "                sentences = dev_X,\n",
    "                labels = dev_y,\n",
    "                tokenizer = bert_tokenizer,\n",
    "                max_len = MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(training_data, BATCH_SIZE, shuffle = False)\n",
    "val_loader = DataLoader(val_data, BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60eb9d8-d730-4058-a1ac-b90053a97283",
   "metadata": {},
   "source": [
    "#### Smaller datasets for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d74ca23-def6-4c4f-a385-586b94222925",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = train_X[:BATCH_SIZE * 4]\n",
    "dev_subset = dev_X[:BATCH_SIZE * 4]\n",
    "train_y_subset = train_y[:BATCH_SIZE * 4]\n",
    "dev_y_subset = dev_y[:BATCH_SIZE * 4]\n",
    "\n",
    "NUM_TRAIN_SUBSET = len(train_subset)\n",
    "NUM_VAL_SUBSET = len(dev_subset)\n",
    "\n",
    "train_subset_data = getSentences(\n",
    "                    sentences = train_subset,\n",
    "                    labels = train_y_subset,\n",
    "                    tokenizer = bert_tokenizer,\n",
    "                    max_len = MAX_LEN)\n",
    "\n",
    "val_subset_data = getSentences(\n",
    "                sentences = dev_subset,\n",
    "                labels = dev_y_subset,\n",
    "                tokenizer = bert_tokenizer,\n",
    "                max_len = MAX_LEN)\n",
    "\n",
    "train_subset_loader = DataLoader(train_subset_data, BATCH_SIZE, shuffle = False)\n",
    "val_subset_loader = DataLoader(val_subset_data, BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02399438-ac8a-4732-b497-d645ec4d2c41",
   "metadata": {},
   "source": [
    "#### Test sample batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "843d313c-daab-453b-81ee-6b5fc1bbaa4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'input_id', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 300]), torch.Size([16, 300]), torch.Size([16]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#down and dirty sample batch check to ensure everything loaded correctly\n",
    "sample_batch = next(iter(train_loader))\n",
    "assert (sample_batch['input_id'].shape[0], sample_batch['input_id'].shape[1]) == (BATCH_SIZE, MAX_LEN)\n",
    "\n",
    "#view of the data dimensions\n",
    "sample_batch.keys()\n",
    "sample_batch['labels']\n",
    "sample_batch['input_id'].shape, sample_batch['attention_mask'].shape, sample_batch['labels'].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984bdfcd-77ea-48a0-89ea-b68f3cb7a40b",
   "metadata": {},
   "source": [
    "## Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7062a864-e3e3-4974-83ce-23af8298058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input to the model --> encoded_ids and attention mask\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes):\n",
    "        super(BertClassifier,self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(p = 0.3)\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size,num_classes)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "    def forward(self,input_ids, attention_mask):\n",
    "        temp = self.bert(input_ids, attention_mask)  # Here we have added one linear layer on top of \n",
    "        pooled_output = temp[1]                             # BERT-base with number of output = 3 \n",
    "        out = self.dropout(pooled_output)          \n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14c0198a-52cf-47f0-a83b-4eb8b86b6228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 3\n",
    "model = BertClassifier(weights_name, 3)\n",
    "model = model.to(device)\n",
    "model.bert.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5a5c69-dba7-4956-9314-b7fe0a8bbb7d",
   "metadata": {},
   "source": [
    "#### Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "961a0c33-01a0-49b8-80e5-17496df0a119",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument index in method wrapper_index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15321/2408925329.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msample_linear_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msoft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfinal_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_linear_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_15321/398259074.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Here we have added one linear layer on top of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m                             \u001b[0;31m# BERT-base with number of output = 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    990\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument index in method wrapper_index_select)"
     ]
    }
   ],
   "source": [
    "#test forward pass to ensure correct output dims\n",
    "input_ids = sample_batch['input_id']\n",
    "mask = sample_batch['attention_mask']\n",
    "\n",
    "sample_linear_output = model(input_ids,mask)\n",
    "soft = nn.Softmax(dim=1)\n",
    "final_out = soft(sample_linear_output)\n",
    "\n",
    "assert (final_out.shape[0], final_out.shape[1]) == (BATCH_SIZE, num_classes)\n",
    "\n",
    "final_out\n",
    "_, pred_labels = torch.max(final_out, dim=1)\n",
    "pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99066aa9-e9fc-445f-acba-92fa639056f2",
   "metadata": {},
   "source": [
    "## Initiate Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1390c27-018e-42fc-ab8c-bcad12bc216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "EPOCHS = 10\n",
    "\n",
    "steps = len(train_loader) * EPOCHS\n",
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optim = AdamW(params = model.parameters(),lr = learning_rate, correct_bias= False)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer = optim,\n",
    "                                                        num_warmup_steps = 0,\n",
    "                                                        num_training_steps = steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1259dbf9-079c-4b82-a5e6-a7662e85ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader=train_loader, loss_function=loss_fn, optimizer=optim, scheduler=scheduler, n_examples=NUM_TRAIN_SAMPLES):\n",
    "    '''\n",
    "    Model training function that represents one pass through the data.\n",
    "    Returns accuracy and mean total loss.  This function is meant to be \n",
    "    paired with an \"eval model\" function.\n",
    "    '''\n",
    "    \n",
    "    model.train()\n",
    "    batches = len(data_loader)\n",
    "    batch_count = 0\n",
    "    train_loss = []\n",
    "    correct_predictions = 0\n",
    "   \n",
    "    \n",
    "    for d in tqdm(data_loader):\n",
    "        \n",
    "        #grab data in batches and move to GPU\n",
    "        input_ids = d['input_id'].to(device)\n",
    "        masks = d['attention_mask'].to(device)\n",
    "        labels = d['labels'].to(device)\n",
    "        batch_count += 1\n",
    "#         if batch_count % 100 == 0:\n",
    "#             print(f'Batch \"{batch_count}\" of {batches} total batches')\n",
    "        \n",
    "        #forward propagation\n",
    "        predictions = model(input_ids, masks)\n",
    "        loss = loss_function(predictions, labels)\n",
    "        _, pred_classes = torch.max(predictions, dim=1)\n",
    "        \n",
    "        #back propagation\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #collect loss and acc measures\n",
    "        train_loss.append(loss.item())\n",
    "        correct_predictions += torch.sum(pred_classes==labels)\n",
    "    \n",
    "    return (correct_predictions/n_examples).cpu().numpy(), np.mean(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3fe5f92c-6c11-4fa7-8027-cb0257ae5b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader=val_loader, loss_function=loss_fn, n_examples=NUM_VAL_SAMPLES):\n",
    "    '''\n",
    "    Model evaluation function that represents one pass through the data.\n",
    "    Returns accuracy and mean total loss.  This function is meant to be \n",
    "    paired with the \"train_model\" function.\n",
    "    '''\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss = []\n",
    "    correct_predictions = 0\n",
    "    all_predictions = []\n",
    "    all_labels = [] \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in tqdm(data_loader):\n",
    "            \n",
    "            input_ids = d['input_id'].to(device)\n",
    "            masks = d['attention_mask'].to(device)\n",
    "            labels = d['labels'].to(device)\n",
    "            \n",
    "            #forward prop for inference\n",
    "            predictions = model(input_ids, masks)\n",
    "            loss = loss_function(predictions, labels)\n",
    "            _,pred_classes = torch.max(predictions, dim=1)\n",
    "            \n",
    "            #collect preds/labels for class_report\n",
    "            all_predictions.extend(pred_classes.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "            \n",
    "            #collect loss and acc measures\n",
    "            eval_loss.append(loss.item())\n",
    "            correct_predictions += torch.sum(pred_classes==labels)\n",
    "    \n",
    "    report = classification_report(all_labels, \n",
    "                                   all_predictions, \n",
    "                                   labels=[0,1,2], \n",
    "                                   target_names=['negative', 'neutral', 'positive'])\n",
    "    \n",
    "    return (correct_predictions / n_examples).cpu().numpy(), np.mean(eval_loss), report\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1f7a7e28-6c3e-4da2-a091-c9133a32966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, epochs: int=20):\n",
    "    \n",
    "    tracking = defaultdict(list)\n",
    "    best_accuracy = 0\n",
    "    best_report = None\n",
    "    \n",
    "    EPOCHS = epochs\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'epoch : {epoch+1}/{EPOCHS}')\n",
    "        \n",
    "        train_acc, train_loss = train_model(model, data_loader=train_loader, n_examples=NUM_TRAIN_SAMPLES)\n",
    "\n",
    "        val_acc , val_loss, report = eval_model(model, data_loader=val_loader, n_examples=NUM_VAL_SAMPLES)\n",
    "\n",
    "        tracking['train_acc'].append(train_acc)\n",
    "        tracking['train_loss'].append(train_loss)\n",
    "        tracking['val_acc'].append(val_acc)\n",
    "        tracking['val_loss'].append(val_loss)\n",
    "        \n",
    "        scores = np.round([train_loss, train_acc, val_loss, val_acc],3)\n",
    "        print(f'train_loss: {scores[0]}, train_acc: {scores[1]}\\nval_loss: {scores[2]}, val_acc: {scores[3]}')\n",
    "            \n",
    "        if val_acc > best_accuracy:\n",
    "            #best_model_name = f'drive/MyDrive/Bert Sentiment/models/best_model_state_{val_acc}.bin'\n",
    "            #torch.save(model.state_dict(), best_model_name)\n",
    "            best_accuracy = val_acc\n",
    "            best_report = report\n",
    "    \n",
    "    end = time.perf_counter() - start\n",
    "    print(f'Total time for {EPOCHS} epochs: {np.round(end/60, 1)} minutes')\n",
    "    print(best_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "df71daa4-a1ad-4fef-9fbc-71f312088c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 534/534 [04:34<00:00,  1.95it/s]\n",
      "100%|██████████| 69/69 [00:12<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.456, train_acc: 0.822\n",
      "val_loss: 0.759, val_acc: 0.706\n",
      "epoch : 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 534/534 [04:34<00:00,  1.95it/s]\n",
      "100%|██████████| 69/69 [00:12<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.333, train_acc: 0.881\n",
      "val_loss: 0.925, val_acc: 0.717\n",
      "epoch : 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 534/534 [04:34<00:00,  1.95it/s]\n",
      "100%|██████████| 69/69 [00:12<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.237, train_acc: 0.922\n",
      "val_loss: 1.036, val_acc: 0.712\n",
      "epoch : 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 534/534 [04:34<00:00,  1.95it/s]\n",
      "100%|██████████| 69/69 [00:12<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.187, train_acc: 0.944\n",
      "val_loss: 1.242, val_acc: 0.707\n",
      "epoch : 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 534/534 [04:34<00:00,  1.95it/s]\n",
      "100%|██████████| 69/69 [00:12<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.165, train_acc: 0.952\n",
      "val_loss: 1.393, val_acc: 0.694\n",
      "epoch : 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 534/534 [04:33<00:00,  1.95it/s]\n",
      "100%|██████████| 69/69 [00:12<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.145, train_acc: 0.961\n",
      "val_loss: 1.576, val_acc: 0.687\n",
      "epoch : 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 534/534 [04:33<00:00,  1.95it/s]\n",
      "100%|██████████| 69/69 [00:12<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.147, train_acc: 0.963\n",
      "val_loss: 1.633, val_acc: 0.681\n",
      "epoch : 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 534/534 [04:33<00:00,  1.95it/s]\n",
      "100%|██████████| 69/69 [00:12<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.184, train_acc: 0.95\n",
      "val_loss: 1.683, val_acc: 0.678\n",
      "epoch : 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 534/534 [04:34<00:00,  1.95it/s]\n",
      "100%|██████████| 69/69 [00:12<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.614, train_acc: 0.846\n",
      "val_loss: 1.811, val_acc: 0.679\n",
      "epoch : 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 534/534 [04:33<00:00,  1.95it/s]\n",
      "100%|██████████| 69/69 [00:12<00:00,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.522, train_acc: 0.872\n",
      "val_loss: 1.811, val_acc: 0.679\n",
      "Total time for 10 epochs: 47.7 minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.78      0.79       428\n",
      "     neutral       0.43      0.33      0.37       229\n",
      "    positive       0.75      0.86      0.80       444\n",
      "\n",
      "    accuracy                           0.72      1101\n",
      "   macro avg       0.66      0.65      0.65      1101\n",
      "weighted avg       0.70      0.72      0.71      1101\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run(model=model, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f0fb63ca-5c98-43e5-a4c5-cc579481b2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 11 21:54:03 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   30C    P0    33W / 250W |   2807MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     28240      C   ...s/py38_pytorch/bin/python     2805MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f5a0a055-b51b-4bf6-a083-8a65258f71c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb1c1d-8d06-40e8-9140-7bea806ecde4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
