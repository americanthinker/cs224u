{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework and bake-off: Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2021\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [Methodological note](#Methodological-note)\n",
    "1. [Set-up](#Set-up)\n",
    "1. [Train set](#Train-set)\n",
    "1. [Dev sets](#Dev-sets)\n",
    "1. [A softmax baseline](#A-softmax-baseline)\n",
    "1. [RNNClassifier wrapper](#RNNClassifier-wrapper)\n",
    "1. [Error analysis](#Error-analysis)\n",
    "1. [Homework questions](#Homework-questions)\n",
    "  1. [Token-level differences [1 point]](#Token-level-differences-[1-point])\n",
    "  1. [Training on some of the bakeoff data [1 point]](#Training-on-some-of-the-bakeoff-data-[1-point])\n",
    "  1. [A more powerful vector-averaging baseline [2 points]](#A-more-powerful-vector-averaging-baseline-[2-points])\n",
    "  1. [BERT encoding [2 points]](#BERT-encoding-[2-points])\n",
    "  1. [Your original system [3 points]](#Your-original-system-[3-points])\n",
    "1. [Bakeoff [1 point]](#Bakeoff-[1-point])\n",
    "1. [Submission Instruction](#Submission-Instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This homework and associated bakeoff are devoted to supervised sentiment analysis using the ternary (positive/negative/neutral) version of the Stanford Sentiment Treebank (SST-3) as well as a new dev/test dataset drawn from restaurant reviews. Our goal in introducing the new dataset is to push you to create a system that performs well in both the movie and restaurant domains.\n",
    "\n",
    "The homework questions ask you to implement some baseline system, and the bakeoff challenge is to define a system that does well at both the SST-3 test set and the new restaurant test set. Both are ternary tasks, and our central bakeoff score is the mean of the macro-FI scores for the two datasets. This assigns equal weight to all classes and datasets regardless of size.\n",
    "\n",
    "The SST-3 test set will be used for the bakeoff evaluation. This dataset is already publicly distributed, so we are counting on people not to cheat by developing their models on the test set. You must do all your development without using the test set at all, and then evaluate exactly once on the test set and turn in the results, with no further system tuning or additional runs. __Much of the scientific integrity of our field depends on people adhering to this honor code__. \n",
    "\n",
    "One of our goals for this homework and bakeoff is to encourage you to engage in __the basic development cycle for supervised models__, in which you\n",
    "\n",
    "1. Design a new system. We recommend starting with something simple.\n",
    "1. Use `sst.experiment` to evaluate your system, using random train/test splits initially.\n",
    "1. If you have time, compare your system with others using `sst.compare_models` or `utils.mcnemar`. (For discussion, see [this notebook section](sst_02_hand_built_features.ipynb#Statistical-comparison-of-classifier-models).)\n",
    "1. Return to step 1, or stop the cycle and conduct a more rigorous evaluation with hyperparameter tuning and assessment on the `dev` set.\n",
    "\n",
    "[Error analysis](#Error-analysis) is one of the most important methods for steadily improving a system, as it facilitates a kind of human-powered hill-climbing on your ultimate objective. Often, it takes a careful human analyst just a few examples to spot a major pattern that can lead to a beneficial change to the feature representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodological note\n",
    "\n",
    "You don't have to use the experimental framework defined below (based on `sst`). The only constraint we need to place on your system is that it must have a `predict_one` method that can map directly from an example text to a prediction, and it must be able to make predictions without having any information beyond the text. (For example, it can't depend on knowing which task the text comes from.) See [the bakeoff section below](#Bakeoff-[1-point]) for examples of functions that conform to this specification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "See [the first notebook in this unit](sst_01_overview.ipynb#Set-up) for set-up instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n",
    "from torch_tree_nn import TorchTreeNN\n",
    "import sst\n",
    "import utils\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers.file_utils import PaddingStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SST_HOME = os.path.join('data', 'sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our primary train set is the SST-3 train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_train = sst.train_reader(SST_HOME, include_subtrees=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the train set we will use for all the regular homework questions. You are welcome to bring in new datasets for your original system. You are also free to add `include_subtrees=True`. This is very likely to lead to better systems, but it substantially increases the overall size of the dataset (from 8,544 examples to 159,274), which will in turn substantially increase the time it takes to run experiments.\n",
    "\n",
    "See [this notebook](sst_01_overview.ipynb) for additional details of this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two development set. SST3-dev consists of sentences from movie reviews, just like SST-3 train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_dev = sst.dev_reader(SST_HOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new bakeoff dev set consists of sentences from restaurant reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bakeoff_dev = sst.bakeoff_dev_reader(SST_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'example_id': 57,\n",
       "  'sentence': 'I would recommend that you make reservations in advance.',\n",
       "  'label': 'neutral',\n",
       "  'is_subtree': 0},\n",
       " {'example_id': 590,\n",
       "  'sentence': 'We were welcomed warmly.',\n",
       "  'label': 'positive',\n",
       "  'is_subtree': 0},\n",
       " {'example_id': 1968,\n",
       "  'sentence': 'We have been to Oceanaire twice in the last 6 weeks.',\n",
       "  'label': 'neutral',\n",
       "  'is_subtree': 0}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bakeoff_dev.sample(3, random_state=1).to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the label distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    0.403270\n",
       "negative    0.388738\n",
       "neutral     0.207993\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "neutral     0.431597\n",
       "positive    0.329098\n",
       "negative    0.239305\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst_dev.label.value_counts(normalize=True)\n",
    "bakeoff_dev.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label distribution for the corresponding test set is similar to this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A softmax baseline\n",
    "\n",
    "This example is here mainly as a reminder of how to use our experimental framework with linear models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigrams_phi(text):\n",
    "    return Counter(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thin wrapper around `LogisticRegression` for the sake of `sst.experiment`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_softmax_classifier(X, y):\n",
    "    mod = LogisticRegression(\n",
    "        fit_intercept=True,\n",
    "        solver='liblinear',\n",
    "        multi_class='ovr')\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experimental run with some notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessment dataset 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.628     0.689     0.657       428\n",
      "     neutral      0.343     0.153     0.211       229\n",
      "    positive      0.629     0.750     0.684       444\n",
      "\n",
      "    accuracy                          0.602      1101\n",
      "   macro avg      0.533     0.531     0.518      1101\n",
      "weighted avg      0.569     0.602     0.575      1101\n",
      "\n",
      "Assessment dataset 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.272     0.690     0.390       565\n",
      "     neutral      0.429     0.113     0.179      1019\n",
      "    positive      0.409     0.346     0.375       777\n",
      "\n",
      "    accuracy                          0.328      2361\n",
      "   macro avg      0.370     0.383     0.315      2361\n",
      "weighted avg      0.385     0.328     0.294      2361\n",
      "\n",
      "Mean of macro-F1 scores: 0.416\n"
     ]
    }
   ],
   "source": [
    "softmax_experiment = sst.experiment(\n",
    "    sst_train,   # Train on any data you like except SST-3 test!\n",
    "    unigrams_phi,                 # Free to write your own!\n",
    "    fit_softmax_classifier,       # Free to write your own!\n",
    "    assess_dataframes=[sst_dev, bakeoff_dev]) # Free to change this during development!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`softmax_experiment` contains a lot of information that you can use for error analysis; see [this section below](#Error-analysis) for starter code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNClassifier wrapper\n",
    "\n",
    "This section illustrates how to use `sst.experiment` with `TorchRNNClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To featurize examples for an RNN, we can just get the words in order, letting the model take care of mapping them into an embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_phi(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model wrapper gets the vocabulary using `sst.get_vocab`. If you want to use pretrained word representations in here, then you can have `fit_rnn_classifier` build that space too; see [this notebook section for details](sst_03_neural_networks.ipynb#Pretrained-embeddings). See also [torch_model_base.py](torch_model_base.py) for details on the many optimization parameters that `TorchRNNClassifier` accepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rnn_classifier(X, y):\n",
    "    sst_glove_vocab = utils.get_vocab(X, mincount=2)\n",
    "    mod = TorchRNNClassifier(\n",
    "        sst_glove_vocab,\n",
    "        early_stopping=True,\n",
    "        device=device)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 37. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 2.1222076565027237"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessment dataset 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.538     0.720     0.616       428\n",
      "     neutral      0.293     0.170     0.215       229\n",
      "    positive      0.649     0.579     0.612       444\n",
      "\n",
      "    accuracy                          0.549      1101\n",
      "   macro avg      0.494     0.490     0.481      1101\n",
      "weighted avg      0.532     0.549     0.531      1101\n",
      "\n",
      "Assessment dataset 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.290     0.632     0.397       565\n",
      "     neutral      0.457     0.275     0.343      1019\n",
      "    positive      0.458     0.304     0.365       777\n",
      "\n",
      "    accuracy                          0.370      2361\n",
      "   macro avg      0.402     0.403     0.369      2361\n",
      "weighted avg      0.417     0.370     0.363      2361\n",
      "\n",
      "Mean of macro-F1 scores: 0.425\n"
     ]
    }
   ],
   "source": [
    "rnn_experiment = sst.experiment(\n",
    "    sst.train_reader(SST_HOME),\n",
    "    rnn_phi,\n",
    "    fit_rnn_classifier,\n",
    "    vectorize=False,  # For deep learning, use `vectorize=False`.\n",
    "    assess_dataframes=[sst_dev, bakeoff_dev])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis\n",
    "\n",
    "This section begins to build an error-analysis framework using the dicts returned by `sst.experiment`. These have the following structure:\n",
    "\n",
    "```\n",
    "'model': trained model\n",
    "'phi': the feature function used\n",
    "'train_dataset':\n",
    "   'X': feature matrix\n",
    "   'y': list of labels\n",
    "   'vectorizer': DictVectorizer,\n",
    "   'raw_examples': list of raw inputs, before featurizing   \n",
    "'assess_datasets': list of datasets, each with the same structure as the value of 'train_dataset'\n",
    "'predictions': list of lists of predictions on the assessment datasets\n",
    "'metric': `score_func.__name__`, where `score_func` is an `sst.experiment` argument\n",
    "'score': the `score_func` score on the each of the assessment dataasets\n",
    "```\n",
    "The following function just finds mistakes, and returns a `pd.DataFrame` for easy subsequent processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_errors(experiment):\n",
    "    \"\"\"Find mistaken predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    experiment : dict\n",
    "        As returned by `sst.experiment`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for i, dataset in enumerate(experiment['assess_datasets']):\n",
    "        df = pd.DataFrame({\n",
    "            'raw_examples': dataset['raw_examples'],\n",
    "            'predicted': experiment['predictions'][i],\n",
    "            'gold': dataset['y']})\n",
    "        df['correct'] = df['predicted'] == df['gold']\n",
    "        df['dataset'] = i\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_analysis = find_errors(softmax_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_analysis = find_errors(rnn_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we merge the sotmax and RNN experiments into a single DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = softmax_analysis.merge(\n",
    "    rnn_analysis, left_on='raw_examples', right_on='raw_examples')\n",
    "\n",
    "analysis = analysis.drop('gold_y', axis=1).rename(columns={'gold_x': 'gold'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code collects a specific subset of examples; small modifications to its structure will give you different interesting subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples where the softmax model is correct, the RNN is not,\n",
    "# and the gold label is 'positive'\n",
    "\n",
    "error_group = analysis[\n",
    "    (analysis['predicted_x'] == analysis['gold'])\n",
    "    &\n",
    "    (analysis['predicted_y'] != analysis['gold'])\n",
    "    &\n",
    "    (analysis['gold'] == 'positive')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_examples</th>\n",
       "      <th>predicted_x</th>\n",
       "      <th>gold</th>\n",
       "      <th>correct_x</th>\n",
       "      <th>dataset_x</th>\n",
       "      <th>predicted_y</th>\n",
       "      <th>correct_y</th>\n",
       "      <th>dataset_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And if you 're not nearly moved to tears by a ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Uses sharp humor and insight into human nature...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Unlike the speedy wham-bam effect of most Holl...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The band 's courage in the face of official re...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Although German cooking does not come readily ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3368</th>\n",
       "      <td>If you go to Maui and can only eat at one rest...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>I went here for Valentines Day and heard good ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3380</th>\n",
       "      <td>The best part of the meyer lemon curd filled f...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3410</th>\n",
       "      <td>Great Location, Very nice atmosphere.</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3435</th>\n",
       "      <td>Nevertheless, my wife loved her salad.</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>231 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           raw_examples predicted_x      gold  \\\n",
       "2     And if you 're not nearly moved to tears by a ...    positive  positive   \n",
       "4     Uses sharp humor and insight into human nature...    positive  positive   \n",
       "10    Unlike the speedy wham-bam effect of most Holl...    positive  positive   \n",
       "12    The band 's courage in the face of official re...    positive  positive   \n",
       "13    Although German cooking does not come readily ...    positive  positive   \n",
       "...                                                 ...         ...       ...   \n",
       "3368  If you go to Maui and can only eat at one rest...    positive  positive   \n",
       "3377  I went here for Valentines Day and heard good ...    positive  positive   \n",
       "3380  The best part of the meyer lemon curd filled f...    positive  positive   \n",
       "3410              Great Location, Very nice atmosphere.    positive  positive   \n",
       "3435             Nevertheless, my wife loved her salad.    positive  positive   \n",
       "\n",
       "      correct_x  dataset_x predicted_y  correct_y  dataset_y  \n",
       "2          True          0    negative      False          0  \n",
       "4          True          0    negative      False          0  \n",
       "10         True          0    negative      False          0  \n",
       "12         True          0     neutral      False          0  \n",
       "13         True          0    negative      False          0  \n",
       "...         ...        ...         ...        ...        ...  \n",
       "3368       True          1    negative      False          1  \n",
       "3377       True          1    negative      False          1  \n",
       "3380       True          1    negative      False          1  \n",
       "3410       True          1     neutral      False          1  \n",
       "3435       True          1     neutral      False          1  \n",
       "\n",
       "[231 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "The movie 's relatively simple plot and uncomplicated morality play well with the affable cast .\n",
      "======================================================================\n",
      "Have visited restaurant in Naples several times and steaks are outstanding.\n",
      "======================================================================\n",
      "The Sunday Brunch at Zefferinos is by far the best brunch at which I have ever dined in my life of 55 years.\n"
     ]
    }
   ],
   "source": [
    "for ex in error_group['raw_examples'].sample(3, random_state=1):\n",
    "    print(\"=\"*70)\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework questions\n",
    "\n",
    "Please embed your homework responses in this notebook, and do not delete any cells from the notebook. (You are free to add as many cells as you like as part of your responses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token-level differences [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can begin to get a sense for how our two dev sets differ by considering the most frequent tokens from each. This question asks you to begin such analysis.\n",
    "\n",
    "Your task: write a function `get_token_counts` that, given a `pd.DataFrame` in the format of our datasets, tokenizes the example sentences based on whitespace and creates a count distribution over all of the tokens. The function should return a `pd.Series` sorted by frequency; if you create a count dictionary `d`, then `pd.Series(d).sort_values(ascending=False)` will give you what you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_counts(df: pd.DataFrame) -> pd.Series:\n",
    "    '''\n",
    "    Given a df with a \"sentence\" col, returns a sorted Series of word counts of entire sentence col vocab.\n",
    "    '''\n",
    "    #coerce to arrays\n",
    "    sentences = df.sentence.values\n",
    "    \n",
    "    #join all sentences together and count\n",
    "    tokens = Counter(' '.join(sentences).split())\n",
    "    \n",
    "    #coerce to Series and sort\n",
    "    sorted_tokens = pd.Series(tokens).sort_values(ascending=False)\n",
    "                              \n",
    "    return sorted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_token_counts(func):\n",
    "    df = pd.DataFrame([\n",
    "        {'sentence': 'a a b'},\n",
    "        {'sentence': 'a b a'},\n",
    "        {'sentence': 'a a a b.'}])\n",
    "    result = func(df)\n",
    "    for token, expected in (('a', 7), ('b', 2), ('b.', 1)):\n",
    "        actual = result.loc[token]\n",
    "        assert actual == expected, \\\n",
    "            \"For token {}, expected {}; got {}\".format(\n",
    "            token, expected, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_get_token_counts(get_token_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you develop your original system, you might review these results. The two dev sets have different vocabularies and different low-level encoding details that are sure to impact model performance, especially when one considers that the train set is like `sst_dev` in all these respects. For additional discussion, see [this notebook section](sst_01_overview.ipynb#Tokenization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on some of the bakeoff data [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have so far presented the bakeoff dev set as purely for evaluation. Since the train set consists entirely of SST-3 data, this makes the bakeoff split especially challenging. We might be able to reduce the challenging by adding some of the bakeoff dev set to the train set, keeping some of it for evaluation. The current question asks to begin explore the effects of such training.\n",
    "\n",
    "Your task: write a function `run_mixed_training_experiment`. The function should:\n",
    "\n",
    "1. Take as inputs (a) a model training wrapper like `fit_softmax_classifier` and (b) an integer `bakeoff_train_size` specifying the number of examples from `bakeoff_dev` that should be included in the train set.\n",
    "1. Split `bakeoff_dev` so that the first `bakeoff_train_size` examples are in the train set and the rest are used for evaluation.\n",
    "1. Use `sst.experiment` with the user-supplied model training wrapper, `unigram_phi` as defined above, and a train set that consists of SST-3 train and the train portion of `bakeoff_dev` as defined in step 2. The value of `assess_dataframes` should be a list consisting of the SST-3 dev set and the evaluation portion of `bakeoff_dev` as defined in step 2.\n",
    "1. Return the return value of `sst.experiment`.\n",
    "\n",
    "The function `test_run_mixed_training_experiment` will help you iterate to the required design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mixed_training_experiment(wrapper_func, bakeoff_train_size, vectorize=True):\n",
    "    \n",
    "    #split bakeoff_dev into train and dev\n",
    "    bakeoff_dev_train, bakeoff_dev_dev = bakeoff_dev.iloc[:bakeoff_train_size,:], bakeoff_dev.iloc[bakeoff_train_size:,:]\n",
    "    \n",
    "    #append training df's together\n",
    "    sst_train = sst.train_reader(SST_HOME)\n",
    "    combined_train = sst_train.append(bakeoff_dev_train, ignore_index=True)\n",
    "    \n",
    "    #grab sst-3 dev, if not already defined in notebook\n",
    "    sst_dev = sst.dev_reader(SST_HOME)\n",
    "    \n",
    "    #run experiment\n",
    "    experiment_results = sst.experiment(\n",
    "                            combined_train,   \n",
    "                            unigrams_phi,                 \n",
    "                            wrapper_func,      \n",
    "                            assess_dataframes=[sst_dev, bakeoff_dev_dev],\n",
    "                            vectorize=vectorize)\n",
    "    return experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run_mixed_training_experiment(func):\n",
    "    bakeoff_train_size = 1000\n",
    "    experiment = func(fit_softmax_classifier, bakeoff_train_size)\n",
    "\n",
    "    assess_size = len(experiment['assess_datasets'])\n",
    "    assert len(experiment['assess_datasets']) == 2, \\\n",
    "        (\"The evaluation should be done on two datasets: \"\n",
    "         \"SST3 and part of the bakeoff dev set. \"\n",
    "         \"You have {} datasets.\".format(assess_size))\n",
    "\n",
    "    bakeoff_test_size = bakeoff_dev.shape[0] - bakeoff_train_size\n",
    "    expected_eval_examples = bakeoff_test_size + sst_dev.shape[0]\n",
    "    eval_examples = sum(len(d['raw_examples']) for d in experiment['assess_datasets'])\n",
    "    assert expected_eval_examples == eval_examples, \\\n",
    "        \"Expected {} evaluation examples; got {}\".format(\n",
    "        expected_eval_examples, eval_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessment dataset 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.627     0.671     0.648       428\n",
      "     neutral      0.319     0.162     0.214       229\n",
      "    positive      0.638     0.757     0.692       444\n",
      "\n",
      "    accuracy                          0.599      1101\n",
      "   macro avg      0.528     0.530     0.518      1101\n",
      "weighted avg      0.567     0.599     0.576      1101\n",
      "\n",
      "Assessment dataset 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.471     0.412     0.440       320\n",
      "     neutral      0.588     0.590     0.589       612\n",
      "    positive      0.503     0.548     0.525       429\n",
      "\n",
      "    accuracy                          0.535      1361\n",
      "   macro avg      0.521     0.517     0.518      1361\n",
      "weighted avg      0.534     0.535     0.534      1361\n",
      "\n",
      "Mean of macro-F1 scores: 0.518\n"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_run_mixed_training_experiment(run_mixed_training_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more powerful vector-averaging baseline [2 points]\n",
    "\n",
    "In [Distributed representations as features](sst_03_neural_networks.ipynb#Distributed-representations-as-features), we looked at a baseline for the ternary SST-3 problem in which each example is modeled as the mean of its GloVe representations. A `LogisticRegression` model was used for prediction. A neural network might do better with these representations, since there might be complex relationships between the input feature dimensions that a linear classifier can't learn. To address this question, we want to get set up to run the experiment with a shallow neural classifier. \n",
    "\n",
    "Your task: write and submit a model wrapper function around `TorchShallowNeuralClassifier`. This function should implement hyperparameter search according to this specification:\n",
    "\n",
    "* Set `early_stopping=True` for all experiments.\n",
    "* Using 3-fold cross-validation, exhaustively explore this set of hyperparameter combinations:\n",
    "  * The hidden dimensionality at 50, 100, and 200.\n",
    "  * The hidden activation function as `nn.Tanh()` and `nn.ReLU()`.\n",
    "* For all other parameters to `TorchShallowNeuralClassifier`, use the defaults.\n",
    "\n",
    "See [this notebook section](sst_02_hand_built_features.ipynb#Hyperparameter-search) for examples. You are not required to run a full evaluation with this function using `sst.experiment`, but we assume you will want to.\n",
    "\n",
    "We're not evaluating the quality of your model. (We've specified the protocols completely, but there will still be variation in the results.) However, the primary goal of this question is to get you thinking more about this strong baseline feature representation scheme for SST-3, so we're sort of hoping you feel compelled to try out variations on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "\n",
    "def fit_shallow_neural_classifier_with_hyperparameter_search(X, y):\n",
    "    model = TorchShallowNeuralClassifier(early_stopping=True, device=device)\n",
    "    cv = 3\n",
    "    param_grid = {\n",
    "        'hidden_dim': [50,100,200],\n",
    "        'hidden_activation':[nn.Tanh(), nn.ReLU()]}\n",
    "    best_model = utils.fit_classifier_with_hyperparameter_search(X, y, model, cv, param_grid=param_grid)\n",
    "        \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 31. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 0.38966774195432663"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'hidden_activation': ReLU(), 'hidden_dim': 100}\n",
      "Best score: 0.519\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.624     0.659     0.641       428\n",
      "     neutral      0.266     0.148     0.190       229\n",
      "    positive      0.641     0.752     0.692       444\n",
      "\n",
      "    accuracy                          0.590      1101\n",
      "   macro avg      0.510     0.520     0.508      1101\n",
      "weighted avg      0.556     0.590     0.568      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best = sst.experiment(sst_train, \n",
    "                      unigrams_phi,\n",
    "                      fit_shallow_neural_classifier_with_hyperparameter_search,\n",
    "                      assess_dataframes=sst_dev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_nn_classifier(X, y):\n",
    "    mod = TorchShallowNeuralClassifier(\n",
    "        hidden_dim=200,\n",
    "        early_stopping=True,      # A basic early stopping set-up.\n",
    "        validation_fraction=0.1,  # If no improvement on the\n",
    "        tol=1e-5,                 # validation set is seen within\n",
    "        n_iter_no_change=10)      # `n_iter_no_change`, we stop.\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT encoding [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might hypothesize that encoding our examples with BERT will yield improvements over the GloVe averaging method explored in the previous question, since BERT implements a much more complex and data-driven function for this kind of combination. This question asks you to begin exploring this general hypothesis.\n",
    "\n",
    "Your task: write a function `hf_cls_phi` that uses Hugging Face functionality to encode individual examples with BERT and returns the final output representation above the [CLS] token.\n",
    "\n",
    "You are not required to evaluate this feature function, but it is easy to do so with `sst.experiment` and `vectorize=False` (since your feature function directly encodes every example as a vector). Your code should also be a natural basis for even more powerful approaches – for example, it might be even better to pool all the output states rather than using just the first output state. Another option is [fine-tuning](finetuning.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import vsm\n",
    "\n",
    "# Instantiate a Bert model and tokenizer based on `bert_weights_name`:\n",
    "bert_weights_name = 'bert-base-uncased'\n",
    "\n",
    "model = BertModel.from_pretrained(bert_weights_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_weights_name)\n",
    "\n",
    "def hf_cls_phi(text):\n",
    "    # Get the ids. `vsm.hf_encode` will help; be sure to\n",
    "    # set `add_special_tokens=True`.\n",
    "    encode = vsm.hf_encode(text, tokenizer, add_special_tokens=True)\n",
    "    \n",
    "    # Get the BERT representations. `vsm.hf_represent` will help:\n",
    "    reps = vsm.hf_represent(encode, model)\n",
    "\n",
    "    # Index into `reps` to get the representation above [CLS].\n",
    "    # The shape of `reps` should be (1, n, 768), where n is the\n",
    "    # number of tokens. You need the 0th element of the 2nd dim:\n",
    "    cls_rep = reps[:,0,:][0]\n",
    "\n",
    "    # These conversions should ensure that you can work with the\n",
    "    # representations flexibly. Feel free to change the variable\n",
    "    # name:\n",
    "    return cls_rep.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_cls_phi(\"The dog ate my homework\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hf_cls_phi(func):\n",
    "    rep = func(\"Just testing!\")\n",
    "\n",
    "    expected_shape = (768,)\n",
    "    result_shape = rep.shape\n",
    "    assert rep.shape == (768,), \\\n",
    "        \"Expected shape {}; got {}\".format(\n",
    "        expected_shape, result_shape)\n",
    "\n",
    "    # String conversion to avoid precision errors:\n",
    "    expected_first_val = str(0.1709)\n",
    "    result_first_val = \"{0:.04f}\".format(rep[0])\n",
    "\n",
    "    assert expected_first_val == result_first_val, \\\n",
    "        (\"Unexpected representation values. Expected the \"\n",
    "        \"first value to be {}; got {}\".format(\n",
    "            expected_first_val, result_first_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    test_hf_cls_phi(hf_cls_phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: encoding all of SST-3 train (no subtrees) takes about 11 minutes on my 2015 iMac, CPU only (32GB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your original system [3 points]\n",
    "\n",
    "Your task is to develop an original model for the SST-3 problem and our new bakeoff dataset. There are many options. If you spend more than a few hours on this homework problem, you should consider letting it grow into your final project! Here are some relatively manageable ideas that you might try:\n",
    "\n",
    "1. We didn't systematically evaluate the `bidirectional` option to the `TorchRNNClassifier`. Similarly, that model could be tweaked to allow multiple LSTM layers (at present there is only one), and you could try adding layers to the classifier portion of the model as well.\n",
    "\n",
    "1. We've already glimpsed the power of rich initial word representations, and later in the course we'll see that smart initialization usually leads to a performance gain in NLP, so you could perhaps achieve a winning entry with a simple model that starts in a great place.\n",
    "\n",
    "1. Our [practical introduction to contextual word representations](finetuning.ipynb) covers pretrained representations and interfaces that are likely to boost the performance of any system.\n",
    "\n",
    "We want to emphasize that this needs to be an __original__ system. It doesn't suffice to download code from the Web, retrain, and submit. You can build on others' code, but you have to do something new and meaningful with it. See the course website for additional guidance on how original systems will be evaluated.\n",
    "\n",
    "In the cell below, please provide a brief technical description of your original system, so that the teaching team can gain an understanding of what it does. This will help us to understand your code and analyze all the submissions to identify patterns and strategies.  We also ask that you report the best score your system got during development (your best average of macro-F1 scores), just to help us understand how systems performed overall.\n",
    "\n",
    "<font color='red'>Please review the descriptions in the following comment and follow the instructions.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. I started by utilizing the sst.experiment framework as provided by the course instructor.  However, after running several experiments, including multiple data augmentation\\ntechniques, I did not get remarkable improvements to the combined mean f1_macros scores.  I therefore decided to try to finetune an existing BERT model from the HF library.\\n\\n2. I did not have a lot of time to try out several different models (i.e. RoBERTa, Distil-BERT, etc.) and after a few trial runs, I realized that experimentation was going to \\nrequire a multi-GPU machine.  I had never used the nn.DataParallel framework before, so I figured I would try it out for this bake-off. \\n\\n3. I ended up creating a simple Classification model that uses the last layer of the pretained BERT model linearly connected to an output layer with dropout to improve generalization.  \\nI started with bert-base which worked well, but saw a 5-point F1_score jump by using a bert-large model.  Training time for a 20K sentence dataset at a batch size of 32 took 30-40 minutes \\nto complete 3 epochs.  GPU capacity was definitely a limiting factor in the fine-tune training and evaluation of these models.  \\n\\n4. I ended up combining dev datasets from SST3, Bake off (restaurant reviews) and a random 5,000 sample of the Dynasent dataset.  If I had more time I would have done a better job\\ndoing error analysis.  One thought I had that I did not have time to implement was creating augmented sentences from the known errors of the model and feeding them back into the system\\nto force the model to recognize where it was messing up and correcting itself.  I evaluated on the f1_scores attained on both the sst3-dev and bakeoff-dev datasets. \\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): BertClassifier(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(28996, 1024, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 1024)\n",
       "        (token_type_embeddings): Embedding(2, 1024)\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (12): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (13): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (14): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (15): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (16): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (17): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (18): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (19): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (20): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (21): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (22): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (23): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (linear): Linear(in_features=1024, out_features=3, bias=True)\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4136/1678078595.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m     \u001b[0mtracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;31m# STOP COMMENT: Please do not remove this comment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4136/1678078595.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model, epochs)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'epoch : {epoch+1}/{EPOCHS}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_TRAIN_SAMPLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0mval_acc\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_VAL_SAMPLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mbakeoff_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbakeoff_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbakeoff_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbakeoff_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_bake_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_BAKE_SAMPLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4136/1678078595.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data_loader, loss_function, optimizer, scheduler, n_examples)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m#grab data in batches and move to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py38_pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4136/1678078595.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0;34m'input_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                     \u001b[0;34m'labels'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                    }\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "# PLEASE MAKE SURE TO INCLUDE THE FOLLOWING BETWEEN THE START AND STOP COMMENTS:\n",
    "#   1) Textual description of your system.\n",
    "#   2) The code for your original system.\n",
    "#   3) The score achieved by your system in place of MY_NUMBER.\n",
    "#        With no other changes to that line.\n",
    "#        You should report your score as a decimal value <=1.0\n",
    "# PLEASE MAKE SURE NOT TO DELETE OR EDIT THE START AND STOP COMMENTS\n",
    "\n",
    "# NOTE: MODULES, CODE AND DATASETS REQUIRED FOR YOUR ORIGINAL SYSTEM\n",
    "# SHOULD BE ADDED BELOW THE 'IS_GRADESCOPE_ENV' CHECK CONDITION. DOING\n",
    "# SO ABOVE THE CHECK MAY CAUSE THE AUTOGRADER TO FAIL.\n",
    "\n",
    "# START COMMENT: Enter your system description in this cell.\n",
    "'''\n",
    "1. I started by utilizing the sst.experiment framework as provided by the course instructor.  However, after running several experiments, including multiple data augmentation\n",
    "techniques, I did not get remarkable improvements to the combined mean f1_macros scores.  I therefore decided to try to finetune an existing BERT model from the HF library.\n",
    "\n",
    "2. I did not have a lot of time to try out several different models (i.e. RoBERTa, Distil-BERT, etc.) and after a few trial runs, I realized that experimentation was going to \n",
    "require a multi-GPU machine.  I had never used the nn.DataParallel framework before, so I figured I would try it out for this bake-off. \n",
    "\n",
    "3. I ended up creating a simple Classification model that uses the last layer of the pretained BERT model linearly connected to an output layer with dropout to improve generalization.  \n",
    "I started with bert-base which worked well, but saw a 5-point F1_score jump by using a bert-large model.  Training time for a 20K sentence dataset at a batch size of 32 took 30-40 minutes \n",
    "to complete 3 epochs.  GPU capacity was definitely a limiting factor in the fine-tune training and evaluation of these models.  \n",
    "\n",
    "4. I ended up combining dev datasets from SST3, Bake off (restaurant reviews) and a random 5,000 sample of the Dynasent dataset.  If I had more time I would have done a better job\n",
    "doing error analysis.  One thought I had that I did not have time to implement was creating augmented sentences from the known errors of the model and feeding them back into the system\n",
    "to force the model to recognize where it was messing up and correcting itself.  I evaluated on the f1_scores attained on both the sst3-dev and bakeoff-dev datasets. \n",
    "'''\n",
    "# My peak score was: 0.741\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    #pytorch imports\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "    #HuggingFace imports\n",
    "    from transformers import BertModel, BertTokenizer\n",
    "    from transformers.file_utils import PaddingStrategy\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    #data science imports\n",
    "    from sklearn.metrics import classification_report, f1_score\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    #cs224u imports\n",
    "    import sst, vsm, utils\n",
    "\n",
    "    #python standard libraries\n",
    "    from collections import defaultdict\n",
    "    import time\n",
    "    import os\n",
    "    \n",
    "    #label GPU device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    #instantiate bert tokenizer\n",
    "    weights_name = 'bert-large-cased'\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(weights_name)\n",
    "    \n",
    "    #function for converting pd.DataFrame data into array-like (X,y) values\n",
    "    def create_dataset(df: pd.DataFrame) -> np.array:\n",
    "        label_types = sorted(df.label.unique().tolist())\n",
    "        assert ['negative', 'neutral', 'positive'] == label_types\n",
    "        label_map = {label:index for index, label in enumerate(label_types)}\n",
    "\n",
    "        text = df.sentence.values.tolist()\n",
    "        labels = df.label.apply(lambda x: label_map[x]).values.tolist()\n",
    "        assert len(text) == len(labels)\n",
    "\n",
    "        return text, labels\n",
    "    \n",
    "    #blueprint for use with torch.DataLoader Class\n",
    "    class getSentences(Dataset):\n",
    "        '''\n",
    "        Given an X and Y input, encodes these values for model consumption,\n",
    "        and is the precursor function for the DataLoader.\n",
    "        \n",
    "        Returns a dict of values.\n",
    "        '''\n",
    "        def __init__(self, sentences, labels, tokenizer, max_len):\n",
    "            self.sentences = sentences\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_len = max_len\n",
    "\n",
    "        def __repr__(self):\n",
    "            return f'Sentences: {len(self.sentences)}     Labels: {len(self.labels)}'\n",
    "\n",
    "        def __len__(self):\n",
    "            return (len(self.sentences))\n",
    "\n",
    "        def __getitem__(self,index):\n",
    "            sentence = self.sentences[index]\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                              sentence,\n",
    "                              add_special_tokens=True,\n",
    "                              max_length=self.max_len,\n",
    "                              truncation=True,\n",
    "                              return_token_type_ids=False,\n",
    "                              padding=PaddingStrategy.MAX_LENGTH,\n",
    "                              return_attention_mask=True,\n",
    "                              return_tensors='pt')\n",
    "\n",
    "            return {'text' : sentence,\n",
    "                    'input_id': encoding['input_ids'].flatten(),\n",
    "                    'attention_mask':encoding['attention_mask'].flatten(),\n",
    "                    'labels': torch.tensor(self.labels[index], dtype = torch.long)\n",
    "                   }\n",
    "    ######################################################################################################################\n",
    "    #in place of real dataset I am passing string values so that autograder won't fail\n",
    "    train_X, train_y, dev_X, dev_y, dev_bake_X, dev_bake_y = 'trainx', 'trainy', 'devx', 'devy', 'dev_bakex', 'dev_bakey'\n",
    "    ######################################################################################################################\n",
    "    \n",
    "    #set user-defined constants\n",
    "    BATCH_SIZE = 64\n",
    "    MAX_LEN = 300\n",
    "    NUM_TRAIN_SAMPLES = len(train_X)\n",
    "    NUM_VAL_SAMPLES = len(dev_X)\n",
    "    NUM_BAKE_SAMPLES = len(dev_bake_X)\n",
    "    \n",
    "    #encoded inputs for DataLoader\n",
    "    #Training Data\n",
    "    #Two sets of dev data, one for SST3 and one for Bake-off\n",
    "    training_data = getSentences(\n",
    "                        sentences = train_X,\n",
    "                        labels = train_y,\n",
    "                        tokenizer = bert_tokenizer,\n",
    "                        max_len = MAX_LEN)\n",
    "\n",
    "    val_data = getSentences(\n",
    "                    sentences = dev_X,\n",
    "                    labels = dev_y,\n",
    "                    tokenizer = bert_tokenizer,\n",
    "                    max_len = MAX_LEN)\n",
    "\n",
    "    val_bake_data = getSentences(\n",
    "                    sentences = dev_bake_X,\n",
    "                    labels = dev_bake_y,\n",
    "                    tokenizer = bert_tokenizer,\n",
    "                    max_len = MAX_LEN)\n",
    "\n",
    "    train_loader = DataLoader(training_data, BATCH_SIZE, shuffle = True)\n",
    "    val_loader = DataLoader(val_data, BATCH_SIZE, shuffle = True)\n",
    "    val_bake_loader = DataLoader(val_bake_data, BATCH_SIZE, shuffle = True)\n",
    "        \n",
    "    #blueprint for BERT Classification task\n",
    "    class BertClassifier(nn.Module):\n",
    "        def __init__(self, model_name, num_classes):\n",
    "            super(BertClassifier,self).__init__()\n",
    "            self.bert = BertModel.from_pretrained(model_name)\n",
    "            self.dropout = nn.Dropout(p = 0.3)\n",
    "            self.linear = nn.Linear(self.bert.config.hidden_size,num_classes)\n",
    "            self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "        def forward(self,input_ids, attention_mask):\n",
    "            temp = self.bert(input_ids, attention_mask)  \n",
    "            pooled_output = temp[1]                            \n",
    "            out = self.dropout(pooled_output)          \n",
    "            out = self.linear(out)\n",
    "            return out   # -> softmax probabilities\n",
    "        \n",
    "    #instantiate model with 3 classes as output\n",
    "    num_classes = 3\n",
    "    model = BertClassifier(weights_name, 3)\n",
    "    \n",
    "    #due to size of model, recommended to run on multi-gpu system\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "        model.to(device)\n",
    "        \n",
    "    #set model hyperparameters\n",
    "    learning_rate = 1e-5\n",
    "    EPOCHS = 3\n",
    "    steps = len(train_loader) * EPOCHS\n",
    "    loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optim = torch.optim.AdamW(params = model.parameters(),lr = learning_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer=optim, num_warmup_steps=0,num_training_steps = steps)\n",
    "    \n",
    "    #set model training loop through one epoch\n",
    "    def train_model(model, data_loader=train_loader, loss_function=loss_fn, optimizer=optim, scheduler=scheduler, n_examples=NUM_TRAIN_SAMPLES):\n",
    "        '''\n",
    "        Model training function that represents one pass through the data.\n",
    "        Returns accuracy and mean total loss.  This function is meant to be \n",
    "        paired with an \"eval model\" function.\n",
    "        '''\n",
    "        #set model in train mode to ensure grad calcs\n",
    "        model.train()\n",
    "        batches = len(data_loader)\n",
    "        train_loss = []\n",
    "        correct_predictions = 0\n",
    "\n",
    "\n",
    "        for d in tqdm(data_loader):\n",
    "\n",
    "            #grab data in batches and move to GPU\n",
    "            input_ids = d['input_id'].to(device)\n",
    "            masks = d['attention_mask'].to(device)\n",
    "            labels = d['labels'].to(device)\n",
    "\n",
    "            #forward propagation\n",
    "            predictions = model(input_ids, masks)\n",
    "            loss = loss_function(predictions, labels)\n",
    "            _, pred_classes = torch.max(predictions, dim=1)\n",
    "\n",
    "            #back propagation\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #collect loss and acc measures\n",
    "            train_loss.append(loss.item())\n",
    "            correct_predictions += torch.sum(pred_classes==labels)\n",
    "\n",
    "        return (correct_predictions/n_examples).cpu().numpy(), np.mean(train_loss)\n",
    "    \n",
    "    #set model eval loop through one epoch\n",
    "    def eval_model(model, data_loader=val_loader, loss_function=loss_fn, n_examples=NUM_VAL_SAMPLES):\n",
    "        '''\n",
    "        Model evaluation function that represents one pass through the data.\n",
    "        Returns accuracy and mean total loss.  This function is meant to be \n",
    "        paired with the \"train_model\" function.\n",
    "        '''\n",
    "        \n",
    "        #set model in eval mode to optimize speed\n",
    "        model.eval()\n",
    "        eval_loss = []\n",
    "        correct_predictions = 0\n",
    "        all_predictions = []\n",
    "        all_labels = [] \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for d in tqdm(data_loader):\n",
    "\n",
    "                input_ids = d['input_id'].to(device)\n",
    "                masks = d['attention_mask'].to(device)\n",
    "                labels = d['labels'].to(device)\n",
    "\n",
    "                #forward prop for inference\n",
    "                predictions = model(input_ids, masks)\n",
    "                loss = loss_function(predictions, labels)\n",
    "                _,pred_classes = torch.max(predictions, dim=1)\n",
    "\n",
    "                #collect preds/labels for class_report\n",
    "                all_predictions.extend(pred_classes.cpu().tolist())\n",
    "                all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "                #collect loss and acc measures\n",
    "                eval_loss.append(loss.item())\n",
    "                correct_predictions += torch.sum(pred_classes==labels)\n",
    "\n",
    "        report = classification_report(all_labels, \n",
    "                                       all_predictions, \n",
    "                                       labels=[0,1,2], \n",
    "                                       target_names=['negative', 'neutral', 'positive'])\n",
    "        \n",
    "        f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
    "\n",
    "        return (correct_predictions / n_examples).cpu().numpy(), np.mean(eval_loss), report, f1_macro\n",
    "    \n",
    "    #this is the execution function for the model training/evaluation cycle\n",
    "    def run(model, epochs: int=3):\n",
    "        '''\n",
    "        Execution function for model train/eval cycle. \n",
    "        Automatically saves model weights to file, if \n",
    "        scores is above updated f1_macro threshold. \n",
    "        '''\n",
    "        \n",
    "        tracking = defaultdict(list)\n",
    "        #initialize at a high rate to ensure extra models are not saved to disk\n",
    "        best_macro = 0.70\n",
    "        best_report = None\n",
    "\n",
    "        EPOCHS = epochs\n",
    "\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            print(f'epoch : {epoch+1}/{EPOCHS}')\n",
    "\n",
    "            train_acc, train_loss = train_model(model, data_loader=train_loader, n_examples=NUM_TRAIN_SAMPLES)\n",
    "            val_acc , val_loss, report, val_f1 = eval_model(model, data_loader=val_loader, n_examples=NUM_VAL_SAMPLES)\n",
    "            bakeoff_acc, bakeoff_loss, bakeoff_report, bakeoff_f1 = eval_model(model, data_loader=val_bake_loader, n_examples=NUM_BAKE_SAMPLES)\n",
    "\n",
    "            mean_f1_macro = np.mean([val_f1, bakeoff_f1])\n",
    "            print(f'Mean f1_macro = {mean_f1_macro}')\n",
    "\n",
    "            tracking['train_acc'].append(train_acc)\n",
    "            tracking['train_loss'].append(train_loss)\n",
    "            tracking['val_acc'].append((val_acc, report))\n",
    "            tracking['val_loss'].append(val_loss)\n",
    "            tracking['bake_acc'].append((bakeoff_acc, bakeoff_report))\n",
    "            tracking['bake_loss'].append(bakeoff_loss)\n",
    "\n",
    "            scores = np.round([train_loss, train_acc, val_loss, val_acc, bakeoff_loss, bakeoff_acc],3)\n",
    "            print(f'train_loss: {scores[0]}, train_acc: {scores[1]}\\\n",
    "                    \\nval_loss: {scores[2]}, val_acc: {scores[3]}\\\n",
    "                    \\nbake_loss: {scores[4]}, bake_acc: {scores[5]}')\n",
    "\n",
    "            if mean_f1_macro > best_macro:\n",
    "                best_model_name = f'/home/americanthinker/notebooks/pytorch/cs224u/saved_models/{weights_name}_{mean_f1_macro}.bin'\n",
    "                torch.save(model.state_dict(), best_model_name)\n",
    "                print(f\"New model saved with mean f1_macro of: {mean_f1_macro}\")\n",
    "                best_macro = mean_f1_macro\n",
    "                best_report = [report, bakeoff_report]\n",
    "            else: \n",
    "                print('mean_f1 not better than best macro')\n",
    "\n",
    "        end = time.perf_counter() - start\n",
    "\n",
    "        print(f'Total time for {EPOCHS} epochs: {np.round(end/60, 1)} minutes')\n",
    "        print(f'Classification Report:')\n",
    "        if best_report:\n",
    "            print(best_report)\n",
    "\n",
    "        return tracking\n",
    "    \n",
    "    tracker = run(model=model, epochs=5)\n",
    "    \n",
    "# STOP COMMENT: Please do not remove this comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bakeoff [1 point]\n",
    "\n",
    "As we said above, the bakeoff evaluation data is the official SST test-set release and a new test set derived from the same sources and labeling methods as for `bakeoff_dev`.\n",
    "\n",
    "For this bakeoff, you'll evaluate your original system from the above homework problem on these test sets. Our metric will be the mean of the macro-F1 values, which weights both datasets equally despite their differing sizes.\n",
    "\n",
    "The central requirement for your system is that you have define a `predict_one` method for it that maps a text (str) directly to a label prediction – one of 'positive', 'negative', 'neutral'. If you used `sst.experiment` with `vectorize=True`, then the following function (for `softmax_experiment`) will be easy to adapt – you probably just need to change the variable `softmax_experiment` to the variable for your experiment output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_softmax(text):\n",
    "    # Singleton list of feature dicts:\n",
    "    feats = [softmax_experiment['phi'](text)]\n",
    "    # Vectorize to get a feature matrix:\n",
    "    X = softmax_experiment['train_dataset']['vectorizer'].transform(feats)\n",
    "    # Standard sklearn `predict` step:\n",
    "    preds = softmax_experiment['model'].predict(X)\n",
    "    # Be sure to return the only member of the predictions,\n",
    "    # rather than the singleton list:\n",
    "    return preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you used an RNN like the one we demoed above, then featurization is a bit more straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_rnn(text):\n",
    "    # List of tokenized examples:\n",
    "    X = [rnn_experiment['phi'](text)]\n",
    "    # Standard `predict` step on a list of lists of str:\n",
    "    preds = rnn_experiment['model'].predict(X)\n",
    "    # Be sure to return the only member of the predictions,\n",
    "    # rather than the singleton list:\n",
    "    return preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to create the bakeoff submission file. Its arguments are your `predict_one` function and an output filename (str)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5481951bd5904da4b59c059a56affd8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed064417f686454bb394046a9e35cdee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c620fb2a7d24a9e8e927190de057f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2778c4f5016142d88170f67a8f4a8a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48400809a124fb2bd0d00983633ebfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=1024, out_features=3, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_name = 'bert-large-cased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(weights_name)\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes):\n",
    "        super(BertClassifier,self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(p = 0.3)\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size,num_classes)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "    def forward(self,input_ids, attention_mask):\n",
    "        temp = self.bert(input_ids, attention_mask)  \n",
    "        pooled_output = temp[1]                            \n",
    "        out = self.dropout(pooled_output)          \n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "num_classes = 3\n",
    "model = BertClassifier(weights_name, 3)\n",
    "model.to(device)\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(f'Device Count: {torch.cuda.device_count()}')\n",
    "#     model = nn.DataParallel(model)\n",
    "#     model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BertClassifier:\n\tMissing key(s) in state_dict: \"bert.embeddings.position_ids\", \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.embeddings.LayerNorm.weight\", \"bert.embeddings.LayerNorm.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.encoder.layer.1.intermediate.dense.weight\", \"bert.encoder.layer.1.intermediate.dense.bias\", \"bert.encoder.layer.1.output.dense.weight\", \"bert.encoder.layer.1.output.dense.bias\", \"bert.encoder.layer.1.output.LayerNorm.weight\", \"bert.encoder.layer.1.output.LayerNorm.bias\", \"bert.encoder.layer.2.attention.self.query.weight\", \"bert.encoder.layer.2.attention.self.query.bias\", \"bert.encoder.layer.2.attention.self.key.weight\", \"bert.encoder.layer.2.attention.self.key.bias\", \"bert.encoder.layer.2.attention.self.value.weight\", \"bert.encoder.layer.2.attention.self.value.bias\", \"bert.encoder.layer.2.attention.output.dense.weight\", \"bert.encoder.layer.2.attention.output.dense.bias\", \"bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.encoder.layer.2.intermediate.dense.weight\", \"bert.encoder.layer.2.intermediate.dense.bias\", \"bert.encoder.layer.2.output.dense.weight\", \"bert.encoder.layer.2.output.dense.bias\", \"bert.encoder.layer.2.output.LayerNorm.weight\", \"bert.encoder.layer.2.output.LayerNorm.bias\", \"bert.encoder.layer.3.attention.self.query.weight\", \"bert.encoder.layer.3.attention.self.query.bias\", \"bert.encoder.layer.3.attention.self.key.weight\", \"bert.encoder.layer.3.attention.self.key.bias\", \"bert.encoder.layer.3.attention.self.value.weight\", \"bert.encoder.layer.3.attention.self.value.bias\", \"bert.encoder.layer.3.attention.output.dense.weight\", \"bert.encoder.layer.3.attention.output.dense.bias\", \"bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.encoder.layer.3.intermediate.dense.weight\", \"bert.encoder.layer.3.intermediate.dense.bias\", \"bert.encoder.layer.3.output.dense.weight\", \"bert.encoder.layer.3.output.dense.bias\", \"bert.encoder.layer.3.output.LayerNorm.weight\", \"bert.encoder.layer.3.output.LayerNorm.bias\", \"bert.encoder.layer.4.attention.self.query.weight\", \"bert.encoder.layer.4.attention.self.query.bias\", \"bert.encoder.layer.4.attention.self.key.weight\", \"bert.encoder.layer.4.attention.self.key.bias\", \"bert.encoder.layer.4.attention.self.value.weight\", \"bert.encoder.layer.4.attention.self.value.bias\", \"bert.encoder.layer.4.attention.output.dense.weight\", \"bert.encoder.layer.4.attention.output.dense.bias\", \"bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.encoder.layer.4.intermediate.dense.weight\", \"bert.encoder.layer.4.intermediate.dense.bias\", \"bert.encoder.layer.4.output.dense.weight\", \"bert.encoder.layer.4.output.dense.bias\", \"bert.encoder.layer.4.output.LayerNorm.weight\", \"bert.encoder.layer.4.output.LayerNorm.bias\", \"bert.encoder.layer.5.attention.self.query.weight\", \"bert.encoder.layer.5.attention.self.query.bias\", \"bert.encoder.layer.5.attention.self.key.weight\", \"bert.encoder.layer.5.attention.self.key.bias\", \"bert.encoder.layer.5.attention.self.value.weight\", \"bert.encoder.layer.5.attention.self.value.bias\", \"bert.encoder.layer.5.attention.output.dense.weight\", \"bert.encoder.layer.5.attention.output.dense.bias\", \"bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.encoder.layer.5.intermediate.dense.weight\", \"bert.encoder.layer.5.intermediate.dense.bias\", \"bert.encoder.layer.5.output.dense.weight\", \"bert.encoder.layer.5.output.dense.bias\", \"bert.encoder.layer.5.output.LayerNorm.weight\", \"bert.encoder.layer.5.output.LayerNorm.bias\", \"bert.encoder.layer.6.attention.self.query.weight\", \"bert.encoder.layer.6.attention.self.query.bias\", \"bert.encoder.layer.6.attention.self.key.weight\", \"bert.encoder.layer.6.attention.self.key.bias\", \"bert.encoder.layer.6.attention.self.value.weight\", \"bert.encoder.layer.6.attention.self.value.bias\", \"bert.encoder.layer.6.attention.output.dense.weight\", \"bert.encoder.layer.6.attention.output.dense.bias\", \"bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.encoder.layer.6.intermediate.dense.weight\", \"bert.encoder.layer.6.intermediate.dense.bias\", \"bert.encoder.layer.6.output.dense.weight\", \"bert.encoder.layer.6.output.dense.bias\", \"bert.encoder.layer.6.output.LayerNorm.weight\", \"bert.encoder.layer.6.output.LayerNorm.bias\", \"bert.encoder.layer.7.attention.self.query.weight\", \"bert.encoder.layer.7.attention.self.query.bias\", \"bert.encoder.layer.7.attention.self.key.weight\", \"bert.encoder.layer.7.attention.self.key.bias\", \"bert.encoder.layer.7.attention.self.value.weight\", \"bert.encoder.layer.7.attention.self.value.bias\", \"bert.encoder.layer.7.attention.output.dense.weight\", \"bert.encoder.layer.7.attention.output.dense.bias\", \"bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.encoder.layer.7.intermediate.dense.weight\", \"bert.encoder.layer.7.intermediate.dense.bias\", \"bert.encoder.layer.7.output.dense.weight\", \"bert.encoder.layer.7.output.dense.bias\", \"bert.encoder.layer.7.output.LayerNorm.weight\", \"bert.encoder.layer.7.output.LayerNorm.bias\", \"bert.encoder.layer.8.attention.self.query.weight\", \"bert.encoder.layer.8.attention.self.query.bias\", \"bert.encoder.layer.8.attention.self.key.weight\", \"bert.encoder.layer.8.attention.self.key.bias\", \"bert.encoder.layer.8.attention.self.value.weight\", \"bert.encoder.layer.8.attention.self.value.bias\", \"bert.encoder.layer.8.attention.output.dense.weight\", \"bert.encoder.layer.8.attention.output.dense.bias\", \"bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.encoder.layer.8.intermediate.dense.weight\", \"bert.encoder.layer.8.intermediate.dense.bias\", \"bert.encoder.layer.8.output.dense.weight\", \"bert.encoder.layer.8.output.dense.bias\", \"bert.encoder.layer.8.output.LayerNorm.weight\", \"bert.encoder.layer.8.output.LayerNorm.bias\", \"bert.encoder.layer.9.attention.self.query.weight\", \"bert.encoder.layer.9.attention.self.query.bias\", \"bert.encoder.layer.9.attention.self.key.weight\", \"bert.encoder.layer.9.attention.self.key.bias\", \"bert.encoder.layer.9.attention.self.value.weight\", \"bert.encoder.layer.9.attention.self.value.bias\", \"bert.encoder.layer.9.attention.output.dense.weight\", \"bert.encoder.layer.9.attention.output.dense.bias\", \"bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.encoder.layer.9.intermediate.dense.weight\", \"bert.encoder.layer.9.intermediate.dense.bias\", \"bert.encoder.layer.9.output.dense.weight\", \"bert.encoder.layer.9.output.dense.bias\", \"bert.encoder.layer.9.output.LayerNorm.weight\", \"bert.encoder.layer.9.output.LayerNorm.bias\", \"bert.encoder.layer.10.attention.self.query.weight\", \"bert.encoder.layer.10.attention.self.query.bias\", \"bert.encoder.layer.10.attention.self.key.weight\", \"bert.encoder.layer.10.attention.self.key.bias\", \"bert.encoder.layer.10.attention.self.value.weight\", \"bert.encoder.layer.10.attention.self.value.bias\", \"bert.encoder.layer.10.attention.output.dense.weight\", \"bert.encoder.layer.10.attention.output.dense.bias\", \"bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.encoder.layer.10.intermediate.dense.weight\", \"bert.encoder.layer.10.intermediate.dense.bias\", \"bert.encoder.layer.10.output.dense.weight\", \"bert.encoder.layer.10.output.dense.bias\", \"bert.encoder.layer.10.output.LayerNorm.weight\", \"bert.encoder.layer.10.output.LayerNorm.bias\", \"bert.encoder.layer.11.attention.self.query.weight\", \"bert.encoder.layer.11.attention.self.query.bias\", \"bert.encoder.layer.11.attention.self.key.weight\", \"bert.encoder.layer.11.attention.self.key.bias\", \"bert.encoder.layer.11.attention.self.value.weight\", \"bert.encoder.layer.11.attention.self.value.bias\", \"bert.encoder.layer.11.attention.output.dense.weight\", \"bert.encoder.layer.11.attention.output.dense.bias\", \"bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.encoder.layer.11.intermediate.dense.weight\", \"bert.encoder.layer.11.intermediate.dense.bias\", \"bert.encoder.layer.11.output.dense.weight\", \"bert.encoder.layer.11.output.dense.bias\", \"bert.encoder.layer.11.output.LayerNorm.weight\", \"bert.encoder.layer.11.output.LayerNorm.bias\", \"bert.encoder.layer.12.attention.self.query.weight\", \"bert.encoder.layer.12.attention.self.query.bias\", \"bert.encoder.layer.12.attention.self.key.weight\", \"bert.encoder.layer.12.attention.self.key.bias\", \"bert.encoder.layer.12.attention.self.value.weight\", \"bert.encoder.layer.12.attention.self.value.bias\", \"bert.encoder.layer.12.attention.output.dense.weight\", \"bert.encoder.layer.12.attention.output.dense.bias\", \"bert.encoder.layer.12.attention.output.LayerNorm.weight\", \"bert.encoder.layer.12.attention.output.LayerNorm.bias\", \"bert.encoder.layer.12.intermediate.dense.weight\", \"bert.encoder.layer.12.intermediate.dense.bias\", \"bert.encoder.layer.12.output.dense.weight\", \"bert.encoder.layer.12.output.dense.bias\", \"bert.encoder.layer.12.output.LayerNorm.weight\", \"bert.encoder.layer.12.output.LayerNorm.bias\", \"bert.encoder.layer.13.attention.self.query.weight\", \"bert.encoder.layer.13.attention.self.query.bias\", \"bert.encoder.layer.13.attention.self.key.weight\", \"bert.encoder.layer.13.attention.self.key.bias\", \"bert.encoder.layer.13.attention.self.value.weight\", \"bert.encoder.layer.13.attention.self.value.bias\", \"bert.encoder.layer.13.attention.output.dense.weight\", \"bert.encoder.layer.13.attention.output.dense.bias\", \"bert.encoder.layer.13.attention.output.LayerNorm.weight\", \"bert.encoder.layer.13.attention.output.LayerNorm.bias\", \"bert.encoder.layer.13.intermediate.dense.weight\", \"bert.encoder.layer.13.intermediate.dense.bias\", \"bert.encoder.layer.13.output.dense.weight\", \"bert.encoder.layer.13.output.dense.bias\", \"bert.encoder.layer.13.output.LayerNorm.weight\", \"bert.encoder.layer.13.output.LayerNorm.bias\", \"bert.encoder.layer.14.attention.self.query.weight\", \"bert.encoder.layer.14.attention.self.query.bias\", \"bert.encoder.layer.14.attention.self.key.weight\", \"bert.encoder.layer.14.attention.self.key.bias\", \"bert.encoder.layer.14.attention.self.value.weight\", \"bert.encoder.layer.14.attention.self.value.bias\", \"bert.encoder.layer.14.attention.output.dense.weight\", \"bert.encoder.layer.14.attention.output.dense.bias\", \"bert.encoder.layer.14.attention.output.LayerNorm.weight\", \"bert.encoder.layer.14.attention.output.LayerNorm.bias\", \"bert.encoder.layer.14.intermediate.dense.weight\", \"bert.encoder.layer.14.intermediate.dense.bias\", \"bert.encoder.layer.14.output.dense.weight\", \"bert.encoder.layer.14.output.dense.bias\", \"bert.encoder.layer.14.output.LayerNorm.weight\", \"bert.encoder.layer.14.output.LayerNorm.bias\", \"bert.encoder.layer.15.attention.self.query.weight\", \"bert.encoder.layer.15.attention.self.query.bias\", \"bert.encoder.layer.15.attention.self.key.weight\", \"bert.encoder.layer.15.attention.self.key.bias\", \"bert.encoder.layer.15.attention.self.value.weight\", \"bert.encoder.layer.15.attention.self.value.bias\", \"bert.encoder.layer.15.attention.output.dense.weight\", \"bert.encoder.layer.15.attention.output.dense.bias\", \"bert.encoder.layer.15.attention.output.LayerNorm.weight\", \"bert.encoder.layer.15.attention.output.LayerNorm.bias\", \"bert.encoder.layer.15.intermediate.dense.weight\", \"bert.encoder.layer.15.intermediate.dense.bias\", \"bert.encoder.layer.15.output.dense.weight\", \"bert.encoder.layer.15.output.dense.bias\", \"bert.encoder.layer.15.output.LayerNorm.weight\", \"bert.encoder.layer.15.output.LayerNorm.bias\", \"bert.encoder.layer.16.attention.self.query.weight\", \"bert.encoder.layer.16.attention.self.query.bias\", \"bert.encoder.layer.16.attention.self.key.weight\", \"bert.encoder.layer.16.attention.self.key.bias\", \"bert.encoder.layer.16.attention.self.value.weight\", \"bert.encoder.layer.16.attention.self.value.bias\", \"bert.encoder.layer.16.attention.output.dense.weight\", \"bert.encoder.layer.16.attention.output.dense.bias\", \"bert.encoder.layer.16.attention.output.LayerNorm.weight\", \"bert.encoder.layer.16.attention.output.LayerNorm.bias\", \"bert.encoder.layer.16.intermediate.dense.weight\", \"bert.encoder.layer.16.intermediate.dense.bias\", \"bert.encoder.layer.16.output.dense.weight\", \"bert.encoder.layer.16.output.dense.bias\", \"bert.encoder.layer.16.output.LayerNorm.weight\", \"bert.encoder.layer.16.output.LayerNorm.bias\", \"bert.encoder.layer.17.attention.self.query.weight\", \"bert.encoder.layer.17.attention.self.query.bias\", \"bert.encoder.layer.17.attention.self.key.weight\", \"bert.encoder.layer.17.attention.self.key.bias\", \"bert.encoder.layer.17.attention.self.value.weight\", \"bert.encoder.layer.17.attention.self.value.bias\", \"bert.encoder.layer.17.attention.output.dense.weight\", \"bert.encoder.layer.17.attention.output.dense.bias\", \"bert.encoder.layer.17.attention.output.LayerNorm.weight\", \"bert.encoder.layer.17.attention.output.LayerNorm.bias\", \"bert.encoder.layer.17.intermediate.dense.weight\", \"bert.encoder.layer.17.intermediate.dense.bias\", \"bert.encoder.layer.17.output.dense.weight\", \"bert.encoder.layer.17.output.dense.bias\", \"bert.encoder.layer.17.output.LayerNorm.weight\", \"bert.encoder.layer.17.output.LayerNorm.bias\", \"bert.encoder.layer.18.attention.self.query.weight\", \"bert.encoder.layer.18.attention.self.query.bias\", \"bert.encoder.layer.18.attention.self.key.weight\", \"bert.encoder.layer.18.attention.self.key.bias\", \"bert.encoder.layer.18.attention.self.value.weight\", \"bert.encoder.layer.18.attention.self.value.bias\", \"bert.encoder.layer.18.attention.output.dense.weight\", \"bert.encoder.layer.18.attention.output.dense.bias\", \"bert.encoder.layer.18.attention.output.LayerNorm.weight\", \"bert.encoder.layer.18.attention.output.LayerNorm.bias\", \"bert.encoder.layer.18.intermediate.dense.weight\", \"bert.encoder.layer.18.intermediate.dense.bias\", \"bert.encoder.layer.18.output.dense.weight\", \"bert.encoder.layer.18.output.dense.bias\", \"bert.encoder.layer.18.output.LayerNorm.weight\", \"bert.encoder.layer.18.output.LayerNorm.bias\", \"bert.encoder.layer.19.attention.self.query.weight\", \"bert.encoder.layer.19.attention.self.query.bias\", \"bert.encoder.layer.19.attention.self.key.weight\", \"bert.encoder.layer.19.attention.self.key.bias\", \"bert.encoder.layer.19.attention.self.value.weight\", \"bert.encoder.layer.19.attention.self.value.bias\", \"bert.encoder.layer.19.attention.output.dense.weight\", \"bert.encoder.layer.19.attention.output.dense.bias\", \"bert.encoder.layer.19.attention.output.LayerNorm.weight\", \"bert.encoder.layer.19.attention.output.LayerNorm.bias\", \"bert.encoder.layer.19.intermediate.dense.weight\", \"bert.encoder.layer.19.intermediate.dense.bias\", \"bert.encoder.layer.19.output.dense.weight\", \"bert.encoder.layer.19.output.dense.bias\", \"bert.encoder.layer.19.output.LayerNorm.weight\", \"bert.encoder.layer.19.output.LayerNorm.bias\", \"bert.encoder.layer.20.attention.self.query.weight\", \"bert.encoder.layer.20.attention.self.query.bias\", \"bert.encoder.layer.20.attention.self.key.weight\", \"bert.encoder.layer.20.attention.self.key.bias\", \"bert.encoder.layer.20.attention.self.value.weight\", \"bert.encoder.layer.20.attention.self.value.bias\", \"bert.encoder.layer.20.attention.output.dense.weight\", \"bert.encoder.layer.20.attention.output.dense.bias\", \"bert.encoder.layer.20.attention.output.LayerNorm.weight\", \"bert.encoder.layer.20.attention.output.LayerNorm.bias\", \"bert.encoder.layer.20.intermediate.dense.weight\", \"bert.encoder.layer.20.intermediate.dense.bias\", \"bert.encoder.layer.20.output.dense.weight\", \"bert.encoder.layer.20.output.dense.bias\", \"bert.encoder.layer.20.output.LayerNorm.weight\", \"bert.encoder.layer.20.output.LayerNorm.bias\", \"bert.encoder.layer.21.attention.self.query.weight\", \"bert.encoder.layer.21.attention.self.query.bias\", \"bert.encoder.layer.21.attention.self.key.weight\", \"bert.encoder.layer.21.attention.self.key.bias\", \"bert.encoder.layer.21.attention.self.value.weight\", \"bert.encoder.layer.21.attention.self.value.bias\", \"bert.encoder.layer.21.attention.output.dense.weight\", \"bert.encoder.layer.21.attention.output.dense.bias\", \"bert.encoder.layer.21.attention.output.LayerNorm.weight\", \"bert.encoder.layer.21.attention.output.LayerNorm.bias\", \"bert.encoder.layer.21.intermediate.dense.weight\", \"bert.encoder.layer.21.intermediate.dense.bias\", \"bert.encoder.layer.21.output.dense.weight\", \"bert.encoder.layer.21.output.dense.bias\", \"bert.encoder.layer.21.output.LayerNorm.weight\", \"bert.encoder.layer.21.output.LayerNorm.bias\", \"bert.encoder.layer.22.attention.self.query.weight\", \"bert.encoder.layer.22.attention.self.query.bias\", \"bert.encoder.layer.22.attention.self.key.weight\", \"bert.encoder.layer.22.attention.self.key.bias\", \"bert.encoder.layer.22.attention.self.value.weight\", \"bert.encoder.layer.22.attention.self.value.bias\", \"bert.encoder.layer.22.attention.output.dense.weight\", \"bert.encoder.layer.22.attention.output.dense.bias\", \"bert.encoder.layer.22.attention.output.LayerNorm.weight\", \"bert.encoder.layer.22.attention.output.LayerNorm.bias\", \"bert.encoder.layer.22.intermediate.dense.weight\", \"bert.encoder.layer.22.intermediate.dense.bias\", \"bert.encoder.layer.22.output.dense.weight\", \"bert.encoder.layer.22.output.dense.bias\", \"bert.encoder.layer.22.output.LayerNorm.weight\", \"bert.encoder.layer.22.output.LayerNorm.bias\", \"bert.encoder.layer.23.attention.self.query.weight\", \"bert.encoder.layer.23.attention.self.query.bias\", \"bert.encoder.layer.23.attention.self.key.weight\", \"bert.encoder.layer.23.attention.self.key.bias\", \"bert.encoder.layer.23.attention.self.value.weight\", \"bert.encoder.layer.23.attention.self.value.bias\", \"bert.encoder.layer.23.attention.output.dense.weight\", \"bert.encoder.layer.23.attention.output.dense.bias\", \"bert.encoder.layer.23.attention.output.LayerNorm.weight\", \"bert.encoder.layer.23.attention.output.LayerNorm.bias\", \"bert.encoder.layer.23.intermediate.dense.weight\", \"bert.encoder.layer.23.intermediate.dense.bias\", \"bert.encoder.layer.23.output.dense.weight\", \"bert.encoder.layer.23.output.dense.bias\", \"bert.encoder.layer.23.output.LayerNorm.weight\", \"bert.encoder.layer.23.output.LayerNorm.bias\", \"bert.pooler.dense.weight\", \"bert.pooler.dense.bias\", \"linear.weight\", \"linear.bias\". \n\tUnexpected key(s) in state_dict: \"module.bert.embeddings.position_ids\", \"module.bert.embeddings.word_embeddings.weight\", \"module.bert.embeddings.position_embeddings.weight\", \"module.bert.embeddings.token_type_embeddings.weight\", \"module.bert.embeddings.LayerNorm.weight\", \"module.bert.embeddings.LayerNorm.bias\", \"module.bert.encoder.layer.0.attention.self.query.weight\", \"module.bert.encoder.layer.0.attention.self.query.bias\", \"module.bert.encoder.layer.0.attention.self.key.weight\", \"module.bert.encoder.layer.0.attention.self.key.bias\", \"module.bert.encoder.layer.0.attention.self.value.weight\", \"module.bert.encoder.layer.0.attention.self.value.bias\", \"module.bert.encoder.layer.0.attention.output.dense.weight\", \"module.bert.encoder.layer.0.attention.output.dense.bias\", \"module.bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.0.intermediate.dense.weight\", \"module.bert.encoder.layer.0.intermediate.dense.bias\", \"module.bert.encoder.layer.0.output.dense.weight\", \"module.bert.encoder.layer.0.output.dense.bias\", \"module.bert.encoder.layer.0.output.LayerNorm.weight\", \"module.bert.encoder.layer.0.output.LayerNorm.bias\", \"module.bert.encoder.layer.1.attention.self.query.weight\", \"module.bert.encoder.layer.1.attention.self.query.bias\", \"module.bert.encoder.layer.1.attention.self.key.weight\", \"module.bert.encoder.layer.1.attention.self.key.bias\", \"module.bert.encoder.layer.1.attention.self.value.weight\", \"module.bert.encoder.layer.1.attention.self.value.bias\", \"module.bert.encoder.layer.1.attention.output.dense.weight\", \"module.bert.encoder.layer.1.attention.output.dense.bias\", \"module.bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.1.intermediate.dense.weight\", \"module.bert.encoder.layer.1.intermediate.dense.bias\", \"module.bert.encoder.layer.1.output.dense.weight\", \"module.bert.encoder.layer.1.output.dense.bias\", \"module.bert.encoder.layer.1.output.LayerNorm.weight\", \"module.bert.encoder.layer.1.output.LayerNorm.bias\", \"module.bert.encoder.layer.2.attention.self.query.weight\", \"module.bert.encoder.layer.2.attention.self.query.bias\", \"module.bert.encoder.layer.2.attention.self.key.weight\", \"module.bert.encoder.layer.2.attention.self.key.bias\", \"module.bert.encoder.layer.2.attention.self.value.weight\", \"module.bert.encoder.layer.2.attention.self.value.bias\", \"module.bert.encoder.layer.2.attention.output.dense.weight\", \"module.bert.encoder.layer.2.attention.output.dense.bias\", \"module.bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.2.intermediate.dense.weight\", \"module.bert.encoder.layer.2.intermediate.dense.bias\", \"module.bert.encoder.layer.2.output.dense.weight\", \"module.bert.encoder.layer.2.output.dense.bias\", \"module.bert.encoder.layer.2.output.LayerNorm.weight\", \"module.bert.encoder.layer.2.output.LayerNorm.bias\", \"module.bert.encoder.layer.3.attention.self.query.weight\", \"module.bert.encoder.layer.3.attention.self.query.bias\", \"module.bert.encoder.layer.3.attention.self.key.weight\", \"module.bert.encoder.layer.3.attention.self.key.bias\", \"module.bert.encoder.layer.3.attention.self.value.weight\", \"module.bert.encoder.layer.3.attention.self.value.bias\", \"module.bert.encoder.layer.3.attention.output.dense.weight\", \"module.bert.encoder.layer.3.attention.output.dense.bias\", \"module.bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.3.intermediate.dense.weight\", \"module.bert.encoder.layer.3.intermediate.dense.bias\", \"module.bert.encoder.layer.3.output.dense.weight\", \"module.bert.encoder.layer.3.output.dense.bias\", \"module.bert.encoder.layer.3.output.LayerNorm.weight\", \"module.bert.encoder.layer.3.output.LayerNorm.bias\", \"module.bert.encoder.layer.4.attention.self.query.weight\", \"module.bert.encoder.layer.4.attention.self.query.bias\", \"module.bert.encoder.layer.4.attention.self.key.weight\", \"module.bert.encoder.layer.4.attention.self.key.bias\", \"module.bert.encoder.layer.4.attention.self.value.weight\", \"module.bert.encoder.layer.4.attention.self.value.bias\", \"module.bert.encoder.layer.4.attention.output.dense.weight\", \"module.bert.encoder.layer.4.attention.output.dense.bias\", \"module.bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.4.intermediate.dense.weight\", \"module.bert.encoder.layer.4.intermediate.dense.bias\", \"module.bert.encoder.layer.4.output.dense.weight\", \"module.bert.encoder.layer.4.output.dense.bias\", \"module.bert.encoder.layer.4.output.LayerNorm.weight\", \"module.bert.encoder.layer.4.output.LayerNorm.bias\", \"module.bert.encoder.layer.5.attention.self.query.weight\", \"module.bert.encoder.layer.5.attention.self.query.bias\", \"module.bert.encoder.layer.5.attention.self.key.weight\", \"module.bert.encoder.layer.5.attention.self.key.bias\", \"module.bert.encoder.layer.5.attention.self.value.weight\", \"module.bert.encoder.layer.5.attention.self.value.bias\", \"module.bert.encoder.layer.5.attention.output.dense.weight\", \"module.bert.encoder.layer.5.attention.output.dense.bias\", \"module.bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.5.intermediate.dense.weight\", \"module.bert.encoder.layer.5.intermediate.dense.bias\", \"module.bert.encoder.layer.5.output.dense.weight\", \"module.bert.encoder.layer.5.output.dense.bias\", \"module.bert.encoder.layer.5.output.LayerNorm.weight\", \"module.bert.encoder.layer.5.output.LayerNorm.bias\", \"module.bert.encoder.layer.6.attention.self.query.weight\", \"module.bert.encoder.layer.6.attention.self.query.bias\", \"module.bert.encoder.layer.6.attention.self.key.weight\", \"module.bert.encoder.layer.6.attention.self.key.bias\", \"module.bert.encoder.layer.6.attention.self.value.weight\", \"module.bert.encoder.layer.6.attention.self.value.bias\", \"module.bert.encoder.layer.6.attention.output.dense.weight\", \"module.bert.encoder.layer.6.attention.output.dense.bias\", \"module.bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.6.intermediate.dense.weight\", \"module.bert.encoder.layer.6.intermediate.dense.bias\", \"module.bert.encoder.layer.6.output.dense.weight\", \"module.bert.encoder.layer.6.output.dense.bias\", \"module.bert.encoder.layer.6.output.LayerNorm.weight\", \"module.bert.encoder.layer.6.output.LayerNorm.bias\", \"module.bert.encoder.layer.7.attention.self.query.weight\", \"module.bert.encoder.layer.7.attention.self.query.bias\", \"module.bert.encoder.layer.7.attention.self.key.weight\", \"module.bert.encoder.layer.7.attention.self.key.bias\", \"module.bert.encoder.layer.7.attention.self.value.weight\", \"module.bert.encoder.layer.7.attention.self.value.bias\", \"module.bert.encoder.layer.7.attention.output.dense.weight\", \"module.bert.encoder.layer.7.attention.output.dense.bias\", \"module.bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.7.intermediate.dense.weight\", \"module.bert.encoder.layer.7.intermediate.dense.bias\", \"module.bert.encoder.layer.7.output.dense.weight\", \"module.bert.encoder.layer.7.output.dense.bias\", \"module.bert.encoder.layer.7.output.LayerNorm.weight\", \"module.bert.encoder.layer.7.output.LayerNorm.bias\", \"module.bert.encoder.layer.8.attention.self.query.weight\", \"module.bert.encoder.layer.8.attention.self.query.bias\", \"module.bert.encoder.layer.8.attention.self.key.weight\", \"module.bert.encoder.layer.8.attention.self.key.bias\", \"module.bert.encoder.layer.8.attention.self.value.weight\", \"module.bert.encoder.layer.8.attention.self.value.bias\", \"module.bert.encoder.layer.8.attention.output.dense.weight\", \"module.bert.encoder.layer.8.attention.output.dense.bias\", \"module.bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.8.intermediate.dense.weight\", \"module.bert.encoder.layer.8.intermediate.dense.bias\", \"module.bert.encoder.layer.8.output.dense.weight\", \"module.bert.encoder.layer.8.output.dense.bias\", \"module.bert.encoder.layer.8.output.LayerNorm.weight\", \"module.bert.encoder.layer.8.output.LayerNorm.bias\", \"module.bert.encoder.layer.9.attention.self.query.weight\", \"module.bert.encoder.layer.9.attention.self.query.bias\", \"module.bert.encoder.layer.9.attention.self.key.weight\", \"module.bert.encoder.layer.9.attention.self.key.bias\", \"module.bert.encoder.layer.9.attention.self.value.weight\", \"module.bert.encoder.layer.9.attention.self.value.bias\", \"module.bert.encoder.layer.9.attention.output.dense.weight\", \"module.bert.encoder.layer.9.attention.output.dense.bias\", \"module.bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.9.intermediate.dense.weight\", \"module.bert.encoder.layer.9.intermediate.dense.bias\", \"module.bert.encoder.layer.9.output.dense.weight\", \"module.bert.encoder.layer.9.output.dense.bias\", \"module.bert.encoder.layer.9.output.LayerNorm.weight\", \"module.bert.encoder.layer.9.output.LayerNorm.bias\", \"module.bert.encoder.layer.10.attention.self.query.weight\", \"module.bert.encoder.layer.10.attention.self.query.bias\", \"module.bert.encoder.layer.10.attention.self.key.weight\", \"module.bert.encoder.layer.10.attention.self.key.bias\", \"module.bert.encoder.layer.10.attention.self.value.weight\", \"module.bert.encoder.layer.10.attention.self.value.bias\", \"module.bert.encoder.layer.10.attention.output.dense.weight\", \"module.bert.encoder.layer.10.attention.output.dense.bias\", \"module.bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.10.intermediate.dense.weight\", \"module.bert.encoder.layer.10.intermediate.dense.bias\", \"module.bert.encoder.layer.10.output.dense.weight\", \"module.bert.encoder.layer.10.output.dense.bias\", \"module.bert.encoder.layer.10.output.LayerNorm.weight\", \"module.bert.encoder.layer.10.output.LayerNorm.bias\", \"module.bert.encoder.layer.11.attention.self.query.weight\", \"module.bert.encoder.layer.11.attention.self.query.bias\", \"module.bert.encoder.layer.11.attention.self.key.weight\", \"module.bert.encoder.layer.11.attention.self.key.bias\", \"module.bert.encoder.layer.11.attention.self.value.weight\", \"module.bert.encoder.layer.11.attention.self.value.bias\", \"module.bert.encoder.layer.11.attention.output.dense.weight\", \"module.bert.encoder.layer.11.attention.output.dense.bias\", \"module.bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.11.intermediate.dense.weight\", \"module.bert.encoder.layer.11.intermediate.dense.bias\", \"module.bert.encoder.layer.11.output.dense.weight\", \"module.bert.encoder.layer.11.output.dense.bias\", \"module.bert.encoder.layer.11.output.LayerNorm.weight\", \"module.bert.encoder.layer.11.output.LayerNorm.bias\", \"module.bert.encoder.layer.12.attention.self.query.weight\", \"module.bert.encoder.layer.12.attention.self.query.bias\", \"module.bert.encoder.layer.12.attention.self.key.weight\", \"module.bert.encoder.layer.12.attention.self.key.bias\", \"module.bert.encoder.layer.12.attention.self.value.weight\", \"module.bert.encoder.layer.12.attention.self.value.bias\", \"module.bert.encoder.layer.12.attention.output.dense.weight\", \"module.bert.encoder.layer.12.attention.output.dense.bias\", \"module.bert.encoder.layer.12.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.12.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.12.intermediate.dense.weight\", \"module.bert.encoder.layer.12.intermediate.dense.bias\", \"module.bert.encoder.layer.12.output.dense.weight\", \"module.bert.encoder.layer.12.output.dense.bias\", \"module.bert.encoder.layer.12.output.LayerNorm.weight\", \"module.bert.encoder.layer.12.output.LayerNorm.bias\", \"module.bert.encoder.layer.13.attention.self.query.weight\", \"module.bert.encoder.layer.13.attention.self.query.bias\", \"module.bert.encoder.layer.13.attention.self.key.weight\", \"module.bert.encoder.layer.13.attention.self.key.bias\", \"module.bert.encoder.layer.13.attention.self.value.weight\", \"module.bert.encoder.layer.13.attention.self.value.bias\", \"module.bert.encoder.layer.13.attention.output.dense.weight\", \"module.bert.encoder.layer.13.attention.output.dense.bias\", \"module.bert.encoder.layer.13.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.13.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.13.intermediate.dense.weight\", \"module.bert.encoder.layer.13.intermediate.dense.bias\", \"module.bert.encoder.layer.13.output.dense.weight\", \"module.bert.encoder.layer.13.output.dense.bias\", \"module.bert.encoder.layer.13.output.LayerNorm.weight\", \"module.bert.encoder.layer.13.output.LayerNorm.bias\", \"module.bert.encoder.layer.14.attention.self.query.weight\", \"module.bert.encoder.layer.14.attention.self.query.bias\", \"module.bert.encoder.layer.14.attention.self.key.weight\", \"module.bert.encoder.layer.14.attention.self.key.bias\", \"module.bert.encoder.layer.14.attention.self.value.weight\", \"module.bert.encoder.layer.14.attention.self.value.bias\", \"module.bert.encoder.layer.14.attention.output.dense.weight\", \"module.bert.encoder.layer.14.attention.output.dense.bias\", \"module.bert.encoder.layer.14.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.14.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.14.intermediate.dense.weight\", \"module.bert.encoder.layer.14.intermediate.dense.bias\", \"module.bert.encoder.layer.14.output.dense.weight\", \"module.bert.encoder.layer.14.output.dense.bias\", \"module.bert.encoder.layer.14.output.LayerNorm.weight\", \"module.bert.encoder.layer.14.output.LayerNorm.bias\", \"module.bert.encoder.layer.15.attention.self.query.weight\", \"module.bert.encoder.layer.15.attention.self.query.bias\", \"module.bert.encoder.layer.15.attention.self.key.weight\", \"module.bert.encoder.layer.15.attention.self.key.bias\", \"module.bert.encoder.layer.15.attention.self.value.weight\", \"module.bert.encoder.layer.15.attention.self.value.bias\", \"module.bert.encoder.layer.15.attention.output.dense.weight\", \"module.bert.encoder.layer.15.attention.output.dense.bias\", \"module.bert.encoder.layer.15.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.15.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.15.intermediate.dense.weight\", \"module.bert.encoder.layer.15.intermediate.dense.bias\", \"module.bert.encoder.layer.15.output.dense.weight\", \"module.bert.encoder.layer.15.output.dense.bias\", \"module.bert.encoder.layer.15.output.LayerNorm.weight\", \"module.bert.encoder.layer.15.output.LayerNorm.bias\", \"module.bert.encoder.layer.16.attention.self.query.weight\", \"module.bert.encoder.layer.16.attention.self.query.bias\", \"module.bert.encoder.layer.16.attention.self.key.weight\", \"module.bert.encoder.layer.16.attention.self.key.bias\", \"module.bert.encoder.layer.16.attention.self.value.weight\", \"module.bert.encoder.layer.16.attention.self.value.bias\", \"module.bert.encoder.layer.16.attention.output.dense.weight\", \"module.bert.encoder.layer.16.attention.output.dense.bias\", \"module.bert.encoder.layer.16.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.16.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.16.intermediate.dense.weight\", \"module.bert.encoder.layer.16.intermediate.dense.bias\", \"module.bert.encoder.layer.16.output.dense.weight\", \"module.bert.encoder.layer.16.output.dense.bias\", \"module.bert.encoder.layer.16.output.LayerNorm.weight\", \"module.bert.encoder.layer.16.output.LayerNorm.bias\", \"module.bert.encoder.layer.17.attention.self.query.weight\", \"module.bert.encoder.layer.17.attention.self.query.bias\", \"module.bert.encoder.layer.17.attention.self.key.weight\", \"module.bert.encoder.layer.17.attention.self.key.bias\", \"module.bert.encoder.layer.17.attention.self.value.weight\", \"module.bert.encoder.layer.17.attention.self.value.bias\", \"module.bert.encoder.layer.17.attention.output.dense.weight\", \"module.bert.encoder.layer.17.attention.output.dense.bias\", \"module.bert.encoder.layer.17.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.17.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.17.intermediate.dense.weight\", \"module.bert.encoder.layer.17.intermediate.dense.bias\", \"module.bert.encoder.layer.17.output.dense.weight\", \"module.bert.encoder.layer.17.output.dense.bias\", \"module.bert.encoder.layer.17.output.LayerNorm.weight\", \"module.bert.encoder.layer.17.output.LayerNorm.bias\", \"module.bert.encoder.layer.18.attention.self.query.weight\", \"module.bert.encoder.layer.18.attention.self.query.bias\", \"module.bert.encoder.layer.18.attention.self.key.weight\", \"module.bert.encoder.layer.18.attention.self.key.bias\", \"module.bert.encoder.layer.18.attention.self.value.weight\", \"module.bert.encoder.layer.18.attention.self.value.bias\", \"module.bert.encoder.layer.18.attention.output.dense.weight\", \"module.bert.encoder.layer.18.attention.output.dense.bias\", \"module.bert.encoder.layer.18.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.18.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.18.intermediate.dense.weight\", \"module.bert.encoder.layer.18.intermediate.dense.bias\", \"module.bert.encoder.layer.18.output.dense.weight\", \"module.bert.encoder.layer.18.output.dense.bias\", \"module.bert.encoder.layer.18.output.LayerNorm.weight\", \"module.bert.encoder.layer.18.output.LayerNorm.bias\", \"module.bert.encoder.layer.19.attention.self.query.weight\", \"module.bert.encoder.layer.19.attention.self.query.bias\", \"module.bert.encoder.layer.19.attention.self.key.weight\", \"module.bert.encoder.layer.19.attention.self.key.bias\", \"module.bert.encoder.layer.19.attention.self.value.weight\", \"module.bert.encoder.layer.19.attention.self.value.bias\", \"module.bert.encoder.layer.19.attention.output.dense.weight\", \"module.bert.encoder.layer.19.attention.output.dense.bias\", \"module.bert.encoder.layer.19.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.19.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.19.intermediate.dense.weight\", \"module.bert.encoder.layer.19.intermediate.dense.bias\", \"module.bert.encoder.layer.19.output.dense.weight\", \"module.bert.encoder.layer.19.output.dense.bias\", \"module.bert.encoder.layer.19.output.LayerNorm.weight\", \"module.bert.encoder.layer.19.output.LayerNorm.bias\", \"module.bert.encoder.layer.20.attention.self.query.weight\", \"module.bert.encoder.layer.20.attention.self.query.bias\", \"module.bert.encoder.layer.20.attention.self.key.weight\", \"module.bert.encoder.layer.20.attention.self.key.bias\", \"module.bert.encoder.layer.20.attention.self.value.weight\", \"module.bert.encoder.layer.20.attention.self.value.bias\", \"module.bert.encoder.layer.20.attention.output.dense.weight\", \"module.bert.encoder.layer.20.attention.output.dense.bias\", \"module.bert.encoder.layer.20.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.20.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.20.intermediate.dense.weight\", \"module.bert.encoder.layer.20.intermediate.dense.bias\", \"module.bert.encoder.layer.20.output.dense.weight\", \"module.bert.encoder.layer.20.output.dense.bias\", \"module.bert.encoder.layer.20.output.LayerNorm.weight\", \"module.bert.encoder.layer.20.output.LayerNorm.bias\", \"module.bert.encoder.layer.21.attention.self.query.weight\", \"module.bert.encoder.layer.21.attention.self.query.bias\", \"module.bert.encoder.layer.21.attention.self.key.weight\", \"module.bert.encoder.layer.21.attention.self.key.bias\", \"module.bert.encoder.layer.21.attention.self.value.weight\", \"module.bert.encoder.layer.21.attention.self.value.bias\", \"module.bert.encoder.layer.21.attention.output.dense.weight\", \"module.bert.encoder.layer.21.attention.output.dense.bias\", \"module.bert.encoder.layer.21.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.21.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.21.intermediate.dense.weight\", \"module.bert.encoder.layer.21.intermediate.dense.bias\", \"module.bert.encoder.layer.21.output.dense.weight\", \"module.bert.encoder.layer.21.output.dense.bias\", \"module.bert.encoder.layer.21.output.LayerNorm.weight\", \"module.bert.encoder.layer.21.output.LayerNorm.bias\", \"module.bert.encoder.layer.22.attention.self.query.weight\", \"module.bert.encoder.layer.22.attention.self.query.bias\", \"module.bert.encoder.layer.22.attention.self.key.weight\", \"module.bert.encoder.layer.22.attention.self.key.bias\", \"module.bert.encoder.layer.22.attention.self.value.weight\", \"module.bert.encoder.layer.22.attention.self.value.bias\", \"module.bert.encoder.layer.22.attention.output.dense.weight\", \"module.bert.encoder.layer.22.attention.output.dense.bias\", \"module.bert.encoder.layer.22.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.22.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.22.intermediate.dense.weight\", \"module.bert.encoder.layer.22.intermediate.dense.bias\", \"module.bert.encoder.layer.22.output.dense.weight\", \"module.bert.encoder.layer.22.output.dense.bias\", \"module.bert.encoder.layer.22.output.LayerNorm.weight\", \"module.bert.encoder.layer.22.output.LayerNorm.bias\", \"module.bert.encoder.layer.23.attention.self.query.weight\", \"module.bert.encoder.layer.23.attention.self.query.bias\", \"module.bert.encoder.layer.23.attention.self.key.weight\", \"module.bert.encoder.layer.23.attention.self.key.bias\", \"module.bert.encoder.layer.23.attention.self.value.weight\", \"module.bert.encoder.layer.23.attention.self.value.bias\", \"module.bert.encoder.layer.23.attention.output.dense.weight\", \"module.bert.encoder.layer.23.attention.output.dense.bias\", \"module.bert.encoder.layer.23.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.23.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.23.intermediate.dense.weight\", \"module.bert.encoder.layer.23.intermediate.dense.bias\", \"module.bert.encoder.layer.23.output.dense.weight\", \"module.bert.encoder.layer.23.output.dense.bias\", \"module.bert.encoder.layer.23.output.LayerNorm.weight\", \"module.bert.encoder.layer.23.output.LayerNorm.bias\", \"module.bert.pooler.dense.weight\", \"module.bert.pooler.dense.bias\", \"module.linear.weight\", \"module.linear.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6d8fa76a2b2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../Downloads/bert-large-cased_0.741.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     encoding = bert_tokenizer.encode_plus(\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1407\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BertClassifier:\n\tMissing key(s) in state_dict: \"bert.embeddings.position_ids\", \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.embeddings.LayerNorm.weight\", \"bert.embeddings.LayerNorm.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.encoder.layer.1.intermediate.dense.weight\", \"bert.encoder.layer.1.intermediate.dense.bias\", \"bert.encoder.layer.1.output.dense.weight\", \"bert.encoder.layer.1.output.dense.bias\", \"bert.encoder.layer.1.output.LayerNorm.weight\", \"bert.encoder.layer.1.output.LayerNorm.bias\", \"bert.encoder.layer.2.attention.self.query.weight\", \"bert.encoder.layer.2.attention.self.query.bias\", \"bert.encoder.layer.2.attention.self.key.weight\", \"bert.encoder.layer.2.attention.self.key.bias\", \"bert.encoder.layer.2.attention.self.value.weight\", \"bert.encoder.layer.2.attention.self.value.bias\", \"bert.encoder.layer.2.attention.output.dense.weight\", \"bert.encoder.layer.2.attention.output.dense.bias\", \"bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.encoder.layer.2.intermediate.dense.weight\", \"bert.encoder.layer.2.intermediate.dense.bias\", \"bert.encoder.layer.2.output.dense.weight\", \"bert.encoder.layer.2.output.dense.bias\", \"bert.encoder.layer.2.output.LayerNorm.weight\", \"bert.encoder.layer.2.output.LayerNorm.bias\", \"bert.encoder.layer.3.attention.self.query.weight\", \"bert.encoder.layer.3.attention.self.query.bias\", \"bert.encoder.layer.3.attention.self.key.weight\", \"bert.encoder.layer.3.attention.self.key.bias\", \"bert.encoder.layer.3.attention.self.value.weight\", \"bert.encoder.layer.3.attention.self.value.bias\", \"bert.encoder.layer.3.attention.output.dense.weight\", \"bert.encoder.layer.3.attention.output.dense.bias\", \"bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.encoder.layer.3.intermediate.dense.weight\", \"bert.encoder.layer.3.intermediate.dense.bias\", \"bert.encoder.layer.3.output.dense.weight\", \"bert.encoder.layer.3.output.dense.bias\", \"bert.encoder.layer.3.output.LayerNorm.weight\", \"bert.encoder.layer.3.output.LayerNorm.bias\", \"bert.encoder.layer.4.attention.self.query.weight\", \"bert.encoder.layer.4.attention.self.query.bias\", \"bert.encoder.layer.4.attention.self.key.weight\", \"bert.encoder.layer.4.attention.self.key.bias\", \"bert.encoder.layer.4.attention.self.value.weight\", \"bert.encoder.layer.4.attention.self.value.bias\", \"bert.encoder.layer.4.attention.output.dense.weight\", \"bert.encoder.layer.4.attention.output.dense.bias\", \"bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.encoder.layer.4.intermediate.dense.weight\", \"bert.encoder.layer.4.intermediate.dense.bias\", \"bert.encoder.layer.4.output.dense.weight\", \"bert.encoder.layer.4.output.dense.bias\", \"bert.encoder.layer.4.output.LayerNorm.weight\", \"bert.encoder.layer.4.output.LayerNorm.bias\", \"bert.encoder.layer.5.attention.self.query.weight\", \"bert.encoder.layer.5.attention.self.query.bias\", \"bert.encoder.layer.5.attention.self.key.weight\", \"bert.encoder.layer.5.attention.self.key.bias\", \"bert.encoder.layer.5.attention.self.value.weight\", \"bert.encoder.layer.5.attention.self.value.bias\", \"bert.encoder.layer.5.attention.output.dense.weight\", \"bert.encoder.layer.5.attention.output.dense.bias\", \"bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.encoder.layer.5.intermediate.dense.weight\", \"bert.encoder.layer.5.intermediate.dense.bias\", \"bert.encoder.layer.5.output.dense.weight\", \"bert.encoder.layer.5.output.dense.bias\", \"bert.encoder.layer.5.output.LayerNorm.weight\", \"bert.encoder.layer.5.output.LayerNorm.bias\", \"bert.encoder.layer.6.attention.self.query.weight\", \"bert.encoder.layer.6.attention.self.query.bias\", \"bert.encoder.layer.6.attention.self.key.weight\", \"bert.encoder.layer.6.attention.self.key.bias\", \"bert.encoder.layer.6.attention.self.value.weight\", \"bert.encoder.layer.6.attention.self.value.bias\", \"bert.encoder.layer.6.attention.output.dense.weight\", \"bert.encoder.layer.6.attention.output.dense.bias\", \"bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.encoder.layer.6.intermediate.dense.weight\", \"bert.encoder.layer.6.intermediate.dense.bias\", \"bert.encoder.layer.6.output.dense.weight\", \"bert.encoder.layer.6.output.dense.bias\", \"bert.encoder.layer.6.output.LayerNorm.weight\", \"bert.encoder.layer.6.output.LayerNorm.bias\", \"bert.encoder.layer.7.attention.self.query.weight\", \"bert.encoder.layer.7.attention.self.query.bias\", \"bert.encoder.layer.7.attention.self.key.weight\", \"bert.encoder.layer.7.attention.self.key.bias\", \"bert.encoder.layer.7.attention.self.value.weight\", \"bert.encoder.layer.7.attention.self.value.bias\", \"bert.encoder.layer.7.attention.output.dense.weight\", \"bert.encoder.layer.7.attention.output.dense.bias\", \"bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.encoder.layer.7.intermediate.dense.weight\", \"bert.encoder.layer.7.intermediate.dense.bias\", \"bert.encoder.layer.7.output.dense.weight\", \"bert.encoder.layer.7.output.dense.bias\", \"bert.encoder.layer.7.output.LayerNorm.weight\", \"bert.encoder.layer.7.output.LayerNorm.bias\", \"bert.encoder.layer.8.attention.self.query.weight\", \"bert.encoder.layer.8.attention.self.query.bias\", \"bert.encoder.layer.8.attention.self.key.weight\", \"bert.encoder.layer.8.attention.self.key.bias\", \"bert.encoder.layer.8.attention.self.value.weight\", \"bert.encoder.layer.8.attention.self.value.bias\", \"bert.encoder.layer.8.attention.output.dense.weight\", \"bert.encoder.layer.8.attention.output.dense.bias\", \"bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.encoder.layer.8.intermediate.dense.weight\", \"bert.encoder.layer.8.intermediate.dense.bias\", \"bert.encoder.layer.8.output.dense.weight\", \"bert.encoder.layer.8.output.dense.bias\", \"bert.encoder.layer.8.output.LayerNorm.weight\", \"bert.encoder.layer.8.output.LayerNorm.bias\", \"bert.encoder.layer.9.attention.self.query.weight\", \"bert.encoder.layer.9.attention.self.query.bias\", \"bert.encoder.layer.9.attention.self.key.weight\", \"bert.encoder.layer.9.attention.self.key.bias\", \"bert.encoder.layer.9.attention.self.value.weight\", \"bert.encoder.layer.9.attention.self.value.bias\", \"bert.encoder.layer.9.attention.output.dense.weight\", \"bert.encoder.layer.9.attention.output.dense.bias\", \"bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.encoder.layer.9.intermediate.dense.weight\", \"bert.encoder.layer.9.intermediate.dense.bias\", \"bert.encoder.layer.9.output.dense.weight\", \"bert.encoder.layer.9.output.dense.bias\", \"bert.encoder.layer.9.output.LayerNorm.weight\", \"bert.encoder.layer.9.output.LayerNorm.bias\", \"bert.encoder.layer.10.attention.self.query.weight\", \"bert.encoder.layer.10.attention.self.query.bias\", \"bert.encoder.layer.10.attention.self.key.weight\", \"bert.encoder.layer.10.attention.self.key.bias\", \"bert.encoder.layer.10.attention.self.value.weight\", \"bert.encoder.layer.10.attention.self.value.bias\", \"bert.encoder.layer.10.attention.output.dense.weight\", \"bert.encoder.layer.10.attention.output.dense.bias\", \"bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.encoder.layer.10.intermediate.dense.weight\", \"bert.encoder.layer.10.intermediate.dense.bias\", \"bert.encoder.layer.10.output.dense.weight\", \"bert.encoder.layer.10.output.dense.bias\", \"bert.encoder.layer.10.output.LayerNorm.weight\", \"bert.encoder.layer.10.output.LayerNorm.bias\", \"bert.encoder.layer.11.attention.self.query.weight\", \"bert.encoder.layer.11.attention.self.query.bias\", \"bert.encoder.layer.11.attention.self.key.weight\", \"bert.encoder.layer.11.attention.self.key.bias\", \"bert.encoder.layer.11.attention.self.value.weight\", \"bert.encoder.layer.11.attention.self.value.bias\", \"bert.encoder.layer.11.attention.output.dense.weight\", \"bert.encoder.layer.11.attention.output.dense.bias\", \"bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.encoder.layer.11.intermediate.dense.weight\", \"bert.encoder.layer.11.intermediate.dense.bias\", \"bert.encoder.layer.11.output.dense.weight\", \"bert.encoder.layer.11.output.dense.bias\", \"bert.encoder.layer.11.output.LayerNorm.weight\", \"bert.encoder.layer.11.output.LayerNorm.bias\", \"bert.encoder.layer.12.attention.self.query.weight\", \"bert.encoder.layer.12.attention.self.query.bias\", \"bert.encoder.layer.12.attention.self.key.weight\", \"bert.encoder.layer.12.attention.self.key.bias\", \"bert.encoder.layer.12.attention.self.value.weight\", \"bert.encoder.layer.12.attention.self.value.bias\", \"bert.encoder.layer.12.attention.output.dense.weight\", \"bert.encoder.layer.12.attention.output.dense.bias\", \"bert.encoder.layer.12.attention.output.LayerNorm.weight\", \"bert.encoder.layer.12.attention.output.LayerNorm.bias\", \"bert.encoder.layer.12.intermediate.dense.weight\", \"bert.encoder.layer.12.intermediate.dense.bias\", \"bert.encoder.layer.12.output.dense.weight\", \"bert.encoder.layer.12.output.dense.bias\", \"bert.encoder.layer.12.output.LayerNorm.weight\", \"bert.encoder.layer.12.output.LayerNorm.bias\", \"bert.encoder.layer.13.attention.self.query.weight\", \"bert.encoder.layer.13.attention.self.query.bias\", \"bert.encoder.layer.13.attention.self.key.weight\", \"bert.encoder.layer.13.attention.self.key.bias\", \"bert.encoder.layer.13.attention.self.value.weight\", \"bert.encoder.layer.13.attention.self.value.bias\", \"bert.encoder.layer.13.attention.output.dense.weight\", \"bert.encoder.layer.13.attention.output.dense.bias\", \"bert.encoder.layer.13.attention.output.LayerNorm.weight\", \"bert.encoder.layer.13.attention.output.LayerNorm.bias\", \"bert.encoder.layer.13.intermediate.dense.weight\", \"bert.encoder.layer.13.intermediate.dense.bias\", \"bert.encoder.layer.13.output.dense.weight\", \"bert.encoder.layer.13.output.dense.bias\", \"bert.encoder.layer.13.output.LayerNorm.weight\", \"bert.encoder.layer.13.output.LayerNorm.bias\", \"bert.encoder.layer.14.attention.self.query.weight\", \"bert.encoder.layer.14.attention.self.query.bias\", \"bert.encoder.layer.14.attention.self.key.weight\", \"bert.encoder.layer.14.attention.self.key.bias\", \"bert.encoder.layer.14.attention.self.value.weight\", \"bert.encoder.layer.14.attention.self.value.bias\", \"bert.encoder.layer.14.attention.output.dense.weight\", \"bert.encoder.layer.14.attention.output.dense.bias\", \"bert.encoder.layer.14.attention.output.LayerNorm.weight\", \"bert.encoder.layer.14.attention.output.LayerNorm.bias\", \"bert.encoder.layer.14.intermediate.dense.weight\", \"bert.encoder.layer.14.intermediate.dense.bias\", \"bert.encoder.layer.14.output.dense.weight\", \"bert.encoder.layer.14.output.dense.bias\", \"bert.encoder.layer.14.output.LayerNorm.weight\", \"bert.encoder.layer.14.output.LayerNorm.bias\", \"bert.encoder.layer.15.attention.self.query.weight\", \"bert.encoder.layer.15.attention.self.query.bias\", \"bert.encoder.layer.15.attention.self.key.weight\", \"bert.encoder.layer.15.attention.self.key.bias\", \"bert.encoder.layer.15.attention.self.value.weight\", \"bert.encoder.layer.15.attention.self.value.bias\", \"bert.encoder.layer.15.attention.output.dense.weight\", \"bert.encoder.layer.15.attention.output.dense.bias\", \"bert.encoder.layer.15.attention.output.LayerNorm.weight\", \"bert.encoder.layer.15.attention.output.LayerNorm.bias\", \"bert.encoder.layer.15.intermediate.dense.weight\", \"bert.encoder.layer.15.intermediate.dense.bias\", \"bert.encoder.layer.15.output.dense.weight\", \"bert.encoder.layer.15.output.dense.bias\", \"bert.encoder.layer.15.output.LayerNorm.weight\", \"bert.encoder.layer.15.output.LayerNorm.bias\", \"bert.encoder.layer.16.attention.self.query.weight\", \"bert.encoder.layer.16.attention.self.query.bias\", \"bert.encoder.layer.16.attention.self.key.weight\", \"bert.encoder.layer.16.attention.self.key.bias\", \"bert.encoder.layer.16.attention.self.value.weight\", \"bert.encoder.layer.16.attention.self.value.bias\", \"bert.encoder.layer.16.attention.output.dense.weight\", \"bert.encoder.layer.16.attention.output.dense.bias\", \"bert.encoder.layer.16.attention.output.LayerNorm.weight\", \"bert.encoder.layer.16.attention.output.LayerNorm.bias\", \"bert.encoder.layer.16.intermediate.dense.weight\", \"bert.encoder.layer.16.intermediate.dense.bias\", \"bert.encoder.layer.16.output.dense.weight\", \"bert.encoder.layer.16.output.dense.bias\", \"bert.encoder.layer.16.output.LayerNorm.weight\", \"bert.encoder.layer.16.output.LayerNorm.bias\", \"bert.encoder.layer.17.attention.self.query.weight\", \"bert.encoder.layer.17.attention.self.query.bias\", \"bert.encoder.layer.17.attention.self.key.weight\", \"bert.encoder.layer.17.attention.self.key.bias\", \"bert.encoder.layer.17.attention.self.value.weight\", \"bert.encoder.layer.17.attention.self.value.bias\", \"bert.encoder.layer.17.attention.output.dense.weight\", \"bert.encoder.layer.17.attention.output.dense.bias\", \"bert.encoder.layer.17.attention.output.LayerNorm.weight\", \"bert.encoder.layer.17.attention.output.LayerNorm.bias\", \"bert.encoder.layer.17.intermediate.dense.weight\", \"bert.encoder.layer.17.intermediate.dense.bias\", \"bert.encoder.layer.17.output.dense.weight\", \"bert.encoder.layer.17.output.dense.bias\", \"bert.encoder.layer.17.output.LayerNorm.weight\", \"bert.encoder.layer.17.output.LayerNorm.bias\", \"bert.encoder.layer.18.attention.self.query.weight\", \"bert.encoder.layer.18.attention.self.query.bias\", \"bert.encoder.layer.18.attention.self.key.weight\", \"bert.encoder.layer.18.attention.self.key.bias\", \"bert.encoder.layer.18.attention.self.value.weight\", \"bert.encoder.layer.18.attention.self.value.bias\", \"bert.encoder.layer.18.attention.output.dense.weight\", \"bert.encoder.layer.18.attention.output.dense.bias\", \"bert.encoder.layer.18.attention.output.LayerNorm.weight\", \"bert.encoder.layer.18.attention.output.LayerNorm.bias\", \"bert.encoder.layer.18.intermediate.dense.weight\", \"bert.encoder.layer.18.intermediate.dense.bias\", \"bert.encoder.layer.18.output.dense.weight\", \"bert.encoder.layer.18.output.dense.bias\", \"bert.encoder.layer.18.output.LayerNorm.weight\", \"bert.encoder.layer.18.output.LayerNorm.bias\", \"bert.encoder.layer.19.attention.self.query.weight\", \"bert.encoder.layer.19.attention.self.query.bias\", \"bert.encoder.layer.19.attention.self.key.weight\", \"bert.encoder.layer.19.attention.self.key.bias\", \"bert.encoder.layer.19.attention.self.value.weight\", \"bert.encoder.layer.19.attention.self.value.bias\", \"bert.encoder.layer.19.attention.output.dense.weight\", \"bert.encoder.layer.19.attention.output.dense.bias\", \"bert.encoder.layer.19.attention.output.LayerNorm.weight\", \"bert.encoder.layer.19.attention.output.LayerNorm.bias\", \"bert.encoder.layer.19.intermediate.dense.weight\", \"bert.encoder.layer.19.intermediate.dense.bias\", \"bert.encoder.layer.19.output.dense.weight\", \"bert.encoder.layer.19.output.dense.bias\", \"bert.encoder.layer.19.output.LayerNorm.weight\", \"bert.encoder.layer.19.output.LayerNorm.bias\", \"bert.encoder.layer.20.attention.self.query.weight\", \"bert.encoder.layer.20.attention.self.query.bias\", \"bert.encoder.layer.20.attention.self.key.weight\", \"bert.encoder.layer.20.attention.self.key.bias\", \"bert.encoder.layer.20.attention.self.value.weight\", \"bert.encoder.layer.20.attention.self.value.bias\", \"bert.encoder.layer.20.attention.output.dense.weight\", \"bert.encoder.layer.20.attention.output.dense.bias\", \"bert.encoder.layer.20.attention.output.LayerNorm.weight\", \"bert.encoder.layer.20.attention.output.LayerNorm.bias\", \"bert.encoder.layer.20.intermediate.dense.weight\", \"bert.encoder.layer.20.intermediate.dense.bias\", \"bert.encoder.layer.20.output.dense.weight\", \"bert.encoder.layer.20.output.dense.bias\", \"bert.encoder.layer.20.output.LayerNorm.weight\", \"bert.encoder.layer.20.output.LayerNorm.bias\", \"bert.encoder.layer.21.attention.self.query.weight\", \"bert.encoder.layer.21.attention.self.query.bias\", \"bert.encoder.layer.21.attention.self.key.weight\", \"bert.encoder.layer.21.attention.self.key.bias\", \"bert.encoder.layer.21.attention.self.value.weight\", \"bert.encoder.layer.21.attention.self.value.bias\", \"bert.encoder.layer.21.attention.output.dense.weight\", \"bert.encoder.layer.21.attention.output.dense.bias\", \"bert.encoder.layer.21.attention.output.LayerNorm.weight\", \"bert.encoder.layer.21.attention.output.LayerNorm.bias\", \"bert.encoder.layer.21.intermediate.dense.weight\", \"bert.encoder.layer.21.intermediate.dense.bias\", \"bert.encoder.layer.21.output.dense.weight\", \"bert.encoder.layer.21.output.dense.bias\", \"bert.encoder.layer.21.output.LayerNorm.weight\", \"bert.encoder.layer.21.output.LayerNorm.bias\", \"bert.encoder.layer.22.attention.self.query.weight\", \"bert.encoder.layer.22.attention.self.query.bias\", \"bert.encoder.layer.22.attention.self.key.weight\", \"bert.encoder.layer.22.attention.self.key.bias\", \"bert.encoder.layer.22.attention.self.value.weight\", \"bert.encoder.layer.22.attention.self.value.bias\", \"bert.encoder.layer.22.attention.output.dense.weight\", \"bert.encoder.layer.22.attention.output.dense.bias\", \"bert.encoder.layer.22.attention.output.LayerNorm.weight\", \"bert.encoder.layer.22.attention.output.LayerNorm.bias\", \"bert.encoder.layer.22.intermediate.dense.weight\", \"bert.encoder.layer.22.intermediate.dense.bias\", \"bert.encoder.layer.22.output.dense.weight\", \"bert.encoder.layer.22.output.dense.bias\", \"bert.encoder.layer.22.output.LayerNorm.weight\", \"bert.encoder.layer.22.output.LayerNorm.bias\", \"bert.encoder.layer.23.attention.self.query.weight\", \"bert.encoder.layer.23.attention.self.query.bias\", \"bert.encoder.layer.23.attention.self.key.weight\", \"bert.encoder.layer.23.attention.self.key.bias\", \"bert.encoder.layer.23.attention.self.value.weight\", \"bert.encoder.layer.23.attention.self.value.bias\", \"bert.encoder.layer.23.attention.output.dense.weight\", \"bert.encoder.layer.23.attention.output.dense.bias\", \"bert.encoder.layer.23.attention.output.LayerNorm.weight\", \"bert.encoder.layer.23.attention.output.LayerNorm.bias\", \"bert.encoder.layer.23.intermediate.dense.weight\", \"bert.encoder.layer.23.intermediate.dense.bias\", \"bert.encoder.layer.23.output.dense.weight\", \"bert.encoder.layer.23.output.dense.bias\", \"bert.encoder.layer.23.output.LayerNorm.weight\", \"bert.encoder.layer.23.output.LayerNorm.bias\", \"bert.pooler.dense.weight\", \"bert.pooler.dense.bias\", \"linear.weight\", \"linear.bias\". \n\tUnexpected key(s) in state_dict: \"module.bert.embeddings.position_ids\", \"module.bert.embeddings.word_embeddings.weight\", \"module.bert.embeddings.position_embeddings.weight\", \"module.bert.embeddings.token_type_embeddings.weight\", \"module.bert.embeddings.LayerNorm.weight\", \"module.bert.embeddings.LayerNorm.bias\", \"module.bert.encoder.layer.0.attention.self.query.weight\", \"module.bert.encoder.layer.0.attention.self.query.bias\", \"module.bert.encoder.layer.0.attention.self.key.weight\", \"module.bert.encoder.layer.0.attention.self.key.bias\", \"module.bert.encoder.layer.0.attention.self.value.weight\", \"module.bert.encoder.layer.0.attention.self.value.bias\", \"module.bert.encoder.layer.0.attention.output.dense.weight\", \"module.bert.encoder.layer.0.attention.output.dense.bias\", \"module.bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.0.intermediate.dense.weight\", \"module.bert.encoder.layer.0.intermediate.dense.bias\", \"module.bert.encoder.layer.0.output.dense.weight\", \"module.bert.encoder.layer.0.output.dense.bias\", \"module.bert.encoder.layer.0.output.LayerNorm.weight\", \"module.bert.encoder.layer.0.output.LayerNorm.bias\", \"module.bert.encoder.layer.1.attention.self.query.weight\", \"module.bert.encoder.layer.1.attention.self.query.bias\", \"module.bert.encoder.layer.1.attention.self.key.weight\", \"module.bert.encoder.layer.1.attention.self.key.bias\", \"module.bert.encoder.layer.1.attention.self.value.weight\", \"module.bert.encoder.layer.1.attention.self.value.bias\", \"module.bert.encoder.layer.1.attention.output.dense.weight\", \"module.bert.encoder.layer.1.attention.output.dense.bias\", \"module.bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.1.intermediate.dense.weight\", \"module.bert.encoder.layer.1.intermediate.dense.bias\", \"module.bert.encoder.layer.1.output.dense.weight\", \"module.bert.encoder.layer.1.output.dense.bias\", \"module.bert.encoder.layer.1.output.LayerNorm.weight\", \"module.bert.encoder.layer.1.output.LayerNorm.bias\", \"module.bert.encoder.layer.2.attention.self.query.weight\", \"module.bert.encoder.layer.2.attention.self.query.bias\", \"module.bert.encoder.layer.2.attention.self.key.weight\", \"module.bert.encoder.layer.2.attention.self.key.bias\", \"module.bert.encoder.layer.2.attention.self.value.weight\", \"module.bert.encoder.layer.2.attention.self.value.bias\", \"module.bert.encoder.layer.2.attention.output.dense.weight\", \"module.bert.encoder.layer.2.attention.output.dense.bias\", \"module.bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.2.intermediate.dense.weight\", \"module.bert.encoder.layer.2.intermediate.dense.bias\", \"module.bert.encoder.layer.2.output.dense.weight\", \"module.bert.encoder.layer.2.output.dense.bias\", \"module.bert.encoder.layer.2.output.LayerNorm.weight\", \"module.bert.encoder.layer.2.output.LayerNorm.bias\", \"module.bert.encoder.layer.3.attention.self.query.weight\", \"module.bert.encoder.layer.3.attention.self.query.bias\", \"module.bert.encoder.layer.3.attention.self.key.weight\", \"module.bert.encoder.layer.3.attention.self.key.bias\", \"module.bert.encoder.layer.3.attention.self.value.weight\", \"module.bert.encoder.layer.3.attention.self.value.bias\", \"module.bert.encoder.layer.3.attention.output.dense.weight\", \"module.bert.encoder.layer.3.attention.output.dense.bias\", \"module.bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.3.intermediate.dense.weight\", \"module.bert.encoder.layer.3.intermediate.dense.bias\", \"module.bert.encoder.layer.3.output.dense.weight\", \"module.bert.encoder.layer.3.output.dense.bias\", \"module.bert.encoder.layer.3.output.LayerNorm.weight\", \"module.bert.encoder.layer.3.output.LayerNorm.bias\", \"module.bert.encoder.layer.4.attention.self.query.weight\", \"module.bert.encoder.layer.4.attention.self.query.bias\", \"module.bert.encoder.layer.4.attention.self.key.weight\", \"module.bert.encoder.layer.4.attention.self.key.bias\", \"module.bert.encoder.layer.4.attention.self.value.weight\", \"module.bert.encoder.layer.4.attention.self.value.bias\", \"module.bert.encoder.layer.4.attention.output.dense.weight\", \"module.bert.encoder.layer.4.attention.output.dense.bias\", \"module.bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.4.intermediate.dense.weight\", \"module.bert.encoder.layer.4.intermediate.dense.bias\", \"module.bert.encoder.layer.4.output.dense.weight\", \"module.bert.encoder.layer.4.output.dense.bias\", \"module.bert.encoder.layer.4.output.LayerNorm.weight\", \"module.bert.encoder.layer.4.output.LayerNorm.bias\", \"module.bert.encoder.layer.5.attention.self.query.weight\", \"module.bert.encoder.layer.5.attention.self.query.bias\", \"module.bert.encoder.layer.5.attention.self.key.weight\", \"module.bert.encoder.layer.5.attention.self.key.bias\", \"module.bert.encoder.layer.5.attention.self.value.weight\", \"module.bert.encoder.layer.5.attention.self.value.bias\", \"module.bert.encoder.layer.5.attention.output.dense.weight\", \"module.bert.encoder.layer.5.attention.output.dense.bias\", \"module.bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.5.intermediate.dense.weight\", \"module.bert.encoder.layer.5.intermediate.dense.bias\", \"module.bert.encoder.layer.5.output.dense.weight\", \"module.bert.encoder.layer.5.output.dense.bias\", \"module.bert.encoder.layer.5.output.LayerNorm.weight\", \"module.bert.encoder.layer.5.output.LayerNorm.bias\", \"module.bert.encoder.layer.6.attention.self.query.weight\", \"module.bert.encoder.layer.6.attention.self.query.bias\", \"module.bert.encoder.layer.6.attention.self.key.weight\", \"module.bert.encoder.layer.6.attention.self.key.bias\", \"module.bert.encoder.layer.6.attention.self.value.weight\", \"module.bert.encoder.layer.6.attention.self.value.bias\", \"module.bert.encoder.layer.6.attention.output.dense.weight\", \"module.bert.encoder.layer.6.attention.output.dense.bias\", \"module.bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.6.intermediate.dense.weight\", \"module.bert.encoder.layer.6.intermediate.dense.bias\", \"module.bert.encoder.layer.6.output.dense.weight\", \"module.bert.encoder.layer.6.output.dense.bias\", \"module.bert.encoder.layer.6.output.LayerNorm.weight\", \"module.bert.encoder.layer.6.output.LayerNorm.bias\", \"module.bert.encoder.layer.7.attention.self.query.weight\", \"module.bert.encoder.layer.7.attention.self.query.bias\", \"module.bert.encoder.layer.7.attention.self.key.weight\", \"module.bert.encoder.layer.7.attention.self.key.bias\", \"module.bert.encoder.layer.7.attention.self.value.weight\", \"module.bert.encoder.layer.7.attention.self.value.bias\", \"module.bert.encoder.layer.7.attention.output.dense.weight\", \"module.bert.encoder.layer.7.attention.output.dense.bias\", \"module.bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.7.intermediate.dense.weight\", \"module.bert.encoder.layer.7.intermediate.dense.bias\", \"module.bert.encoder.layer.7.output.dense.weight\", \"module.bert.encoder.layer.7.output.dense.bias\", \"module.bert.encoder.layer.7.output.LayerNorm.weight\", \"module.bert.encoder.layer.7.output.LayerNorm.bias\", \"module.bert.encoder.layer.8.attention.self.query.weight\", \"module.bert.encoder.layer.8.attention.self.query.bias\", \"module.bert.encoder.layer.8.attention.self.key.weight\", \"module.bert.encoder.layer.8.attention.self.key.bias\", \"module.bert.encoder.layer.8.attention.self.value.weight\", \"module.bert.encoder.layer.8.attention.self.value.bias\", \"module.bert.encoder.layer.8.attention.output.dense.weight\", \"module.bert.encoder.layer.8.attention.output.dense.bias\", \"module.bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.8.intermediate.dense.weight\", \"module.bert.encoder.layer.8.intermediate.dense.bias\", \"module.bert.encoder.layer.8.output.dense.weight\", \"module.bert.encoder.layer.8.output.dense.bias\", \"module.bert.encoder.layer.8.output.LayerNorm.weight\", \"module.bert.encoder.layer.8.output.LayerNorm.bias\", \"module.bert.encoder.layer.9.attention.self.query.weight\", \"module.bert.encoder.layer.9.attention.self.query.bias\", \"module.bert.encoder.layer.9.attention.self.key.weight\", \"module.bert.encoder.layer.9.attention.self.key.bias\", \"module.bert.encoder.layer.9.attention.self.value.weight\", \"module.bert.encoder.layer.9.attention.self.value.bias\", \"module.bert.encoder.layer.9.attention.output.dense.weight\", \"module.bert.encoder.layer.9.attention.output.dense.bias\", \"module.bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.9.intermediate.dense.weight\", \"module.bert.encoder.layer.9.intermediate.dense.bias\", \"module.bert.encoder.layer.9.output.dense.weight\", \"module.bert.encoder.layer.9.output.dense.bias\", \"module.bert.encoder.layer.9.output.LayerNorm.weight\", \"module.bert.encoder.layer.9.output.LayerNorm.bias\", \"module.bert.encoder.layer.10.attention.self.query.weight\", \"module.bert.encoder.layer.10.attention.self.query.bias\", \"module.bert.encoder.layer.10.attention.self.key.weight\", \"module.bert.encoder.layer.10.attention.self.key.bias\", \"module.bert.encoder.layer.10.attention.self.value.weight\", \"module.bert.encoder.layer.10.attention.self.value.bias\", \"module.bert.encoder.layer.10.attention.output.dense.weight\", \"module.bert.encoder.layer.10.attention.output.dense.bias\", \"module.bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.10.intermediate.dense.weight\", \"module.bert.encoder.layer.10.intermediate.dense.bias\", \"module.bert.encoder.layer.10.output.dense.weight\", \"module.bert.encoder.layer.10.output.dense.bias\", \"module.bert.encoder.layer.10.output.LayerNorm.weight\", \"module.bert.encoder.layer.10.output.LayerNorm.bias\", \"module.bert.encoder.layer.11.attention.self.query.weight\", \"module.bert.encoder.layer.11.attention.self.query.bias\", \"module.bert.encoder.layer.11.attention.self.key.weight\", \"module.bert.encoder.layer.11.attention.self.key.bias\", \"module.bert.encoder.layer.11.attention.self.value.weight\", \"module.bert.encoder.layer.11.attention.self.value.bias\", \"module.bert.encoder.layer.11.attention.output.dense.weight\", \"module.bert.encoder.layer.11.attention.output.dense.bias\", \"module.bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.11.intermediate.dense.weight\", \"module.bert.encoder.layer.11.intermediate.dense.bias\", \"module.bert.encoder.layer.11.output.dense.weight\", \"module.bert.encoder.layer.11.output.dense.bias\", \"module.bert.encoder.layer.11.output.LayerNorm.weight\", \"module.bert.encoder.layer.11.output.LayerNorm.bias\", \"module.bert.encoder.layer.12.attention.self.query.weight\", \"module.bert.encoder.layer.12.attention.self.query.bias\", \"module.bert.encoder.layer.12.attention.self.key.weight\", \"module.bert.encoder.layer.12.attention.self.key.bias\", \"module.bert.encoder.layer.12.attention.self.value.weight\", \"module.bert.encoder.layer.12.attention.self.value.bias\", \"module.bert.encoder.layer.12.attention.output.dense.weight\", \"module.bert.encoder.layer.12.attention.output.dense.bias\", \"module.bert.encoder.layer.12.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.12.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.12.intermediate.dense.weight\", \"module.bert.encoder.layer.12.intermediate.dense.bias\", \"module.bert.encoder.layer.12.output.dense.weight\", \"module.bert.encoder.layer.12.output.dense.bias\", \"module.bert.encoder.layer.12.output.LayerNorm.weight\", \"module.bert.encoder.layer.12.output.LayerNorm.bias\", \"module.bert.encoder.layer.13.attention.self.query.weight\", \"module.bert.encoder.layer.13.attention.self.query.bias\", \"module.bert.encoder.layer.13.attention.self.key.weight\", \"module.bert.encoder.layer.13.attention.self.key.bias\", \"module.bert.encoder.layer.13.attention.self.value.weight\", \"module.bert.encoder.layer.13.attention.self.value.bias\", \"module.bert.encoder.layer.13.attention.output.dense.weight\", \"module.bert.encoder.layer.13.attention.output.dense.bias\", \"module.bert.encoder.layer.13.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.13.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.13.intermediate.dense.weight\", \"module.bert.encoder.layer.13.intermediate.dense.bias\", \"module.bert.encoder.layer.13.output.dense.weight\", \"module.bert.encoder.layer.13.output.dense.bias\", \"module.bert.encoder.layer.13.output.LayerNorm.weight\", \"module.bert.encoder.layer.13.output.LayerNorm.bias\", \"module.bert.encoder.layer.14.attention.self.query.weight\", \"module.bert.encoder.layer.14.attention.self.query.bias\", \"module.bert.encoder.layer.14.attention.self.key.weight\", \"module.bert.encoder.layer.14.attention.self.key.bias\", \"module.bert.encoder.layer.14.attention.self.value.weight\", \"module.bert.encoder.layer.14.attention.self.value.bias\", \"module.bert.encoder.layer.14.attention.output.dense.weight\", \"module.bert.encoder.layer.14.attention.output.dense.bias\", \"module.bert.encoder.layer.14.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.14.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.14.intermediate.dense.weight\", \"module.bert.encoder.layer.14.intermediate.dense.bias\", \"module.bert.encoder.layer.14.output.dense.weight\", \"module.bert.encoder.layer.14.output.dense.bias\", \"module.bert.encoder.layer.14.output.LayerNorm.weight\", \"module.bert.encoder.layer.14.output.LayerNorm.bias\", \"module.bert.encoder.layer.15.attention.self.query.weight\", \"module.bert.encoder.layer.15.attention.self.query.bias\", \"module.bert.encoder.layer.15.attention.self.key.weight\", \"module.bert.encoder.layer.15.attention.self.key.bias\", \"module.bert.encoder.layer.15.attention.self.value.weight\", \"module.bert.encoder.layer.15.attention.self.value.bias\", \"module.bert.encoder.layer.15.attention.output.dense.weight\", \"module.bert.encoder.layer.15.attention.output.dense.bias\", \"module.bert.encoder.layer.15.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.15.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.15.intermediate.dense.weight\", \"module.bert.encoder.layer.15.intermediate.dense.bias\", \"module.bert.encoder.layer.15.output.dense.weight\", \"module.bert.encoder.layer.15.output.dense.bias\", \"module.bert.encoder.layer.15.output.LayerNorm.weight\", \"module.bert.encoder.layer.15.output.LayerNorm.bias\", \"module.bert.encoder.layer.16.attention.self.query.weight\", \"module.bert.encoder.layer.16.attention.self.query.bias\", \"module.bert.encoder.layer.16.attention.self.key.weight\", \"module.bert.encoder.layer.16.attention.self.key.bias\", \"module.bert.encoder.layer.16.attention.self.value.weight\", \"module.bert.encoder.layer.16.attention.self.value.bias\", \"module.bert.encoder.layer.16.attention.output.dense.weight\", \"module.bert.encoder.layer.16.attention.output.dense.bias\", \"module.bert.encoder.layer.16.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.16.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.16.intermediate.dense.weight\", \"module.bert.encoder.layer.16.intermediate.dense.bias\", \"module.bert.encoder.layer.16.output.dense.weight\", \"module.bert.encoder.layer.16.output.dense.bias\", \"module.bert.encoder.layer.16.output.LayerNorm.weight\", \"module.bert.encoder.layer.16.output.LayerNorm.bias\", \"module.bert.encoder.layer.17.attention.self.query.weight\", \"module.bert.encoder.layer.17.attention.self.query.bias\", \"module.bert.encoder.layer.17.attention.self.key.weight\", \"module.bert.encoder.layer.17.attention.self.key.bias\", \"module.bert.encoder.layer.17.attention.self.value.weight\", \"module.bert.encoder.layer.17.attention.self.value.bias\", \"module.bert.encoder.layer.17.attention.output.dense.weight\", \"module.bert.encoder.layer.17.attention.output.dense.bias\", \"module.bert.encoder.layer.17.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.17.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.17.intermediate.dense.weight\", \"module.bert.encoder.layer.17.intermediate.dense.bias\", \"module.bert.encoder.layer.17.output.dense.weight\", \"module.bert.encoder.layer.17.output.dense.bias\", \"module.bert.encoder.layer.17.output.LayerNorm.weight\", \"module.bert.encoder.layer.17.output.LayerNorm.bias\", \"module.bert.encoder.layer.18.attention.self.query.weight\", \"module.bert.encoder.layer.18.attention.self.query.bias\", \"module.bert.encoder.layer.18.attention.self.key.weight\", \"module.bert.encoder.layer.18.attention.self.key.bias\", \"module.bert.encoder.layer.18.attention.self.value.weight\", \"module.bert.encoder.layer.18.attention.self.value.bias\", \"module.bert.encoder.layer.18.attention.output.dense.weight\", \"module.bert.encoder.layer.18.attention.output.dense.bias\", \"module.bert.encoder.layer.18.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.18.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.18.intermediate.dense.weight\", \"module.bert.encoder.layer.18.intermediate.dense.bias\", \"module.bert.encoder.layer.18.output.dense.weight\", \"module.bert.encoder.layer.18.output.dense.bias\", \"module.bert.encoder.layer.18.output.LayerNorm.weight\", \"module.bert.encoder.layer.18.output.LayerNorm.bias\", \"module.bert.encoder.layer.19.attention.self.query.weight\", \"module.bert.encoder.layer.19.attention.self.query.bias\", \"module.bert.encoder.layer.19.attention.self.key.weight\", \"module.bert.encoder.layer.19.attention.self.key.bias\", \"module.bert.encoder.layer.19.attention.self.value.weight\", \"module.bert.encoder.layer.19.attention.self.value.bias\", \"module.bert.encoder.layer.19.attention.output.dense.weight\", \"module.bert.encoder.layer.19.attention.output.dense.bias\", \"module.bert.encoder.layer.19.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.19.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.19.intermediate.dense.weight\", \"module.bert.encoder.layer.19.intermediate.dense.bias\", \"module.bert.encoder.layer.19.output.dense.weight\", \"module.bert.encoder.layer.19.output.dense.bias\", \"module.bert.encoder.layer.19.output.LayerNorm.weight\", \"module.bert.encoder.layer.19.output.LayerNorm.bias\", \"module.bert.encoder.layer.20.attention.self.query.weight\", \"module.bert.encoder.layer.20.attention.self.query.bias\", \"module.bert.encoder.layer.20.attention.self.key.weight\", \"module.bert.encoder.layer.20.attention.self.key.bias\", \"module.bert.encoder.layer.20.attention.self.value.weight\", \"module.bert.encoder.layer.20.attention.self.value.bias\", \"module.bert.encoder.layer.20.attention.output.dense.weight\", \"module.bert.encoder.layer.20.attention.output.dense.bias\", \"module.bert.encoder.layer.20.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.20.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.20.intermediate.dense.weight\", \"module.bert.encoder.layer.20.intermediate.dense.bias\", \"module.bert.encoder.layer.20.output.dense.weight\", \"module.bert.encoder.layer.20.output.dense.bias\", \"module.bert.encoder.layer.20.output.LayerNorm.weight\", \"module.bert.encoder.layer.20.output.LayerNorm.bias\", \"module.bert.encoder.layer.21.attention.self.query.weight\", \"module.bert.encoder.layer.21.attention.self.query.bias\", \"module.bert.encoder.layer.21.attention.self.key.weight\", \"module.bert.encoder.layer.21.attention.self.key.bias\", \"module.bert.encoder.layer.21.attention.self.value.weight\", \"module.bert.encoder.layer.21.attention.self.value.bias\", \"module.bert.encoder.layer.21.attention.output.dense.weight\", \"module.bert.encoder.layer.21.attention.output.dense.bias\", \"module.bert.encoder.layer.21.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.21.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.21.intermediate.dense.weight\", \"module.bert.encoder.layer.21.intermediate.dense.bias\", \"module.bert.encoder.layer.21.output.dense.weight\", \"module.bert.encoder.layer.21.output.dense.bias\", \"module.bert.encoder.layer.21.output.LayerNorm.weight\", \"module.bert.encoder.layer.21.output.LayerNorm.bias\", \"module.bert.encoder.layer.22.attention.self.query.weight\", \"module.bert.encoder.layer.22.attention.self.query.bias\", \"module.bert.encoder.layer.22.attention.self.key.weight\", \"module.bert.encoder.layer.22.attention.self.key.bias\", \"module.bert.encoder.layer.22.attention.self.value.weight\", \"module.bert.encoder.layer.22.attention.self.value.bias\", \"module.bert.encoder.layer.22.attention.output.dense.weight\", \"module.bert.encoder.layer.22.attention.output.dense.bias\", \"module.bert.encoder.layer.22.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.22.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.22.intermediate.dense.weight\", \"module.bert.encoder.layer.22.intermediate.dense.bias\", \"module.bert.encoder.layer.22.output.dense.weight\", \"module.bert.encoder.layer.22.output.dense.bias\", \"module.bert.encoder.layer.22.output.LayerNorm.weight\", \"module.bert.encoder.layer.22.output.LayerNorm.bias\", \"module.bert.encoder.layer.23.attention.self.query.weight\", \"module.bert.encoder.layer.23.attention.self.query.bias\", \"module.bert.encoder.layer.23.attention.self.key.weight\", \"module.bert.encoder.layer.23.attention.self.key.bias\", \"module.bert.encoder.layer.23.attention.self.value.weight\", \"module.bert.encoder.layer.23.attention.self.value.bias\", \"module.bert.encoder.layer.23.attention.output.dense.weight\", \"module.bert.encoder.layer.23.attention.output.dense.bias\", \"module.bert.encoder.layer.23.attention.output.LayerNorm.weight\", \"module.bert.encoder.layer.23.attention.output.LayerNorm.bias\", \"module.bert.encoder.layer.23.intermediate.dense.weight\", \"module.bert.encoder.layer.23.intermediate.dense.bias\", \"module.bert.encoder.layer.23.output.dense.weight\", \"module.bert.encoder.layer.23.output.dense.bias\", \"module.bert.encoder.layer.23.output.LayerNorm.weight\", \"module.bert.encoder.layer.23.output.LayerNorm.bias\", \"module.bert.pooler.dense.weight\", \"module.bert.pooler.dense.bias\", \"module.linear.weight\", \"module.linear.bias\". "
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('../../Downloads/bert-large-cased_0.741.bin', map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "def predict_one(text):\n",
    "    encoding = bert_tokenizer.encode_plus(\n",
    "                          text,\n",
    "                          add_special_tokens=True,\n",
    "                          max_length=300,\n",
    "                          truncation=True,\n",
    "                          return_token_type_ids=False,\n",
    "                          padding=PaddingStrategy.MAX_LENGTH,\n",
    "                          return_attention_mask=True,\n",
    "                          return_tensors='pt')\n",
    "    \n",
    "    input_id = encoding['input_ids'].to(device)\n",
    "    mask = encoding['attention_mask'].to(device)\n",
    "    output = model(input_id, mask)\n",
    "    _, prediction_class = torch.max(output, dim=1)\n",
    "    prediction_class = prediction_class.cpu().numpy().tolist()[0]\n",
    "    \n",
    "    label_types = ['negative', 'neutral', 'positive']\n",
    "    class_map = {index:label for index, label in enumerate(label_types)}\n",
    "    prediction = class_map[prediction_class]\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_bakeoff_submission(predict_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bakeoff_submission(\n",
    "        predict_one_func,\n",
    "        output_filename='cs224u-sentiment-bakeoff-entry.csv'):\n",
    "\n",
    "    bakeoff_test = sst.bakeoff_test_reader(SST_HOME)\n",
    "    sst_test = sst.test_reader(SST_HOME)\n",
    "    bakeoff_test['dataset'] = 'bakeoff'\n",
    "    sst_test['dataset'] = 'sst3'\n",
    "    df = pd.concat((bakeoff_test, sst_test))\n",
    "\n",
    "    df['prediction'] = df['sentence'].apply(predict_one_func)\n",
    "\n",
    "    df.to_csv(output_filename, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for example, the following will create a bake-off entry based on `predict_one_softmax`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This check ensure that the following code only runs on the local environment only.\n",
    "# The following call will not be run on the autograder environment.\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    pass\n",
    "    create_bakeoff_submission(predict_one_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a file `cs224u-sentiment-bakeoff-entry.csv` in the current directory. That file should be uploaded as-is. Please do not change its name.\n",
    "\n",
    "Only one upload per team is permitted, and you should do no tuning of your system based on what you see in our bakeoff prediction file – you should not study that file in anyway, beyond perhaps checking that it contains what you expected it to contain. The upload function will do some additional checking to ensure that your file is well-formed.\n",
    "\n",
    "People who enter will receive the additional homework point, and people whose systems achieve the top score will receive an additional 0.5 points. We will test the top-performing systems ourselves, and only systems for which we can reproduce the reported results will win the extra 0.5 points.\n",
    "\n",
    "Late entries will be accepted, but they cannot earn the extra 0.5 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instruction\n",
    "\n",
    "Review and follow the [Homework and bake-off code: Formatting guide](hw_formatting_guide.ipynb).\n",
    "Please do not change the file name as described below.\n",
    "\n",
    "Submit the following files to Gradescope:\n",
    "\n",
    "- `hw_sentiment.ipynb` (this notebook)\n",
    "- `cs224u-sentiment-bakeoff-entry.csv` (bake-off output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
