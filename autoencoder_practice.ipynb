{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85f6ff74-10bd-42dc-8f18-7b812310c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch imports\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "#python standard\n",
    "import time, os\n",
    "\n",
    "#python data science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#cs224u imports\n",
    "import utils\n",
    "import vsm\n",
    "\n",
    "def devdf_generator(df, scoring=str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Removes repeated pairs with distinct scores from dev_df based on scoring removal string.\n",
    "    '''\n",
    "    repeats = df.groupby(['word1', 'word2']).apply(lambda x: x.score.var())\n",
    "    repeats = repeats[repeats > 0].sort_values(ascending=False)\n",
    "    repeats.name = 'score variance'\n",
    "    repeat_list = repeats.index.tolist()\n",
    "    \n",
    "    def repeat_words(first_word, second_word):\n",
    "        return df[(df['word1']==first_word)&(df['word2']==second_word)]\n",
    "\n",
    "    temp_df = df.copy()\n",
    "    for pair in repeat_list:\n",
    "        repeat = repeat_words(pair[0], pair[1])\n",
    "        \n",
    "        if scoring == 'highest':\n",
    "            while len(repeat.index) > 1:\n",
    "                min_score = repeat.score.min()\n",
    "                index = repeat[repeat.score == min_score].index.values[0]\n",
    "                temp_df = temp_df.drop(index=index, axis=0)\n",
    "                repeat = repeat.drop(index=index, axis=0)\n",
    "                \n",
    "        elif scoring == 'lowest':\n",
    "            while len(repeat.index) > 1:\n",
    "                max_score = repeat.score.max()\n",
    "                index = repeat[repeat.score == max_score].index.values[0]\n",
    "                temp_df = temp_df.drop(index=index, axis=0)\n",
    "                repeat = repeat.drop(index=index, axis=0)\n",
    "                \n",
    "        elif scoring == 'mean':\n",
    "                mean_score = repeat.score.mean()\n",
    "                index = repeat.index[0]\n",
    "                drops = repeat.index[1:]\n",
    "                temp_df.loc[index, 'score'] = mean_score\n",
    "                temp_df = temp_df.drop(index=drops, axis=0)\n",
    "                \n",
    "    #check to see that variant pairs are dropped\n",
    "    repeats = temp_df.groupby(['word1', 'word2']).apply(lambda x: x.score.var())\n",
    "    repeats = repeats[repeats > 0.06].sort_values(ascending=False)\n",
    "    repeats.name = 'score variance'\n",
    "    answer = repeats[repeats > 0].sort_values(ascending=False)\n",
    "    if len(answer) < 1:\n",
    "        print('All problematic word pairs are removed')\n",
    "    \n",
    "    return temp_df\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, hidden_activation=nn.Tanh(), tol=0.001, verbose=True):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = input_dim\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.fc1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, 128)\n",
    "        self.fc3 = nn.Linear(128, self.hidden_dim)\n",
    "        self.fc4 = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        self.loss = nn.MSELoss(reduction=\"mean\")\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def forward(self, features):\n",
    "        encode1 = self.fc1(features)\n",
    "        act1 = self.hidden_activation(encode1)\n",
    "        encode2 = self.fc2(act1)\n",
    "        code = self.hidden_activation(encode2)\n",
    "        decode = self.fc3(code)\n",
    "        act3 = self.hidden_activation(decode)\n",
    "        y = self.fc4(act3)\n",
    "        return code, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061fe046-e38f-41fa-bba7-da82a4bf2c4a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim),\n",
    "            self.hidden_activation,\n",
    "            nn.Linear(self.hidden_dim, self.output_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2094ae8d-9522-4011-8813-85dfe1a4b06d",
   "metadata": {},
   "source": [
    "#### Preprocessing work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c41cac55-e8b7-4434-91a5-801ecdf90a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All problematic word pairs are removed\n"
     ]
    }
   ],
   "source": [
    "from vsm import observed_over_expected, pmi\n",
    "\n",
    "VSM_HOME = os.path.join('data', 'vsmdata')\n",
    "DATA_HOME = os.path.join('data', 'wordrelatedness')\n",
    "giga5 = pd.read_csv(os.path.join(VSM_HOME, 'giga_window5-scaled.csv.gz'), index_col=0)\n",
    "dev = pd.read_csv(os.path.join(DATA_HOME, \"cs224u-wordrelatedness-dev.csv\"))\n",
    "highest = devdf_generator(dev, scoring='highest')\n",
    "\n",
    "def run_ppmi_lsa_pipeline(count_df, k):\n",
    "    #reweights count matrix with PPMI\n",
    "    ppmi_df = pmi(count_df)\n",
    "    \n",
    "    #reduce dimensions to k\n",
    "    lsa_df = vsm.lsa(ppmi_df, k=k)\n",
    "    \n",
    "    #evaluate matrices and return rho value\n",
    "    return lsa_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3598d61b-6398-4951-a1b9-987b81becdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_pmi_df = run_ppmi_lsa_pipeline(giga5, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eadb86ae-60cc-4cbc-a238-8533a8a3145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = lsa_pmi_df.values.astype(np.float32)\n",
    "N = dataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43b084c0-f3c4-42e6-b9ba-5b6921602fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AE(200,275).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fx = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49b8f86c-943c-4acf-923b-eae963203875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batch 0 - 256\n",
      "epoch : 0/200, loss = 3.660520\n",
      "epoch : 100/200, loss = 1.016900\n",
      "Training on batch 256 - 512\n",
      "epoch : 0/200, loss = 2.037327\n",
      "epoch : 100/200, loss = 0.343519\n",
      "Training on batch 512 - 768\n",
      "epoch : 0/200, loss = 2.293734\n",
      "epoch : 100/200, loss = 0.239240\n",
      "Training on batch 768 - 1024\n",
      "epoch : 0/200, loss = 2.246789\n",
      "epoch : 100/200, loss = 0.171374\n",
      "Training on batch 1024 - 1280\n",
      "epoch : 0/200, loss = 2.116736\n",
      "epoch : 100/200, loss = 0.147670\n",
      "Training on batch 1280 - 1536\n",
      "epoch : 0/200, loss = 2.148747\n",
      "epoch : 100/200, loss = 0.142657\n",
      "Training on batch 1536 - 1792\n",
      "epoch : 0/200, loss = 2.135506\n",
      "epoch : 100/200, loss = 0.129096\n",
      "Training on batch 1792 - 2048\n",
      "epoch : 0/200, loss = 2.011657\n",
      "epoch : 100/200, loss = 0.122687\n",
      "Training on batch 2048 - 2304\n",
      "epoch : 0/200, loss = 2.304222\n",
      "epoch : 100/200, loss = 0.128825\n",
      "Training on batch 2304 - 2560\n",
      "epoch : 0/200, loss = 2.128673\n",
      "epoch : 100/200, loss = 0.123068\n",
      "Training on batch 2560 - 2816\n",
      "epoch : 0/200, loss = 1.975822\n",
      "epoch : 100/200, loss = 0.117880\n",
      "Training on batch 2816 - 3072\n",
      "epoch : 0/200, loss = 2.078559\n",
      "epoch : 100/200, loss = 0.115532\n",
      "Training on batch 3072 - 3328\n",
      "epoch : 0/200, loss = 2.023072\n",
      "epoch : 100/200, loss = 0.119079\n",
      "Training on batch 3328 - 3584\n",
      "epoch : 0/200, loss = 1.991105\n",
      "epoch : 100/200, loss = 0.119065\n",
      "Training on batch 3584 - 3840\n",
      "epoch : 0/200, loss = 2.032043\n",
      "epoch : 100/200, loss = 0.115520\n",
      "Training on batch 3840 - 4096\n",
      "epoch : 0/200, loss = 1.968498\n",
      "epoch : 100/200, loss = 0.113278\n",
      "Training on batch 4096 - 4352\n",
      "epoch : 0/200, loss = 1.824280\n",
      "epoch : 100/200, loss = 0.110907\n",
      "Training on batch 4352 - 4608\n",
      "epoch : 0/200, loss = 1.873388\n",
      "epoch : 100/200, loss = 0.110831\n",
      "Training on batch 4608 - 4864\n",
      "epoch : 0/200, loss = 1.914769\n",
      "epoch : 100/200, loss = 0.108059\n",
      "Training on batch 4864 - 5120\n",
      "epoch : 0/200, loss = 2.059478\n",
      "epoch : 100/200, loss = 0.116207\n",
      "Training on batch 5120 - 5376\n",
      "epoch : 0/200, loss = 1.946983\n",
      "epoch : 100/200, loss = 0.111277\n",
      "Training on batch 5376 - 5632\n",
      "epoch : 0/200, loss = 1.903409\n",
      "epoch : 100/200, loss = 0.108563\n",
      "Training on batch 5632 - 5888\n",
      "epoch : 0/200, loss = 1.860210\n",
      "epoch : 100/200, loss = 0.103802\n",
      "Training on batch 5888 - 6144\n",
      "epoch : 0/200, loss = 1.719318\n",
      "epoch : 100/200, loss = 0.027426\n",
      "Total time for 200 epochs: 10.757392470016566 minutes\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "batch_size = 256\n",
    "frames = []\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "for x in range(0,N,batch_size):\n",
    "    \n",
    "    subset = dataset[x:x+batch_size]\n",
    "    print(f'Training on batch {x} - {x+batch_size}')\n",
    "    train_loader = torch.utils.data.DataLoader(subset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        loss = 0\n",
    "        previous_loss = 100\n",
    "        \n",
    "        for batch in train_loader:\n",
    "\n",
    "            # load tensors to GPU device, size of feature matrix is [batch_size, hidden_dim]\n",
    "            batch_features = batch.to(device)\n",
    "\n",
    "            # reset the gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # grab hidden layer for conversion to df and output for loss calculations\n",
    "            code, outputs = model(batch_features)\n",
    "            \n",
    "            # compute training loss\n",
    "            train_loss = loss_fx(outputs, batch_features)\n",
    "                \n",
    "            # compute gradients\n",
    "            train_loss.backward()\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # add training loss to epoch loss\n",
    "            loss += train_loss.item()\n",
    "\n",
    "        # compute the epoch training loss\n",
    "        loss = loss / len(train_loader)\n",
    "        \n",
    "        #early stop training if tolerance is reached\n",
    "        if loss < previous_loss:\n",
    "            if abs(previous_loss - loss) < model.tol:\n",
    "                break\n",
    "            else:\n",
    "                previous_loss = loss\n",
    "                \n",
    "        # display the epoch training loss\n",
    "        if model.verbose:\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"epoch : {epoch}/{epochs}, loss = {loss:.6f}\")\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        weights = code.cpu().numpy()\n",
    "    frame = pd.DataFrame(weights)\n",
    "    frames.append(frame)\n",
    "\n",
    "end = time.perf_counter() - start\n",
    "print(f'Total time for {epochs} epochs: {end/60} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cbeaa5f-2f65-4b49-8cc5-c00f06c3fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_df = pd.concat(frames)\n",
    "\n",
    "ae_df.index = lsa_pmi_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef6e901b-68cb-4cbc-bd3a-5efa07ed012a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model sucks\n",
      "0.6803743112316648 0.5934984669463605\n"
     ]
    }
   ],
   "source": [
    "df, baseline_rho = vsm.word_relatedness_evaluation(highest, lsa_pmi_df)\n",
    "df, output_rho = vsm.word_relatedness_evaluation(highest, ae_df)\n",
    "if baseline_rho > output_rho:\n",
    "    print('Your model sucks')\n",
    "else:\n",
    "    print('Good job')\n",
    "print(baseline_rho, output_rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a706e3c-3424-4a59-9cf8-776b1b57c768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5934984669463605"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c5871097-d577-4beb-a8c6-a53602bca9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/wordrelatedness/cs224u-wordrelatedness-test-unlabeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6c14a48a-bdf1-4705-9319-43856323307c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandon</td>\n",
       "      <td>frost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandon</td>\n",
       "      <td>railroad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abortion</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abstract</td>\n",
       "      <td>candle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abstract</td>\n",
       "      <td>frog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>water</td>\n",
       "      <td>water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>ways</td>\n",
       "      <td>ways</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>weather</td>\n",
       "      <td>winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>wednesday</td>\n",
       "      <td>weekday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>word</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          word1     word2\n",
       "0       abandon     frost\n",
       "1       abandon  railroad\n",
       "2      abortion  religion\n",
       "3      abstract    candle\n",
       "4      abstract      frog\n",
       "...         ...       ...\n",
       "1495      water     water\n",
       "1496       ways      ways\n",
       "1497    weather    winter\n",
       "1498  wednesday   weekday\n",
       "1499       word      word\n",
       "\n",
       "[1500 rows x 2 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d702c711-8306-4df0-8c4b-54db9d4a2421",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = vsm.word_relatedness_evaluation(test_df, ae_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d3b2c1a5-5975-4551-b363-71f4e3f42ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(          word1     word2  prediction\n",
       " 0       abandon     frost   -0.994743\n",
       " 1       abandon  railroad   -0.938083\n",
       " 2      abortion  religion   -0.668551\n",
       " 3      abstract    candle   -0.734666\n",
       " 4      abstract      frog   -0.759742\n",
       " ...         ...       ...         ...\n",
       " 1495      water     water    0.000000\n",
       " 1496       ways      ways    0.000000\n",
       " 1497    weather    winter   -0.472126\n",
       " 1498  wednesday   weekday   -0.751630\n",
       " 1499       word      word    0.000000\n",
       " \n",
       " [1500 rows x 3 columns],\n",
       " None)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "efb8b0cb-0c34-41a5-90c9-ce5a3f15e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bakeoff_submission(\n",
    "        vsm_df,\n",
    "        distfunc,\n",
    "        output_filename=\"cs224u-wordrelatedness-bakeoff-entry.csv\"):\n",
    "\n",
    "    test_df = pd.read_csv('data/wordrelatedness/cs224u-wordrelatedness-test-unlabeled.csv')\n",
    "\n",
    "    pred_df, _ = vsm.word_relatedness_evaluation(test_df, vsm_df, distfunc=distfunc)\n",
    "\n",
    "    pred_df.to_csv(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d51128f2-8698-4f6a-bc6b-a28a7355bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bakeoff_submission(ae_df, vsm.cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f978aa0e-4b6e-4374-ba9a-1fdcc5ebe856",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = pd.read_csv('cs224u-wordrelatedness-bakeoff-entry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6f0fea1-7052-4901-94fa-6de8a22fc23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {'path':'somehwereonyourharddrive', 'caption':'This is a blue truck'}\n",
    "test2 = test\n",
    "hits = [test, test2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c605b457-ad36-40c3-94e2-9c5f3a0acabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'somehwereonyourharddrive'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hits[0].values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e000957f-6814-4466-aff6-a8f187cfa15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'somehwereonyourharddrive'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits[0]['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc618d01-490b-4cde-a8fc-25ee7b6d1101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': 'somehwereonyourharddrive', 'caption': 'This is a blue truck'},\n",
       " {'path': 'somehwereonyourharddrive', 'caption': 'This is a blue truck'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b7c0505-cd45-417e-bcda-50d6e9012f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': 'somehwereonyourharddrive', 'caption': 'This is a blue truck'},\n",
       " {'path': 'somehwereonyourharddrive', 'caption': 'This is a blue truck'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits[:9000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0298cfe-efaa-4353-8e01-90f4b7a48c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
